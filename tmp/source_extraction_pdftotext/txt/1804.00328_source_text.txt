BEGIN \protect \problemname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \examplename  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \examplename  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \propositionname  \protect \let  
BEGIN \protect \problemname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \examplename  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \remarkname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \factname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \factname  \protect \let  
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 We shall prove it for the operator $\art ,$ as the proof for the other operator $\tra $ will be similar. By linearity, it is enough to prove the lemma in the case where $f=\cqs $ for some $S\subseteq \left [n\right ].$ Let $y\in \left \{ 0,1\right \} ^{n}.$ Note that \begin {align} \art \left [\cqs \right ]\left (y\right ) & =\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y}\left [\cqs \left (\mathbf {x}\right )\right ]=\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y}\left [\prod _{i\in S}\chi _{i}^{q}\left (\mathbf {x}_{i}\right )\right ].\label {eq:comp1} \end {align} Claim \ref {claim:comutation silly} below shows that \[ \e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y}\left [\chi _{i}^{q}\left (\mathbf {x}_{i}\right )\right ]=\rho \chi _{i}^{p}\left (y_{i}\right ). \] By the independence of the random variables $\chi _{i}^{q}\left (\mathbf {x}_{i}\right )$ for any $\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y$ we obtain: \begin {align} \e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y}\left [\prod _{i\in S}\chi _{\left \{ i\right \} }^{q}\left (\mathbf {x}_{i}\right )\right ] & =\prod _{i\in S}\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y}=y}\left [\chi _{i}^{q}\left (\mathbf {x}_{i}\right )\right ]\label {eq:comp2}\\ & =\rho ^{\left |S\right |}\prod _{i\in S}\chi _{i}^{p}\left (y_{i}\right )=\rho ^{\left |S\right |}\chi _{S}\left (y\right ).\nonumber \end {align} Combining (\ref {eq:comp1}) with (\ref {eq:comp2}), we complete the proof.
END Proof
 
BEGIN \protect \claimname  \protect \let  
BEGIN Proof 
 Since the functions $\chi _{i}^{q},\chi _{i}^{p}$ depend only on the $i$th coordinate we may assume that $n=1$, and we shall write $\chi ^{p}=\chi _{1}^{p}$ as well as $\chi ^{q}=\chi _{1}^{q}$ for brevity. We shall start by showing (\ref {eq:evil expectation 1}), and the proof of (\ref {eq: Evil expectation 2}) will be similar. Let $h\in L^{2}\left (\left \{ 0,1\right \} ^{n},\mu _{p}\right )$ be the map \[ y\mapsto \e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y=}y}\left [\chi ^{q}\left (\mathbf {x}\right )\right ]. \] Note the space $L^{2}\left (\left \{ 0,1\right \} ,\mu _{p}\right )$ is a linear space of dimension $2$. We shall show that $h=\rho \chi ^{p}$ by showing that there are two independent linear functionals on the space $L^{2}\left (\left \{ 0,1\right \} ,\mu _{p}\right )$ that agree on the functions $h$ and $\rho \chi _{p}$. Namely, the first functional is the functional of evaluating at $0$, and the second functional is the expectation according to the $p$-biased distribution. Indeed, we may use the fact that elements $\boldsymbol {x,y}\sim D\left (q,p\right )$ satisfy $\boldsymbol {x}_{i}\le \boldsymbol {y}_{i}$ with probability $1$ to obtain: \[ \e _{\mathbf {x},\mathbf {y}\sim \left (\left \{ 0,1\right \} ,D\left (q,p\right )\right )|\,\mathbf {y}=0}\left [\chi ^{q}\left (\mathbf {x}\right )\right ]=\chi ^{q}\left (0\right )=\sqrt {\frac {q}{1-q}}=\rho \sqrt {\frac {p}{1-p}}=\rho \chi ^{p}\left (0\right ). \] On the other hand, \begin {align*} \e _{\z \sim \mu _{p}}\left [h\left (\z \right )\right ] & =\e _{\mathbf {z}\sim \mu _{p}}\left [\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {y=\mathbf {z}}}\left [\chi ^{q}\left (\mathbf {x}\right )\right ]\right ]\\ & =\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )}\left [\chi ^{q}\left (\mathbf {x}\right )\right ]=\e _{\mathbf {x}\sim \mu _{q}}\left [\chi _{q}\left (\mathbf {x}\right )\right ]=0\\ & =\e _{\z \sim \mu _{p}}\left [\rho \chi _{p}\left (\boldsymbol {\boldsymbol {z}}\right )\right ]. \end {align*} Since the expectation functional and the evaluating by 0 functionals are independent, and since the space $L^{2}\left (\left \{ 0,1\right \} ,\mu _{p}\right )$ is of dimension 2, we obtain $h=\rho \chi _{p}$. This completes the proof of (\ref {eq:evil expectation 1}). \par We prove (\ref {eq: Evil expectation 2}) in a similar fashion. Define $h\in L^{2}\left (\left \{ 0,1\right \} ,\mu _{q}\right )$ by \[ x\mapsto \e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {x}=x}\left [\chi ^{p}\left (\mathbf {y}\right )\right ]. \] Similarly to the proof of (\ref {eq:evil expectation 1}), it is enough to prove the identities \[ \e _{\mathbf {z}\sim \mu _{q}}\left (h\right )=0,h\left (1\right )=\rho \chi _{q}\left (1\right ). \] To prove the former, note that \[ \e _{\mathbf {z}\sim \mu _{q}}\left (h\left (\z \right )\right )=\e _{\mathbf {z}\sim \mu _{q}}\left [\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {x}=\mathbf {z}}\left [\chi ^{p}\left (\mathbf {y}\right )\right ]\right ]=\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )}\left [\chi ^{p}\left (\mathbf {y}\right )\right ]=0. \] To prove the latter, note that \[ h\left (1\right )=\e _{\mathbf {x},\mathbf {y}\sim D\left (q,p\right )|\,\mathbf {x}=1}\left [\chi ^{p}\left (\mathbf {y}\right )\right ]=\chi _{p}\left (1\right )=\rho \chi _{q}\left (1\right ). \]
END Proof
 
BEGIN \protect \examplename  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \remarkname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \remarkname  \protect \let  
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $\delta =\left \langle \mathsf {T}_{1-\epsilon }\left [F_{\epsilon }\right ],1-F_{1-\epsilon }\right \rangle >0$. We have \begin {align*} \Lambda _{\rho }\left (\mu ,\nu \right ) & =\left \langle \mathsf {T}_{\rho }\left [F_{\mu }\right ],F_{\nu }\right \rangle =\left \langle \mathsf {T}_{\rho }\left [F_{\mu }\right ],1\right \rangle +\left \langle \mathsf {T}_{\rho }\left [F_{\mu }\right ],F_{\nu }-1\right \rangle \\ & =\mu -\left \langle \mathsf {T}_{\rho }\left [F_{\mu }\right ],1-F_{\nu }\right \rangle \le \mu -\left \langle \mathsf {T}_{1-\epsilon }\left [F_{\epsilon }\right ],1-F_{1-\epsilon }\right \rangle \\ & =\mu -\delta . \end {align*} \par 
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN \protect \factname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \corollaryname  \protect \let  
BEGIN \protect \propositionname  \protect \let  
BEGIN \protect \lemmaname  \protect \let  
BEGIN \protect \claimname  \protect \let  
BEGIN Proof 
 Note that it follows from Jensen's inequality that the operator $\t _{\rho }$ on the space $L^{2}\left (\mathbb {R}^{n},\gamma \right )$ is a contraction. Indeed, for each function $h\in L^{2}\left (\mathbb {R}^{n},\gamma \right )$ we have \begin {align*} \left \Vert \t _{\rho }\left (h\right )\right \Vert ^{2} & =\e _{\boldsymbol {x}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [\left (\e _{\boldsymbol {y}\sim N_{\rho }\left (x\right )}\left [h\left (\boldsymbol {y}\right )\right ]\right )^{2}\right ]\\ & \le \left (\e _{\boldsymbol {x}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [\e _{\boldsymbol {y}\sim N_{\rho }\left (x\right )}\left [h\left (\boldsymbol {y}\right )\right ]\right ]\right )^{2}\\ & =\left (\e _{\boldsymbol {y}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [h\left (\boldsymbol {y}\right )\right ]\right )^{2}\\ & =\|h\|^{2}. \end {align*} Moreover, we note that by Parseval $\|\tilde {g}\|=\sum \hat {g}\left (S\right )^{2}=\sqrt {\e _{\mathbf {y\sim \mu }_{p}}\left [g\left (\boldsymbol {y}\right )^{2}\right ]}\le 1.$ Therefore, \begin {align*} \epsilon _{1} & =\left |\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \right |\\ & \le \left |\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\tilde {g}\right \rangle \right |\\ & +\left |\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \right |\\ (\text {By Cauchy-Schwarz}) & \le \left \Vert \t _{\rho }\left (\tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right )\right \Vert \left \Vert \tilde {g}\right \Vert +\left \Vert \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right )\right \Vert \left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert \\ (\text {Since }\t _{\rho }\text { is a contraction)} & \le \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert \left \Vert \tilde {g}\right \Vert +\left \Vert \mathrm {Chop}\left (\tilde {f}\right )\right \Vert \left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert \\ & \le \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert +\left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert . \end {align*} We may now apply Corollary \ref {cor:of invariance} with $\epsilon $ replacing $\eta $ and $\frac {\epsilon }{4}$ replacing $\epsilon $, to obtain that \begin {equation} \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert +\left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert <\frac {\epsilon }{2},\label {eq:chops are small} \end {equation} provided that $\delta $ is sufficiently small. This completes the proof of the claim.
END Proof
 
BEGIN \protect \claimname  \protect \let  
BEGIN Proof 
 Let $\epsilon >0$ and suppose that $\delta =\delta \left (\epsilon \right )$ is sufficiently small. Let $\tilde {f}$ be the Gaussian analogue of $f$ and let $\tilde {g}$ be the Gaussian analogue of $g.$ By Fact \ref {Fact: Fourier formula for gaussians} we have \[ \sum _{S\subseteq \left [n\right ]}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )=\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle . \] So our goal is to show that \begin {equation} \left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )<\epsilon ,\label {eq:Goal 1} \end {equation} provided that $\delta $ is sufficiently small. Let \[ \epsilon _{1}:=\left |\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \right |, \] and let \begin {align*} \epsilon _{2} & =\left |\Lambda _{\rho }\left (\mathbb {E}\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathbb {E}\left (\mathrm {Chop}\left (\tilde {g}\right )\right )\right )-\Lambda _{\rho }\left (\mathbb {E}\left [\tilde {f}\right ],\mathbb {E}\left [\tilde {g}\right ]\right )\right |\\ & =\left |\Lambda _{\rho }\left (\mathbb {E}\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathbb {E}\left (\mathrm {Chop}\left (\tilde {g}\right )\right )\right )-\Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )\right |. \end {align*} Applying Borrel\textquoteright s Theorem on the functions $\mathrm {Chop}\left (\tilde {f}\right ),\mathrm {Chop}\left (\tilde {g}\right ),$ we obtain \begin {equation} \left \langle \t _{\rho }\mathrm {Chop}\left (\tilde {f}\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \le \Lambda _{\rho }\left (\mathbb {E}\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathbb {E}\left (\mathrm {Chop}\left (\tilde {g}\right )\right )\right ),\label {eq:Borell} \end {equation} and hence \[ \left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle \le \Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )+\epsilon _{1}+\epsilon _{2}. \] So to complete the proof we need to show that $\epsilon _{1}+\epsilon _{2}<\epsilon $ provided that $\delta $ is sufficiently small. \begin {claim} Provided that $\delta $ is sufficiently small we have $\epsilon _{1}<\epsilon /2$. \end {claim} \par \begin {proof} Note that it follows from Jensen's inequality that the operator $\t _{\rho }$ on the space $L^{2}\left (\mathbb {R}^{n},\gamma \right )$ is a contraction. Indeed, for each function $h\in L^{2}\left (\mathbb {R}^{n},\gamma \right )$ we have \begin {align*} \left \Vert \t _{\rho }\left (h\right )\right \Vert ^{2} & =\e _{\boldsymbol {x}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [\left (\e _{\boldsymbol {y}\sim N_{\rho }\left (x\right )}\left [h\left (\boldsymbol {y}\right )\right ]\right )^{2}\right ]\\ & \le \left (\e _{\boldsymbol {x}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [\e _{\boldsymbol {y}\sim N_{\rho }\left (x\right )}\left [h\left (\boldsymbol {y}\right )\right ]\right ]\right )^{2}\\ & =\left (\e _{\boldsymbol {y}\sim \left (\mathbb {R}^{n},\gamma \right )}\left [h\left (\boldsymbol {y}\right )\right ]\right )^{2}\\ & =\|h\|^{2}. \end {align*} Moreover, we note that by Parseval $\|\tilde {g}\|=\sum \hat {g}\left (S\right )^{2}=\sqrt {\e _{\mathbf {y\sim \mu }_{p}}\left [g\left (\boldsymbol {y}\right )^{2}\right ]}\le 1.$ Therefore, \begin {align*} \epsilon _{1} & =\left |\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \right |\\ & \le \left |\left \langle \t _{\rho }\tilde {f},\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\tilde {g}\right \rangle \right |\\ & +\left |\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\tilde {g}\right \rangle -\left \langle \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right ),\mathrm {Chop}\left (\tilde {g}\right )\right \rangle \right |\\ (\text {By Cauchy-Schwarz}) & \le \left \Vert \t _{\rho }\left (\tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right )\right \Vert \left \Vert \tilde {g}\right \Vert +\left \Vert \t _{\rho }\left (\mathrm {Chop}\left (\tilde {f}\right )\right )\right \Vert \left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert \\ (\text {Since }\t _{\rho }\text { is a contraction)} & \le \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert \left \Vert \tilde {g}\right \Vert +\left \Vert \mathrm {Chop}\left (\tilde {f}\right )\right \Vert \left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert \\ & \le \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert +\left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert . \end {align*} We may now apply Corollary \ref {cor:of invariance} with $\epsilon $ replacing $\eta $ and $\frac {\epsilon }{4}$ replacing $\epsilon $, to obtain that \begin {equation} \left \Vert \tilde {f}-\mathrm {Chop}\left (\tilde {f}\right )\right \Vert +\left \Vert \tilde {g}-\mathrm {Chop}\left (\tilde {g}\right )\right \Vert <\frac {\epsilon }{2},\label {eq:chops are small} \end {equation} provided that $\delta $ is sufficiently small. This completes the proof of the claim. \end {proof} To finish the proof of the lemma it remains to prove the following claim. \begin {claim} Provided that $\delta $ is sufficiently small, we have $\epsilon _{2}<\epsilon /2.$ \par Choose $X\sim \left (\mathbb {R},\gamma \right ),Y\sim N_{\rho }\left (X\right ).$ Then $\Lambda _{\rho }\left (\mathbb {E}\left [\tilde {f}\right ],\mathbb {E}\left [\tilde {g}\right ]\right )$ is the probability of the event $X<t_{1},Y<t_{2}$ for the proper values of $t_{1},t_{2}.$ Similarly, $\Lambda _{\rho }\left (\mathbb {E}\left [\chop \left (\tilde {f}\right )\right ],\mathbb {E}\left [\chop \left (\tilde {g}\right )\right ]\right )$ is the probability of the event $X<t_{3},Y<t_{4}$ for the proper values of $t_{3},t_{4}.$ These events differ either if $X$ is in the interval whose endpoints are $t_{1},t_{3}$ or if $Y$ is in the interval whose endpoints are $t_{2},t_{4}.$ The Probability of the former event is $\left |\e \left [\chop \left (\tilde {f}\right )\right ]-\e \left [\tilde {f}\right ]\right |$, and the probability of the latter event is $|\e \left [\chop \left (\tilde {g}\right )\right ]-\e \left [\tilde {g}\right ]|.$ Therefore, a union bound implies that: \begin {align*} \epsilon _{2} & =\left |\Lambda _{\rho }\left (\mathbb {E}\left [\tilde {f}\right ],\mathbb {E}\left [\tilde {g}\right ]\right )-\Lambda _{\rho }\left (\mathbb {E}\left [\chop \left (\tilde {f}\right )\right ],\mathbb {E}\left [\chop \left (\tilde {g}\right )\right ]\right )\right |\\ & \le \left |\e \left [\chop \left (\tilde {f}\right )\right ]-\e \left [\tilde {f}\right ]\right |+|\e \left [\chop \left (\tilde {g}\right )\right ]-\e \left [\tilde {g}\right ]|\\ (\text {By Cauchy-Schwarz and \eqref {eq:chops are small}}) & \le \|\chop \left (\tilde {f}\right )-\tilde {f}\|+\|\chop \left (\tilde {g}\right )-\tilde {g}\|<\frac {\epsilon }{2}. \end {align*} \par \end {claim} \par 
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $\epsilon >0$, let $\delta _{1}=\delta _{1}\left (\epsilon \right )$ be sufficiently small, and let $\delta =\delta \left (\delta _{1}\right )$ be sufficiently small. Let $f'=\t _{q,1-\delta _{1}}\left (f\right ),g'=\t _{p,1-\delta _{1}}\left (g\right ),$ $\rho '=\frac {\rho }{\left (1-\delta _{1}\right )^{2}}.$ \par We assert that the functions $f'$ is $\left (\delta ,1-\delta _{1},\mu _{q}\right )$-smooth and the function $g'$ is $\left (\delta ,1-\delta _{1},\mu _{p}\right )$-smooth, provided that $\delta $ is small enough. Indeed, the functions $f'$ is $\left (\delta ,1-\delta _{1},\mu _{q}\right )$-smooth since: \[ \mathrm {Inf}_{i}\left [f'\right ]=\mathrm {Inf}_{i}^{\left (1-\delta _{1},q\right )}\left [f\right ]\le \mathrm {Inf}_{i}^{\left (1-\delta ,q\right )}\left [f\right ]<\delta , \] provided that $\delta \le \delta _{1},$ and \[ \left |\hat {f'}\left (S\right )\right |=\left |\left (1-\delta _{1}\right )^{\left |S\right |}\hat {f}\left (S\right )\right |\le \left (1-\delta _{1}\right )^{\left |S\right |}. \] The function $g$ is $\left (\delta ,1-\delta _{1},\mu _{p}\right )$-smooth for similar reasons. Provided that $\delta $ is small enough, Lemma \ref {lem:Counting Lemma 1} implies that \[ \left \langle \t _{\rho }f,g\right \rangle =\left \langle \t _{\rho '}f',g'\right \rangle \le \Lambda _{\rho '}\left (\mu _{q}\left (f'\right ),\mu _{p}\left (g'\right )\right )+\delta _{1}=\Lambda _{\rho '}\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )+\delta _{1}. \] By Lemma \ref {lem:Estimossel} we have \[ \Lambda _{\rho '}\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )<\Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )+\epsilon -\delta _{1}, \] provided that $\delta _{1}$ is sufficiently small. Hence \[ \left \langle \t _{\rho }f,g\right \rangle \le \Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )+\epsilon . \]
END Proof
 
BEGIN Proof 
 Let $\epsilon >0$, let $\delta _{1}=\delta _{1}\left (\epsilon \right )$ be sufficiently small, and let $\delta =\delta \left (\delta _{1}\right )$ be sufficiently small. Let \[ A_{1}=\left \{ i\in \left [n\right ]:\,\mathrm {Inf}_{i}^{\left (1-\delta _{1},q\right )}\left [f\right ]>\delta _{1}\right \} ,\,\,\,A_{2}=\left \{ i\in \left [n\right ]:\,\mathrm {Inf}_{i}^{\left (1-\delta _{1},q\right )}\left [g\right ]>\delta _{1}\right \} , \] let $A=A_{1}\cup A_{2},$ and set $B=\left [n\right ]\backslash A.$ Write $f'=\av _{A}\left (f\right ),g'=\av _{A}\left (g\right ).$ We have \[ \sum _{S\subseteq \left [n\right ]}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )=\sum _{S\subseteq B}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )+\sum _{S\cap A\ne \varnothing }\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right ). \] We shall now bound $\sum _{S\subseteq \left [n\right ]}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )$ by bounding each of the terms in the right hand side. \par \textbf {Upper bounding $\sum _{S\subseteq B}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )$} \par Since $f',g'$ satisfy the hypothesis of Lemma \ref {lem:Counting lemma 2} (with $\delta _{1}$ replacing $\delta $), we have \[ \sum _{S\subseteq B}\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right )=\sum _{S\subseteq \left [n\right ]}\rho ^{\left |S\right |}\hat {f}'\left (S\right )\hat {g'}\left (S\right )\le \Lambda _{\rho }\left (\mu _{q}\left (f'\right ),\mu _{p}\left (g'\right )\right )+\frac {\epsilon }{2}=\Lambda _{\rho }\left (\mu _{q}\left (f\right ),\mu _{p}\left (g\right )\right )+\frac {\epsilon }{2}, \] provided that $\delta _{1}$ is small enough. \par \textbf {Upper bounding} $\sum _{S\cap A\ne \varnothing }\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right ).$ \par By Cauchy Schwarz, we have \begin {align*} \sum _{S\cap A\ne \varnothing }\rho ^{\left |S\right |}\hat {f}\left (S\right )\hat {g}\left (S\right ) & \le \sum _{i\in A}\sum _{S\ni i}\rho ^{\left |S\right |}\left |\hat {f}\left (S\right )\hat {g}\left (S\right )\right |\le \sum _{i\in A}\sqrt {\sum _{S\ni i}\rho ^{\left |S\right |}\hat {f}\left (S\right )^{2}}\sqrt {\sum _{S\ni i}\rho ^{\left |S\right |}\hat {g}\left (S\right )^{2}}.\\ & =\sum _{i\in A}\sqrt {\mathrm {Inf}_{i}^{\left (\rho ,q\right )}\left (f\right )}\sqrt {\mathrm {Inf}_{i}^{\left (\rho ,p\right )}\left (g\right )}. \end {align*} Now note that \[ \max \left \{ \mathrm {Inf}_{i}^{\left (\rho ,q\right )}\left (f\right ),\mathrm {Inf}_{i}^{\left (\rho ,p\right )}\left (g\right )\right \} \le 1 \] for any $i\in \left [n\right ].$ Moreover, we have $\mathrm {Inf}_{i}^{\left (\rho ,q\right )}\left (f\right )\le \mathrm {Inf}_{i}^{\left (1-\delta ,q\right )}\left (f\right )$ and $\mathrm {Inf}_{i}^{\left (\rho ,p\right )}\left (f\right )\le \mathrm {Inf}_{i}^{\left (1-\delta ,p\right )}\left (f\right ),$ provided that $\delta <1-\rho .$ The hypothesis implies that \[ \max _{i\in \left [n\right ]}\min \left \{ \mathrm {Inf}_{i}^{\left (1-\delta ,q\right )}\left (f\right ),\mathrm {Inf}_{i}^{\left (1-\delta ,p\right )}\left (f\right )\right \} \le \delta . \] Therefore, \[ \sum _{i\in A}\sqrt {\mathrm {Inf}_{i}^{\left (\rho ,q\right )}\left (f\right )}\sqrt {\mathrm {Inf}_{i}^{\left (\rho ,p\right )}\left (g\right )}\le \left |A\right |\sqrt {\delta }. \] So this completes the proof provided that $\delta \le \frac {\epsilon ^{2}}{4\left |A\right |^{2}}$. We shall now complete the proof by showing that $\left |A\right |=O_{\delta _{1}}\left (1\right ).$ \par \textbf {Upper bounding $\left |A\right |$} \par We show that $\left |A_{1}\right |=O_{\delta _{1}}\left (1\right ),$ as the proof that $\left |A_{2}\right |=O_{\delta _{1}}\left (1\right )$ is similar. Note that the quantity $\sum _{i=1}^{n}\mathrm {Inf}_{i}^{\left (1-\delta _{1},q\right )}\left (f\right )$ is on the one hand bounded from below by $\left |A_{1}\right |\delta _{1},$ and on the other hand we have the following upper bound on it. \[ \sum _{i=1}^{n}\mathrm {Inf}_{i}^{\left (q,1-\delta _{1}\right )}\left (f\right )=\sum _{S\subseteq \left [n\right ]}\left (1-\delta _{1}\right )^{\left |S\right |}\left |S\right |\hat {f}\left (S\right )^{2}\le \sum _{s=1}^{\infty }s\left (1-\delta _{1}\right )^{s}=O_{\delta _{1}}\left (1\right ). \] Hence $\left |A_{1}\right |=O_{\delta _{1}}\left (1\right ).$ This completes the proof of the proposition.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN Proof 
 Let $\delta _{1}=\delta _{1}\left (\epsilon \right )$ be sufficiently small, let $\delta _{2}=\delta _{2}\left (\delta _{1}\right )$ be sufficiently small, let $j=j\left (\delta _{2}\right )$ be sufficiently large, and let $\delta =\delta \left (j,\delta _{1},\epsilon \right )$ be sufficiently small. By Theorem \ref {thm:Jones regularity}, there exists a set $J$ of size at most $j$, such that the for a random $\mathbf {x}\sim \left (\left \{ 0,1\right \} ^{J},\mu _{q}\right )$ the function $f_{J\to \mathbf {x}}$ does not have $\left (1-\delta _{2},\delta _{2},\mu _{q}\right )$-small noisy influences with probability at most $\delta _{2}$. \par Let $Q\subseteq \left \{ 0,1\right \} ^{J}$ be the set of `quasirandom parts' consisting of all $x\in \left \{ 0,1\right \} ^{J},$ such that $f_{J\to x}$ has $\left (1-\delta _{2},\delta _{2},\mu _{q}\right )$-small noisy influences. So $\Pr _{\boldsymbol {x}\sim \left \{ 0,1\right \} ^{J}}\left [\x \in Q\right ]>1-\delta _{2}.$ \par Let $N\subseteq \left \{ 0,1\right \} ^{J}$ be the set of `negligible parts' consisting of all $x\in \left \{ 0,1\right \} ^{J},$ such that $\mu _{q}\left (f_{J\to x}\right )<\epsilon /2.$ Note that \begin {align*} \Pr _{\x \sim \mu _{q}}\left [f\left (\x \right )=1\,|\,\x _{J}\in N\right ] & \le \max _{y\in N}\Pr _{\mathbf {x}\sim \mu _{q}}\left [f\left (\mathbf {x}\right )=1\,|\,\mathbf {x}_{J}=y\right ]\le \frac {\epsilon }{2}. \end {align*} \par Let $A$ be the up-closure of $Q/N$, i.e. the set of all $x\in \left \{ 0,1\right \} ^{J},$ such that there exists some $y\in Q\backslash N$ that satisfies $\forall i:\,y_{i}\le x_{i}$. Finally, we let $g\colon \left \{ 0,1\right \} ^{J}\to \left \{ 0,1\right \} $ be the indicator function of $A.$ \par \textbf {Showing that $\Pr _{\x \sim \mu _{q}}\left [f\left (\x \right )>g\left (\x \right )\right ]<\epsilon $.} \par For each $\mathbf {x}\in \left \{ 0,1\right \} ^{n}$ with $f\left (\x \right )>g\left (\x \right )$ we have $g\left (x\right )=0$, and particularly $x\notin Q\backslash N.$ So we either have $\mathbf {x}\notin Q$ or we have the unlikely event that $f\left (\mathbf {x}\right )=1$ although $\mathbf {x}_{J}\in N.$ The former event occurs with probability at most $\delta _{2},$ and the latter event occurs with probability at most $\frac {\epsilon }{2}$ so \[ \Pr _{\x \sim \mu _{q}}\left [f\left (\x \right )>g\left (\x \right )\right ]<\frac {\epsilon }{2}+\delta _{2}<\epsilon , \] provided that $\delta _{2}$ is sufficiently small. \par \textbf {Showing that $\Pr _{\x \sim \mu _{p}}\left [f\left (\x \right )<g\left (\x \right )\right ]<\epsilon $.} \par Let $y\in A,$ let $x\in Q\backslash N$ be with $\forall i:\,x_{i}\le y_{i}$, and let $\rho =\sqrt {\frac {q\left (1-p\right )}{p\left (1-q\right )}}$. Since $x$ is in $Q$, we may apply Proposition \ref {prop:Counting lemma for the approximation by junta thm} to obtain that \begin {equation} \left \langle \t ^{q\to p}f_{J\to x},f_{J\to y}\right \rangle \le \Lambda _{\rho }\left (\mu _{q}\left (f_{J\to x}\right ),\mu _{p}\left (f_{J\to y}\right )\right )+\delta _{1},\label {eq:sec4 1} \end {equation} provided that $\delta _{2}$ is sufficiently small. \par This gives us an upper bound on $\left \langle \t ^{q\to p}f_{J\to x},f_{J\to y}\right \rangle .$ On the other hand we may use the fact that $f$ is almost monotone to obtain a lower bound on $\left \langle \t ^{q\to p}f_{J\to x},f_{J\to y}\right \rangle $ as follows. Note that we have \begin {align} \delta & \ge \left \langle \t ^{q\to p}f,1-f\right \rangle =\Pr _{\mathbf {z,}\mathbf {w}\sim D\left (q,p\right )}\left [f\left (\mathbf {z}\right )=1,f\left (\mathbf {w}\right )=0\right ]\nonumber \\ & \ge \Pr _{\mathbf {z,}\mathbf {w}\sim D\left (q,p\right )}\left [\mathbf {z}_{J}=x,\mathbf {w}_{J}=y\right ]\Pr _{\mathbf {z,}\mathbf {w}\sim D\left (q,p\right )}\left [f_{J\to x}\left (\mathbf {z}_{\left [n\right ]\backslash J}\right )=1,f_{J\to y}\left (\mathbf {w}_{\left [n\right ]\backslash J}\right )=0\right ]\label {eq:Theorem 4.1}\\ & =\Pr _{\mathbf {x},\mathbf {y}\sim \left (\left \{ 0,1\right \} ^{J},D\left (q,p\right )\right )}\left [\mathbf {x}=x,\mathbf {y}=y\right ]\left \langle \t ^{q\to p}f_{J\to x},1-f_{J\to y}\right \rangle .\nonumber \end {align} Thus, \begin {align} \left \langle \t ^{q\to p}f_{J\to x},f_{J\to y}\right \rangle & =\left \langle \t ^{q\to p}f_{J\to x},1\right \rangle -\left \langle \t ^{q\to p}f_{J\to x},1-f_{J\to y}\right \rangle \nonumber \\ & \ge \mu _{q}\left (f_{J\to x}\right )-\frac {\delta }{\Pr _{\mathbf {x},\mathbf {y}\sim \left (\left \{ 0,1\right \} ^{J},D\left (q,p\right )\right )}\left [\mathbf {x}=x,\mathbf {y}=y\right ]}\label {eq: sec 4 2}\\ & \ge \mu _{q}\left (f_{J\to x}\right )-\delta _{1},\nonumber \end {align} provided that $\delta =\delta \left (\delta _{1},j,\epsilon \right )$ is small enough. Combining (\ref {eq:sec4 1}) and (\ref {eq: sec 4 2}) we obtain \[ \Lambda _{\rho }\left (\mu _{q}\left (f_{J\to x}\right ),\mu _{p}\left (f_{J\to y}\right )\right )\ge \mu _{q}\left (f_{J\to x}\right )-2\delta _{1}. \] \par By Lemma \ref {lem:Estimate from Borell's Thm} we have $\mu _{p}\left (f_{J\to y}\right )>1-\frac {\epsilon }{2}$ provided that $\delta _{1}$ is small enough (note that $\mu _{q}\left (f_{J\to x}\right )>\epsilon /2,$ since $x\notin N$). \par This shows that any $y$ with $g\left (y\right )=1,f\left (y\right )=0$ satisfies the unlikely event that $f\left (y\right )=0$ while $\mu _{p}\left (f_{J\to y_{J}}\right )>1-\epsilon /2.$ Since a random $\mathbf {y}\sim \mu _{p}$ satisfies this event with probability at most $\epsilon $, we obtain $\Pr _{\boldsymbol {y}\sim \mu _{p}}\left [f\left (\boldsymbol {y}\right )<g\left (\boldsymbol {y}\right )\right ]<\epsilon $. This completes the proof of the theorem.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $\delta _{1}=\delta _{1}\left (\epsilon \right )$ be sufficiently small, and let $\delta =\delta \left (\delta _{1},j\right )$ be sufficiently small. Similarly to (\ref {eq:Theorem 4.1}) we have \begin {align*} \delta & \ge \left \langle \t ^{q\to p}f,1-g\right \rangle \ge \left \langle \t ^{q\to p}f_{J\to x},1-g_{J\to y}\right \rangle \Pr _{\mathbf {x},\mathbf {y}\sim \left (\left \{ 0,1\right \} ^{J},D\left (q,p\right )\right )}\left [\mathbf {x}=x,\mathbf {y}=y\right ]. \end {align*} Similarly to (\ref {eq: sec 4 2}) we have \[ \left \langle \t ^{q\to p}f_{J\to x},g_{J\to y}\right \rangle \ge \mu _{q}\left (f_{J\to x}\right )-\delta _{1}, \] provided that $\delta _{1}$ is small enough. Similarly to (\ref {eq:sec4 1}), we have \[ \left \langle \t ^{q\to p}f_{J\to x},g_{J\to y}\right \rangle \le \Lambda _{\rho }\left (\mu _{q}\left (f_{J\to x}\right ),\mu _{p}\left (g_{J\to y}\right )\right )+\delta _{1}. \] Hence, \[ \Lambda _{\rho }\left (\mu _{q}\left (f_{J\to x}\right ),\mu _{p}\left (g_{J\to y}\right )\right )\ge \mu _{q}\left (f_{J\to x}\right )-2\delta _{1}. \] As in the proof of Theorem (\ref {thm:Monotone approximation}), we may now apply Lemma \ref {lem:Estimate from Borell's Thm} to complete the proof.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN Proof 
 Let $\delta _{1}=\delta _{1}\left (\epsilon \right )$ be sufficiently small, let $j=j\left (\delta _{1},\epsilon \right )$ be sufficiently large, and let $\delta =\delta \left (j,\delta _{1},\epsilon \right )$ be sufficiently small. By Theorem \ref {thm:Jones regularity}, there exists a set $J$ of size at most $j$, such that for a random $\mathbf {x}\sim \left (\left \{ 0,1\right \} ^{J},\mu _{q}\right )$ the function $f_{J\to x}$ does not have $\left (1-\delta _{1},\delta _{1},\mu _{q}\right )$-small noisy influences with probability at most $\delta _{1}$. Let $Q\subseteq \left \{ 0,1\right \} ^{J}$ be the set of `quasirandom elements' consisting of all $x\in \left \{ 0,1\right \} ^{J},$ such that $f_{J\to x}$ has $\left (1-\delta _{1},\delta _{1},\mu _{q}\right )$-small noisy influences. Let $A$ be the up-closure of $Q$. Since $A$ is monotone, we have \[ \mu _{p}\left (A\right )\ge \mu _{q}\left (A\right )\ge 1-\delta _{2}. \] Moreover, the fact that $f$ is $\left (\left \lceil \frac {1}{\delta }\right \rceil ,\delta ,\mu _{q}\right )$-regular implies that \[ \mu _{q}\left (f_{J\to x}\right )\ge \epsilon -\delta >\epsilon /2 \] for each $x\in \left \{ 0,1\right \} ^{J},$ provided that $\delta <\min \left \{ \frac {\epsilon }{2},\frac {1}{j}\right \} .$ By Lemma \ref {lem:Section 4} (applied with $\epsilon /2$ rather than $\epsilon $), we obtain that $\mu _{p}\left (g_{J\to x}\right )>1-\epsilon /2$ for all $x\in A.$ So this implies that \[ \mu _{p}\left (g\right )\ge \left (1-\epsilon /2\right )\mu _{p}\left (A\right )\ge \left (1-\epsilon /2\right )\left (1-\delta /2\right )>1-\epsilon . \] This completes the proof of the theorem.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 We have \begin {align*} \mu _{q}\left (f_{\mathcal {F}}\right ) & =\mathbb {E}_{\mathbf {x}\sim \mu _{q}}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\right ]=\Pr \left [\mathbf {x}\ge k\right ]\mathbb {E}_{\mathbf {x}\sim \mu _{q}^{\ge k}}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\right ]+\Pr \left [\mathbf {x}<k\right ]\mathbb {E}_{\mathbf {x}\sim \mu _{q}^{<k}}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\right ].\\ & =\Pr _{\mathbf {x}\sim \mu _{q}}\left [\left |\mathbf {x}\right |\ge k\right ]\mathbb {E}_{\mathbf {x}\sim \mu _{q}^{\ge k}}\left [\Pr _{\mathbf {A}\sim \binom {\mathbf {x}}{k}}\left [\mathbf {A}\in \mathcal {F}\right ]\right ]. \end {align*} However, whenever we choose $\mathbf {x}\sim \mu _{q}^{\ge k}$, and an $\mathbf {A}\sim \binom {\mathbf {x}}{k}$, we obtain a set $\mathbf {A}$ that is distributed uniformly in $\binom {\left [n\right ]}{k}$. Thus, \begin {equation} \mu _{q}\left (f_{\mathcal {F}}\right )=\Pr _{\mathbf {x}\sim \mu _{q}}\left [\left |\mathbf {x}\right |\ge k\right ]\mu \left (\mathcal {F}\right ).\label {eq:computation ff} \end {equation} The lemma follows by combining (\ref {eq:computation ff}) with the fact that $\Pr _{\mathbf {x}\sim \mu _{q}}\left [\left |\mathbf {x}\right |\ge k\right ]$ tends to 1 as $n$ tends to infinity.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 As in Lemma \ref {lem:measure of ff}, we have \begin {equation} \mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to x}\right )=\Pr _{\mathbf {y}\sim \left (\mu _{q},J\to x\right )}\left [\left |\mathbf {y}\right |\ge k\right ]\underset {\mathbf {y}\sim \left (\mu _{q}^{\ge k},J\to x\right )}{\mathbb {E}}\left [f_{\mathcal {F}}\left (\mathbf {y}\right )\right ].\label {eq:restricted ff 1} \end {equation} Note that \[ \Pr _{\mathbf {y}\sim \left (\mu _{q},J\to x\right )}\left [\left |\mathbf {y}\right |\ge k\right ]=1-o\left (1\right ), \] where the $o\left (1\right )$ is with respect to $n$ tending to infinity. So to complete the proof it remains show that \[ \underset {\mathbf {y}\sim \left (\mu _{q}^{\ge k},J\to x\right )}{\mathbb {E}}\left [f_{\mathcal {F}}\left (\mathbf {y}\right )\right ]=\left (1+o\left (1\right )\right )\underset {\mathbf {C}\sim \left (\mathcal {P}\left (x\right ),\mu _{\lambda }\right )}{\mathbb {E}}\left [\mu \left (\mathcal {F}_{J\to \mathbf {C}}\right )\right ]. \] Choose $\mathbf {y}\sim \left (\mu _{q}^{\ge k},J\to x\right ),$$\mathbf {A\sim }\binom {\mathbf {y}}{k},$ then \textbf {$\mathbf {A}\cap J$ }is equal to some subset $\mathbf {C}$ of $\mathbf {x}.$ Note also that the conditional distribution of $\mathbf {A}$ given that $\mathbf {C=}C$ is the distribution of a uniformly random element of $\binom {\left [n\right ]}{k}$ that intersects $J$ at the set $C.$ Therefore, \begin {align} \underset {\mathbf {y}\sim \left (\mu _{q}^{\ge k},J\to x\right )}{\mathbb {E}}\left [f_{\mathcal {F}}\left (\mathbf {y}\right )\right ] & =\Pr _{\mathbf {y}\sim \left (\mu _{q}^{\ge k},J\to x\right ),\mathbf {A}\sim \binom {\mathbf {y}}{k}}\left [A\in \mathcal {F}\right ]\nonumber \\ & \sum _{C\subseteq x}\Pr \left [\mathbf {C}=C\right ]\Pr \left [\mathbf {A\in \mathcal {F}\,|\,}\mathbf {C}=C\right ]\nonumber \\ & =\sum _{C\subseteq x}\Pr \left [\mathbf {C}=C\right ]\mu \left (\mathcal {F}_{J}^{C}\right ).\label {eq:restricted ff2} \end {align} So to complete the proof it remains to show that \begin {equation} \Pr \left [\mathbf {C}=C\right ]=\lambda ^{\left |C\right |}\left (1-\lambda \right )^{\left |x\right |\backslash \left |C\right |}\left (1+o\left (1\right )\right ).\label {eq:restricted ff3} \end {equation} Indeed, with high probability $\left |\mathbf {y}\right |=qn\left (1+o\left (1\right )\right ),$ and the conditional probability that $\mathbf {C}=C$ given that $\left |\mathbf {y}\right |=s$ is \[ \frac {\left |\left \{ S\in \binom {\mathbf {y}}{k}\mbox { that satisfy }\left |S\cap x\right |=C\right \} \right |}{\left |\binom {\mathbf {y}}{k}\right |}=\frac {\binom {s-\left |x\right |}{k-\left |C\right |}}{\binom {s}{k}}=\left (\frac {k}{s}\right )^{\left |C\right |}\left (1-\frac {k}{s}\right )^{\left |x\right |\backslash \left |C\right |}\left (1+o\left (1\right )\right ). \] Let $\mathbf {s}=\left |\mathbf {y}\right |.$ Thus, \begin {align*} \Pr \left [\mathbf {C}=C\right ] & =\mathbb {E}_{\mathbf {s}}\left [\Pr \left [\mathbf {C}=C\,|\,\mathbf {s}\right ]\right ]=\mathbb {E}_{\mathbf {s}}\left [\left (\frac {k}{\mathbf {s}}\right )^{\left |C\right |}\left (1-\frac {k}{\mathbf {s}}\right )^{\left |x\right |\backslash \left |C\right |}\left (1+o\left (1\right )\right )\right ]\\ & =\lambda ^{\left |C\right |}\left (1-\lambda \right )^{\left |x\right |\backslash \left |C\right |}\left (1+o\left (1\right )\right ), \end {align*} where the last equality follows from the fact that $\frac {k}{\mathbf {s}}=\lambda \left (1+o\left (1\right )\right )$ with high probability. This completes the proof of the lemma.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Fix $\epsilon >0,$ let $n_{0}$ be sufficiently large, and let $\mathcal {F}\subseteq \binom {\left [n\right ]}{k},$ be as in the hypothesis of the lemma. Let $B\subseteq J\subseteq \left [n\right ]$ be sets, such that $\left |J\right |\le \left \lceil \frac {1}{\epsilon }\right \rceil $. By Lemma \ref {lem:measure of ff} \begin {equation} \left |\mu \left (\mathcal {F}\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right |<\frac {\epsilon }{4},\label {eq:regularity k large1} \end {equation} provided that $n_{0}$ is large enough. By Lemma \ref {lem:measure of restricted ff} \begin {equation} \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\mu \left (\mathcal {F}_{J}^{C}\right )\right |<\frac {\epsilon }{4},\label {eq:regularity k large2} \end {equation} provided that $n_{0}$ is large enough. \par By hypothesis \begin {align} \left |\mu \left (\mathcal {F}\right )-\mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\mu \left (\mathcal {F}_{J}^{C}\right )\right | & =\left |\mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\left (\mu \left (\mathcal {F}_{J}^{C}\right )-\mu \left (\f \right )\right )\right |\label {eq:regularity k large3}\\ & \le \mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\frac {\epsilon }{2}=\frac {\epsilon }{2}.\nonumber \end {align} Thus, \begin {align*} \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right | & \le \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\mu \left (\mathcal {F}_{J}^{C}\right )\right |\\ & +\left |\mathbb {E}_{\mathbf {C}\sim \left (\p \left (x\right ),\mu _{\frac {k}{qn}}\right )}\mu \left (\mathcal {F}_{J}^{C}\right )-\mu \left (\mathcal {F}\right )\right |+\left |\mu \left (\mathcal {F}\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right |\\ & <\frac {\epsilon }{4}+\frac {\epsilon }{4}+\frac {\epsilon }{2}=\epsilon . \end {align*} This completes the proof that $f_{\f }$ is $\left (\left \lceil \frac {1}{\epsilon }\right \rceil ,\epsilon ,\mu _{q}\right )$-regular.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $J$ be of size at most $\left \lceil \frac {1}{\epsilon }\right \rceil ,$ and let $B\subseteq J.$ We have \begin {equation} \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right |\le \left |\mu \left (\mathcal {F}_{J}^{\varnothing }\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right |+\left |\mu \left (\mathcal {F}_{J}^{\varnothing }\right )-\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )\right |.\label {eq:k small 1} \end {equation} We shall complete the proof by giving an upper bound of $\frac {\epsilon }{2}$ for each of the summands in the right hand side of (\ref {eq:k small 1}). \par \textbf {Showing that }$\left |\mu \left (\mathcal {F}_{J}^{\varnothing }\right )-\mu _{q}\left (f_{\mathcal {F}}\right )\right |\le \frac {\epsilon }{2}.$ \par By decreasing $\delta $ if necessary we may assume that $n$ is as large as we wish. Therefore Lemma \ref {lem:measure of ff} implies that \[ \left |\mu _{q}\left (f_{\mathcal {F}}\right )-\mu \left (\mathcal {F}\right )\right |<\frac {\epsilon }{4}, \] provided that $\delta $ is small enough. Now note that \begin {align} \mu \left (\mathcal {F}\right ) & =\sum _{B\subseteq J}\Pr _{\mathbf {A}\sim \binom {\left [n\right ]}{k}}\left [\mathbf {A}\cap J=B\right ]\mu \left (\mathcal {F}_{J}^{B}\right )\label {eq:k divided by n is negligible}\\ & =\left (1-O\left (\frac {k}{n}\right )\right )\mu \left (\mathcal {F}_{J}^{\varnothing }\right )+\sum _{\varnothing \ne B\subseteq J}O\left (\left (\frac {k}{n}\right )^{\left |B\right |}\right )\mu \left (\mathcal {F}_{J}^{B}\right ).\nonumber \end {align} So provided that $\delta $ is small enough, we have \[ \left |\mu _{q}\left (f_{\mathcal {F}}\right )-\mu \left (\mathcal {F}_{J}^{\varnothing }\right )\right |\le \left |\mu _{q}\left (f_{\mathcal {F}}\right )-\mu \left (\mathcal {F}\right )\right |+\left |\mu \left (\mathcal {F}\right )-\mu \left (\mathcal {F}_{J}^{\varnothing }\right )\right |\le \frac {\epsilon }{4}+O_{\epsilon }\left (\frac {k}{n}\right )<\epsilon /2. \] \textbf {Showing that} $\left |\mu \left (\mathcal {F}_{J}^{\varnothing }\right )-\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )\right |<\frac {\epsilon }{2}.$ \par By Lemma \ref {lem:k small} \[ \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\sum _{C\subseteq B}\left (\frac {k}{qn}\right )^{\left |C\right |}\left (1-\frac {k}{qn}\right )^{\left |B\right |\backslash \left |C\right |}\mu \left (\mathcal {F}_{J}^{C}\right )\right |\le \frac {\epsilon }{4}, \] provided that $n$ is large enough. Now note that similarly to (\ref {eq:k divided by n is negligible}) we have \[ \sum _{C\subseteq B}\left (\frac {k}{qn}\right )^{\left |C\right |}\left (1-\frac {k}{qn}\right )^{\left |B\right |\backslash \left |C\right |}\mu \left (\mathcal {F}_{J}^{C}\right )=\mu \left (\mathcal {F}_{J}^{\varnothing }\right )+O_{\epsilon }\left (\frac {k}{n}\right ). \] Thus, \[ \left |\mu _{q}\left (\left (f_{\mathcal {F}}\right )_{J\to B}\right )-\mu \left (\mathcal {F}_{J}^{\varnothing }\right )\right |\le \frac {\epsilon }{4}+O_{\epsilon }\left (\frac {k}{n}\right )<\frac {\epsilon }{2}, \] provided that $\delta $ is small enough. This completes the proof of the lemma.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 We show that the stronger statement that for each value $y$ of $\mathbf {y}$, we obtain that if we choose conditionally $\mathbf {x},\mathbf {y}\sim D\left (q,p\right )$ given that $\mathbf {y}=y,$ then \begin {equation} \mathbb {E}_{\mathbf {x}}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\left (1-\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\left (y\right )\right )\right ]\le \delta .\label {eq:stronger lemma stability} \end {equation} This clearly holds if $\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\left (y\right )=1$. So suppose that $\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\left (y\right )=0,$ we also suppose that $\left |y\right |\ge k$ for otherwise we would have $\left |\mathbf {x}\right |<k$, and hence $f_{\mathcal {F}}\left (\mathbf {x}\right )=0.$ Now note that \begin {align*} \mathbb {E}_{\mathbf {x}}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\right ] & \le \mathbb {E}_{\mathbf {x}}\left [\Pr _{A\sim \binom {\mathbf {x}}{k}}\left [A\in \mathcal {F}\right ]\,|\,\left |\mathbf {x}\right |\ge k\right ]=\Pr _{\mathbf {A}\sim \binom {y}{k}}\left [\mathbf {A}\in \mathcal {F}\right ]=f_{\mathcal {F}}\left (y\right )\le \delta . \end {align*} This completes the proof of the lemma.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $q=\frac {k}{n}+\frac {\epsilon }{2},$ and note that by Lemma \ref {lem:ff cut delta ff are stable} we have \[ \mathbb {E}_{\mathbf {x,y}\sim D\left (q,p\right )}\left [f_{\mathcal {F}}\left (\mathbf {x}\right )\left (1-\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\left (\mathbf {y}\right )\right )\right ]\le \delta . \] By decreasing $\delta $ if necessary, we may assume that $n$ is sufficiently large for Lemma \ref {lem:measure of ff} to imply that $\mu _{q}\left (f_{\mathcal {F}}\right )\ge \frac {\epsilon }{2}$. By Theorem \ref {thm:Robust version of Friedgut-Kalai Theorem} (applied with $\frac {\epsilon }{2}$ rather than $\epsilon $) we have $\mu _{p}\left (f_{\mathcal {F}}\right )\ge 1-\frac {\epsilon }{2}>1-\epsilon $, provided that $\delta $ is small enough. This completes the proof of the lemma.
END Proof
 
BEGIN \protect \corollaryname  \protect \let  
BEGIN Proof 
 By Lemma \ref {lem:mu one s large}, we have $\mu _{\frac {1}{h}}\left (\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right )>1-\frac {\epsilon }{2}$ provided that $\delta $ is small enough. Also note that we may assume that $n$ is sufficiently large by decreasing $\delta $ if necessary. \par We shall now define a coupling between $\frac {1}{h}$-biased matching, and $\left (\frac {1}{h},k\right )$-biased matchings as follows. We choose a $\frac {1}{h}$-biased matching $\mathbf {M}_{1},\mathbf {M}_{2},\ldots ,\mathbf {M}_{h}$, and we then let $\mathbf {M}_{1}',\ldots ,\mathbf {M}_{s}'$ to be equal to the original $\frac {1}{s}$-biased matching if the (likely) event $\forall i:\,\left |\mathbf {M}_{i}\right |\ge k$ occurred, and we let it be equal to a new $\left (\frac {1}{h},k\right )$-biased matching otherwise. Note that \[ \Pr \left [\mathbf {M}_{1}\in \mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right ]=\mu _{\frac {1}{h}}\left (\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right ), \] and that \[ \Pr \left [\mathbf {M}'_{1}\in \mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right ]=\mu _{\frac {1}{h},k}^{\mbox {matching}}\left (\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right ). \] Thus, \[ \mu _{\frac {1}{h},k}^{\mbox {matching}}\left (\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right )\ge \mu _{\frac {1}{h}}\left (\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}}\right )\right )-\Pr \left [\mathbf {M}_{1}\ne \mathbf {M}_{1}'\right ]\ge 1-\epsilon , \] provided that $n$ is sufficiently large to imply $\Pr \left [\mathbf {M}_{1}\ne \mathbf {M}_{1}'\right ]<\frac {\epsilon }{2}.$
END Proof
 
BEGIN Proof 
 [Proof of Theorem \ref {thm:Counting matchings}] Let $\mathcal {F}_{1}\subseteq \binom {\left [n\right ]}{k_{1}},\ldots ,\mathcal {F}_{h}\subseteq \binom {\left [n\right ]}{k_{h}}$ be some families that satisfy the hypothesis of the theorem, let $\delta '=\delta '\left (\epsilon \right )$ be sufficiently small, and let $\delta =\frac {\left (\delta '\right )^{h}}{3}$. By Corollary \ref {cor:mu matching large}, \[ \mu _{\frac {1}{h},k}^{\mbox {matching}}\left (\mathrm {Cut}_{\delta '}\left (f_{\mathcal {F}_{i}}\right )\right )>1-\frac {1}{2h} \] for each $i$, provided that $\delta '$ is small enough. A union bound implies that if we choose a $\left (\frac {1}{h},k\right )$-biased matching $\mathbf {A}_{1},\ldots ,\mathbf {A}_{h},$ then the event $\forall i\,\mathrm {Cut}_{\delta '}\left (f_{\mathcal {F}_{i}}\right )\left (\mathbf {A}_{i}\right )=1$ happens with probability greater than $\frac {1}{2}.$ Now choose independently a matching $\mathbf {M}_{i}\sim \binom {\mathbf {A}_{i}}{k_{i}}.$ For each choice of values $A_{1},\ldots ,A_{h}$ of the sets $\boldsymbol {A}_{1},\ldots ,\boldsymbol {A}_{h}$, we obtain that the events $\left \{ \mathbf {M}_{i}\in \mathcal {F}_{i}|\boldsymbol {A}_{i}=A_{i}\right \} _{i=1}^{h}$ are independent. Therefore \[ \Pr \left [\forall i:\,\boldsymbol {M}_{i}\in \f _{i}\right ]\ge \Pr \left [\forall i\,\mathrm {Cut}_{\delta }\left (f_{\mathcal {F}_{i}}\right )\left (\mathbf {A}_{i}\right )=1\right ]\delta '^{h}\ge \frac {\delta '^{h}}{2}=\delta . \] This completes the proof of the theorem, since the hypergraph $\left \{ \mathbf {M}_{1},\ldots ,\mathbf {M}_{h}\right \} $ is a uniformly random matching.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN \protect \definitionname  \protect \let  
BEGIN \protect \propositionname  \protect \let  
BEGIN Proof 
 [Proof of Theorem \ref {thm:Counting expanded hypergraphs} ] Let $\boldsymbol {E}_{i},\boldsymbol {C},\boldsymbol {D}_{i}$ be as above. Our goal is to show that the families $\left (\mathcal {F}_{i}\right )_{\mathbf {C}}^{\mathbf {E}_{i}}$ cross contain the uniformly random matching $\mathbf {D}_{1},\ldots ,\mathbf {D}_{h}$ with probability $\ge \delta $. Noting that the size of $\mathbf {C}$ is fixed, the following observations are easy to verify provided that $\delta $ is sufficiently small: \par \begin {itemize} \item Proposition \ref {prop:fairness}, implies that the set $\mathbf {C}$ is $\frac {1}{2}$-fair with probability at least $\frac {1}{2}$. For any such $\mathbf {C}$ the measure of the family $\left (\mathcal {F}_{i}\right )_{\mathbf {C}}^{\mathbf {E}_{i}}\subseteq \binom {\left [n\right ]\backslash \mathbf {C}}{k_{i}-\left |E_{i}\right |}$ is at least $\frac {\epsilon }{2}$. \item For any $i$ such that $\frac {k_{i}}{n}<\delta ,$ we have $\frac {k_{i}-\left |E_{i}\right |}{n-\left |\mathbf {C}\right |}<2\delta .$ \item If $\mathcal {F}_{i}$ is $\left (\left \lceil \frac {1}{\delta }\right \rceil ,\delta \right )$-regular, then $\left (\mathcal {F}_{i}\right )_{\mathbf {C}}^{\mathbf {E}_{i}}$ is $\left (\left \lceil \frac {1}{2\delta }\right \rceil ,2\delta \right )$-regular. \end {itemize} We shall also assume that the $\delta $ of this lemma is small enough for Theorem \ref {thm:Counting matchings} to hold with $2\delta $ replacing $\delta ,$ and $\frac {\epsilon }{2}$ replacing $\epsilon .$ These observations allow us to apply Theorem \ref {thm:Counting matchings}, and to deduce that for each set $C'$ that is $\frac {1}{2}$-fair for $\mathcal {F}$, and for each set $E'_{i}\in \binom {C'}{\left |E_{i}\right |}$ we have \[ \Pr \left [\forall i:\,\mathbf {D}_{i}\in \mathcal {F}_{C'}^{E_{i}}\right ]>2\delta . \] Therefore, \begin {align*} \Pr \left [\forall i:\,\mathbf {D}_{i}\in \mathcal {F}_{C'}^{E_{i}}\right ] & \ge \Pr \left [\mathbf {C}\mbox { is }\frac {1}{2}\text {-fair}\right ]\Pr \left [\forall i:\,\mathbf {D}_{i}\in \mathcal {F}_{\mathbf {C}}^{\mathbf {E}_{i}}\,|\,\mathbf {C}\text { is }\frac {1}{2}\text {-fair}\right ]>\delta . \end {align*} This completes the proof of the theorem.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 We start by showing that if (2) does not hold, then (1) does not holds. By hypothesis, there exist a trace $\left \{ C_{1},\ldots ,C_{h}\right \} $ of $\mathcal {H}$ in $\mathcal {G}$ whose center is of size at most $s.$ Let $B_{1}\in \binom {\left [n\right ]\backslash J}{k-\left |C_{1}\right |},\ldots ,B_{h}\in \binom {\left [n\right ]\backslash J}{k-\left |C_{h}\right |}$ be some pairwise disjoint sets (such sets exist provided that $\delta $ is large enough). Then the hypergraph $\left \{ C_{1}\cup B_{1},\ldots ,C_{h}\cup B_{h}\right \} $ is contained in $\mathcal {\left \langle \mathcal {G}\right \rangle }$, it is the resolution of the hypergraph $\mathcal {H}$ and its center is of size at most $s$. Therefore, the family $\mathcal {\left \langle G\right \rangle }$ is not $\left (\mathcal {H},s\right )$-free and so (1) does not hold. \par We now show that if (1) does not hold, then (2) does not holds. Let $\left \{ A_{1},\ldots ,A_{h}\right \} \subseteq \left \langle \mathcal {G}\right \rangle $ be a resolution of $\mathcal {H}$ whose center is of size at most $s.$ The hypergraph $\left \{ A_{1}\cap J,\ldots ,A_{h}\cap J\right \} $ is contained in $\mathcal {G},$ its center is of size at most $s$, and in order to complete the proof we need to show that it is a trace of $\mathcal {H}.$ \par For each $i=1,\ldots ,h$ let $D_{i}\subseteq \left [n\right ]\backslash J$ be a sufficiently large set that is contained in $A_{i}$ and does not intersect any other edge of $\mathcal {H}$. The fact that $\left \{ A_{1},\ldots ,A_{h}\right \} $ is a resolution of $\mathcal {H}$ implies that there exist sets $E_{1},\ldots ,E_{h}\subseteq \left [n\right ]\backslash J,$ such that \[ \mathcal {H}':=\left \{ \left (A_{1}\backslash D_{1}\right )\cup E_{1},\ldots ,\left (A_{h}\backslash D_{h}\right )\cup E_{h}\right \} \] is a copy of $\mathcal {H}.$ Now note that if we intersect each of the edges of $\mathcal {H}'$ with $J$, we obtain the original hypergraph $\left \{ A_{1}\cap J,\ldots ,A_{h}\cap J\right \} .$ Therefore, $A_{1}\cap J,\ldots ,A_{h}\cap J$ is indeed a trace of $\mathcal {H}.$ This completes the proof of the lemma.
END Proof
 
BEGIN \protect \lemmaname  \protect \let  
BEGIN Proof 
 Let $\mathbf {C}$ be the center of $\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} $, let $\mathbf {C}_{i}=\mathbf {C}\cap \mathbf {A}_{i},$ and let $\mathbf {D}_{i}=\mathbf {A}_{i}\backslash \mathbf {C}$ for each $i$. Similarly, let $C$ be the center of $\left \{ B_{1},\ldots ,B_{h}\right \} ,$ write $C_{i}=B_{i}\cap C,$ and $D_{i}=B_{i}\backslash C$. We define the following three events: \par \begin {itemize} \item We let $E_{0}$ be the event that $\mathbf {C}\cap J=C$ \item We let $E_{1}^{i}$ be the event that $\mathbf {C}_{i}\cap J=C_{i}$. \item We let $E_{2}^{i}$ be the event that \textbf {$\mathbf {D}_{i}\cap J=D_{i}$}. \end {itemize} Note that the event that $\mathbf {A}_{i}\cap J=B_{i}$ for each $i$ is the intersection of all the above events. The lemma will follow from the fact that $E_{0}$ occurs with probability $\frac {\binom {n-j}{c-d}}{\binom {n}{c}}=\Theta \left (\frac {1}{n^{s}}\right ),$ once we show that the other events occur with conditional probability $\Theta \left (1\right )$ given that $E_{0}$ holds. \par Since $\left |\mathbf {C}\right |=c$ is constant, the event $\bigcap E_{1}^{i}$ occur with conditional probability $\Theta \left (1\right )$ given $E_{0}$. \par We now complete the proof by showing that $\Pr \left [\bigcap _{i=1}^{h}E_{2}^{i}|E_{1}^{1},\ldots ,E_{1}^{h},E_{0}\right ]$ is $\Theta \left (1\right ).$ \par Note that $\mathbf {D}_{1},\ldots ,\mathbf {D}_{h}$ may be chosen by first taking a set $\mathbf {D}_{1}\sim \binom {\left [n\right ]\backslash C_{1}}{k-\left |\mathbf {C}_{1}\right |},$ then taking a set $\mathbf {D}_{2}\sim \binom {\left [n\right ]\backslash \left (\mathbf {C}\cup \mathbf {D}_{1}\right )}{k-\left |\mathbf {C}_{2}\right |},$ and so on until we choose a set $\mathbf {D}_{h}\sim \binom {\left [n\right ]\backslash \left (\mathbf {C}\cup \bigcup _{i=1}^{h-1}\mathbf {D}_{i}\right )}{k-\left |\mathbf {C}_{h}\right |}.$ Since for each $i$ the term $\frac {k-\left |\mathbf {C}_{i}\right |}{n-\left |\mathbf {C}\right |-\sum _{i=1}^{h}\left |\mathbf {D}_{h}\right |}$ is bounded away from 0 to 1, we have \[ \Pr \left [\forall i:\,\mathbf {D}_{i}\cap J\mathbf {=}D_{i}\right ]=\Theta \left (1\right ). \] This completes the proof.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN Proof 
 [Proof of Theorem \ref {thm:Genneral removal Lemma}] Let $\delta _{1}=\delta _{1}\left (h,c,\epsilon ,s\right )$ be sufficiently small, let $j=j\left (\delta _{1}\right )$ be sufficiently large, and let $\delta =\delta \left (j\right )$ be sufficiently small. By Theorem \ref {thm:Counting expanded hypergraphs} we may take $\mathcal {J}$ to be $\varnothing $ if $\frac {k}{n}<\delta _{1},$ provided that $\delta _{1}$ is sufficiently small. So suppose that $\frac {k}{n}>\delta _{1}.$ By Theorem \ref {thm:k-uniform regularity}, there exists a set $J$ and a family $\mathcal {\g }\subseteq \p \left (J\right ),$ such that $\f $ is $\epsilon $-essentially contained in $\j :=\left \langle \g \right \rangle $, and such that for each $B\in \g $ the family $\mathcal {F}_{J}^{B}$ is $\left (\left \lceil \frac {1}{\delta _{1}}\right \rceil ,\delta _{1}\right )$-regular and $\mu \left (\f _{J}^{B}\right )\ge \frac {\epsilon }{2}$. \par \textbf {Showing that $\mathcal {J}$ is $\left (\mathcal {H},s\right )$-free.} \par By Lemma \ref {lem:junta is h d free meaning} it is enough to show that $\mathcal {\mathcal {G}}$ does not contain a trace of $\mathcal {H}$ in $\mathcal {G}$ whose center is of size at most $s.$ Suppose on the contrary that $\left \{ C_{1},\ldots ,C_{h}\right \} $ is a trace of $\mathcal {H}$ in $\mathcal {G}$ whose center is of size at most $s$. Then there exist sets $B_{1},\ldots ,B_{h}\subseteq \left [n\right ]\backslash J,$ such that $\mathcal {H}'=\left \{ C_{1}\cup B_{1},\ldots ,C_{h}\cup B_{h}\right \} $ is a copy of $\mathcal {H}.$ Let $\left \{ \mathbf {H}_{1},\ldots ,\mathbf {H}_{h}\right \} $ be a random copy of $\mathcal {H}$ on $\left [n\right ].$ By Lemma \ref {lem:removal expanded calculation} we have \[ \Pr \left [\forall i:\,\mathbf {H}_{i}\cap J=C_{i}\right ]=\Omega _{\delta _{1},j}\left (\frac {1}{n^{s}}\right ), \] and we have \[ \Pr \left [\left \{ \mathbf {H}_{1},\ldots ,\mathbf {H}_{h}\right \} \subseteq \mathcal {F}|\,\forall i:\,\mathbf {H}_{i}\cap J=C_{i}\right ]=\Pr \left [\forall i:\,\mathbf {H}_{i}\backslash J\in \mathcal {F}_{J}^{C_{i}}\right ]. \] \par Now note the families $\f _{J}^{C_{i}}$ are $\left (\left \lceil \frac {1}{\delta _{1}}\right \rceil ,\delta _{1}\right )$-regular and have measure greater than $\frac {\epsilon }{2}$. Provided that $\delta _{1}$ is small enough, we may apply Theorem \ref {thm:Counting expanded hypergraphs} with $\epsilon /2$ instead of $\epsilon $, the hypergraph $\left (\boldsymbol {H}_{1}\backslash J,\ldots ,\boldsymbol {H}_{h}\backslash J\right )$, and $\delta _{1}$ instead of $\delta $, to obtain \[ \Pr \left [\forall i:\,\mathbf {H}_{i}\backslash J\in \mathcal {F}_{J}^{C_{i}}\right ]\ge \delta _{1}. \] Putting everything together, we obtain \begin {align*} \Pr \left [\left \{ \mathbf {H}_{1},\ldots ,\mathbf {H}_{h}\right \} \subseteq \mathcal {F}\right ] & \ge \Pr \left [\left \{ \mathbf {H}_{1},\ldots ,\mathbf {H}_{h}\right \} \subseteq \mathcal {F}|\,\forall i:\,\mathbf {H}_{i}\cap J=C_{i}\right ]\cdot \Pr \left [\forall i:\,\mathbf {H}_{i}\cap J=C_{i}\right ]\\ & =\Omega _{\delta _{1},\epsilon }\left (\frac {1}{n^{s}}\right ). \end {align*} Provided that $\delta $ is small enough, this contradicts the hypothesis.
END Proof
 
BEGIN \protect \propositionname  \protect \let  
BEGIN Proof 
 Let $\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} $ be a random copy of $\mathcal {H},$ and let $J$ be a set of size at most $j$, such that $\mathcal {J}$ depends on $J$. Let $\mathbf {C}_{1}=\mathbf {A}_{1}\cap J,\ldots ,\mathbf {C}_{h}=\mathbf {A}_{h}\cap J.$ Since $\mathcal {J}$ is $\left (\mathcal {H},s\right )$-free, we obtain by Lemma \ref {lem:junta is h d free meaning} that for any copy $\left \{ A_{1},\ldots ,A_{h}\right \} $ of $\mathcal {H}$ in $\mathcal {J}$ the center of $\left \{ A_{1}\cap J,\ldots ,A_{h}\cap J\right \} $ is of size at least $s+1$. Now for any hypergraph $C_{1},\ldots ,C_{h}$ of center of size at least $s+1$ we have $\Pr \left [\forall i:\,\mathbf {C}_{i}=C_{i}\right ]=O\left (\frac {1}{n^{s+1}}\right )$ by Lemma \ref {lem:removal expanded calculation}. Since there are only a constant number of subsets $\left \{ C_{1},\ldots ,C_{h}\right \} \subseteq \mathcal {P}\left (J\right ),$ we obtain that$\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} $ is a copy of $\mathcal {H}$ with probability at most $O\left (\frac {1}{n^{s+1}}\right ).$ This completes the proof of the proposition.
END Proof
 
BEGIN \protect \theoremname  \protect \let  
BEGIN Proof 
 [Proof of Theorem \ref {thm:Removal for matchings}] (1) $\implies $ (2) follows by applying Theorem \ref {thm:Genneral removal Lemma} with $s=0,$ noting that a family is $\left (\mathcal {H},0\right )$-free if and only if it is free of a matching. We now show the converse implication. \par Suppose that (2) holds. By Theorem (\ref {thm:Genneral removal Lemma}) $\f $ is $\frac {\epsilon }{h+1}$-essentially contained in an $\mathcal {M}_{h}$-free junta, provided that $\delta $ is small enough. Let $\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} $ be a random copy of $\mathcal {H}.$ Note that the event $\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} \subseteq \mathcal {F}$ can occur only if for some $i$ we have $\mathbf {A}_{i}\in \mathcal {J}\backslash \mathcal {F},$ or if $\left \{ \mathbf {A}_{1},\ldots ,\mathbf {A}_{h}\right \} \subseteq \mathcal {J}.$ So a union bound implies that it is enough to show that each of these events occurs with probability $<\frac {\epsilon }{h+1}.$ \par By Proposition \ref {Prop:converse to removal lemma} a random copy of $\mathcal {H}$ lies in $\mathcal {J}$ with probability $O\left (\frac {1}{n}\right )<\frac {\epsilon }{h+1},$ provided that $C$ is sufficiently large to imply the needed lower bound on $n$. Moreover, each $\mathbf {A}_{i}$ is uniformly distributed in $\binom {\left [n\right ]}{k}$. Therefore, for each $i$ the probability that $\mathbf {A}_{i}$ is in $\mathcal {F}$ but not in $\mathcal {J}$ is at most $\frac {\epsilon }{h+1}$. This completes the proof of the theorem.
END Proof
 
