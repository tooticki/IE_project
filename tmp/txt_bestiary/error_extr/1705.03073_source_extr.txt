BEGIN Proposition \protect \let  
BEGIN Proof 
 It suffices to consider only the case $\delta _n\geq 0$ for all $n=1,2,...,N$. Since by assumption we have $y(h) \geq y_1$ then, by induction, we can assume that the assertion holds for the $n$ first terms. The inductive step can be conducted as follows \begin {equation} \begin {split} y((n+1)h)^{m+1} &= \int _0^{(n+1)h} K((n+1)h,t) y(t) dt = h\sum _{i=1}^{n} w_{n+1,i} K_{n+1,i} \; y(ih) + \delta _{n+1}(h) \\ &\geq h\sum _{i=1}^{n} w_{n+1,i} K_{n+1,i} \; y_i + \delta _{n+1}(h) \geq y_{n+1}^{m+1}. \end {split} \end {equation} The first inequality is a consequence of the inductive assumption, while in the second we use the fact that $\delta _{n+1} \geq 0$ and (\ref {eqn:NumMet}). It follows that $y((n+1)h) \geq y_{n+1}$ what ends the proof.
END Proof
 
BEGIN Lemma \protect \let  
BEGIN Proof 
 The proof will proceed for the $\geq $-case. The reasoning for the other follows exactly the same route. Let $y$ be as in the assumption, and denote \begin {equation} z(x) := \int _0^x t^\nu y(t) dt, \end {equation} with $z(0)=0$. We have then \begin {equation} y(x) \geq C^\frac {1}{m+1} x^\frac {\mu }{m+1} z(x)^\frac {1}{m+1}, \label {eqn:yEst} \end {equation} and therefore \begin {equation} z'(x) = x^\nu y(x) \geq C^\frac {1}{m+1} x^{\nu +\frac {\mu }{m+1}} z(x)^\frac {1}{m+1}. \end {equation} After division by $z^\frac {1}{m+1}$ we can integrate to obtain \begin {equation} \int _0^x z(t)^{-\frac {1}{m+1}}z'(t)dt \geq C^\frac {1}{m+1} \int _0^x t^{\nu +\frac {\mu }{m+1}} dt, \end {equation} whence \begin {equation} \frac {m+1}{m} z(x)^{\frac {m}{m+1}} \geq C^\frac {1}{m+1} \frac {1}{1+\nu +\frac {\mu }{m+1}} x^{1+\nu +\frac {\mu }{m+1}}. \end {equation} Using the above estimate in (\ref {eqn:yEst}) implies \begin {equation} y(x) \geq \left (C\;\frac {m}{m+1}\frac {1}{1+\nu +\frac {\mu }{m+1}}\right )^\frac {1}{m} x^\frac {1+\nu +\mu }{m}. \end {equation} This ends the proof.
END Proof
 
BEGIN Lemma \protect \let  
BEGIN Proof 
 We start by deriving an representation of the solution to (\ref {eqn:IntEq}). By the Mean Value Theorem we have \begin {equation} y(x)^{m+1} = K(x,\tau (x)) \int _0^x y(t) dt, \end {equation} because $y$ is positive while $0\leq \tau (x) \leq x$. Similarly as before we substitute \begin {equation} z(x) := \int _0^x y(t) dt, \end {equation} and therefore \begin {equation} z'(x) = y(x) = \left (K(x,\tau (x)) z(x)\right )^\frac {1}{m+1}. \end {equation} Solving this differential equation yields \begin {equation} z(x) = \left (\frac {m}{m+1}\int _0^x K(t,\tau (t))^\frac {1}{m+1}dt\right )^\frac {m+1}{m}. \end {equation} Finally, \begin {equation} y(x) = K(x,\tau (x))^\frac {1}{m+1}\left (\frac {m}{m+1}\int _0^x K(t,\tau (t))^\frac {1}{m+1}dt\right )^\frac {1}{m}. \end {equation} If now $K(x,t)\sim C x^\mu $ as $x\rightarrow 0^+$ with $t\leq x$ we have \begin {equation} y(x) \sim \left (C x^\mu \right )^\frac {1}{m+1}\left (C^\frac {1}{m+1}\frac {m}{m+1}\frac {1}{1+\frac {\mu }{m+1}}x^{1+\frac {\mu }{m+1}}\right )^\frac {1}{m}. \end {equation} The proof is complete.
END Proof
 
BEGIN Lemma \protect \let  
BEGIN Proof 
 Due to the assumption we can proceed to the inductive step. To this end, let $y_i$ satisfy (\ref {eqn:LemIteration}) for every $i=1,2,...n$. Observe that from (\ref {eqn:NumMet}) with (\ref {eqn:KBound}) we have \begin {equation} y_{n+1}^{m+1} \geq C\left (1-\frac {m+1}{m} \frac {\epsilon (h)}{h^{1+\frac {1}{m}}}\right )\left (C \frac {m}{m+1}\right )^\frac {1}{m} h \sum _{i=1}^{n} w_{n,i} (i h)^\frac {1}{m}. \end {equation} Because the sum above is a discretization of the integral (\ref {eqn:LocCons}) we can further write \begin {equation} \begin {split} y_{n+1}^{m+1} &\geq C\left (1-\frac {m+1}{m} \frac {\epsilon (h)}{h^{1+\frac {1}{m}}}\right )\left (C \frac {m}{m+1}\right )^\frac {1}{m} \left (\int _0^{(n+1)h} t^\frac {1}{m}dt - \epsilon _{n+1}(h)\right ) \\ & = \left (1-\frac {m+1}{m}\frac {\epsilon (h)}{h^{1+\frac {1}{m}}}\right )\left (C \frac {m}{m+1}(n+1)h\right )^\frac {m+1}{m}\left (1-\frac {m}{m+1}\frac {\epsilon _{n+1}(h)}{\left ((n+1)h\right )^\frac {m+1}{m}}\right ), \end {split} \end {equation} where $\epsilon _{n+1}$ is the $n+1$th local consistency error for the quadrature. Now, in order to complete the induction we have to show that \begin {equation} 1-\frac {m}{m+1}\frac {\epsilon _{n+1}(h)}{\left ((n+1)h\right )^\frac {m+1}{m}}\geq \left (1-\frac {m+1}{m} \frac {\epsilon (h)}{h^{1+\frac {1}{m}}}\right )^m. \end {equation} Since by definition, $\epsilon _{n+1} \leq \epsilon $ and obviously $(n+1)h\geq h$ we further have \begin {equation} 1-\frac {m}{m+1}\frac {\epsilon (h)}{h^\frac {m+1}{m}}\geq \left (1-\frac {m+1}{m} \frac {\epsilon (h)}{h^{1+\frac {1}{m}}}\right )^m. \end {equation} Now, due to convexity we have the inequality $(1-x)^m \leq 1-x$ for $m\geq 1$ which grants the validity of the above estimate. This ends the proof.
END Proof
 
BEGIN Lemma \protect \let  
BEGIN Proof 
 First, notice that $-1<A \zeta (1-A) < 0$ for $0<A<1$ \cite {Tit86}. Therefore, the above bounds are well-defined. To prove them we proceed by induction. Assume (\ref {eqn:LemRecurAssum}) and notice that obviously we have $e_1 \leq M$. \par \par Next, we make the inductive step and assume that (\ref {eqn:LemRecurSol}) holds for all terms up to a fixed $n$. We will show that this assertion is also true for the $(n+1)$th term. First, assume that $A\geq 1$ and use the inductive assumption \begin {equation} e_{n+1} \leq \frac {M}{n+1} \left (A \sum _{i=1}^{n} i^{A-1}+\frac {B}{M}\right ). \end {equation} Now, we need an estimate for the above sum. Since the function $x^{A-1}$ is increasing, we can bound the series by an integral \begin {equation} \sum _{i=1}^{n} i^{A-1} \leq \int _1^{n+1} x^{A-1} dx = \frac {1}{A}\left ((n+1)^A-1\right ), \end {equation} therefore \begin {equation} e_{n+1} \leq \frac {M}{n+1} \left ((n+1)^A-1+\frac {B}{M}\right ) \leq \frac {M}{(n+1)^{1-A}}, \end {equation} since $B\leq M$. On the other hand, for the case $0<A<1$ we have \begin {equation} e_{n+1} \leq \frac {M}{n+1} \left (A\sum _{i=1}^{n}\frac {1}{i^{1-A}} + \frac {B}{M}\right ). \end {equation} The appropriate bound for the sum comes from the Riemann-Siegel formula \cite {Tit86}, which gives \begin {equation} \sum _{i=1}^{n}\frac {1}{i^{1-A}} \leq \frac {n^A}{A} + \zeta (1-A), \end {equation} whence \begin {equation} e_{n+1} \leq \frac {M}{n+1} \left (n^A+A\zeta (1-A)+\frac {B}{M}\right ) \leq \frac {M}{n+1} (n+1)^A = \frac {M}{(n+1)^{1-A}}, \end {equation} where the last inequality follows from the fact that $B \leq - M A\zeta (1-A)$. This concludes the reasoning.
END Proof
 
BEGIN Theorem \protect \let  
BEGIN Proof 
 First, we can use the Langrange's mean-value theorem \begin {equation} \left |y(nh)^{m+1}-y_n^{m+1}\right | = (m+1) \xi ^m \left |y(nh)-y_n\right |, \quad m>0, \label {eqn:xi} \end {equation} where $\xi $ is between $y(nh)$ and $y_n$. Now, we use the above estimate with (\ref {eqn:IntEq}) and (\ref {eqn:LocCons}) to obtain \begin {equation} (m+1) \xi ^m |e_n| \leq \left |y(nh)^{m+1}-y_{n}^{m+1}\right | \leq h W D \sum _{i=1}^{n-1} |e_i| + \delta . \end {equation} \par Now, we have to consider two cases depending on the relative size of $y(nh)$ and $y_n$. If $y_n \leq y(nh)$, then $\xi \geq y_n$ and from Lemma \ref {lem:LemIteration} it follows that \begin {equation} m C \; nh \left (1-\frac {m+1}{m}\frac {\epsilon (h)}{h^\frac {m+1}{m}} \right ) |e_n|\leq (m+1) |e_n| \leq h W D\sum _{i=1}^{n-1} |e_i| + \delta . \end {equation} Eventually, defining \begin {equation} E(h) := 1-\frac {m+1}{m}\frac {\epsilon (h)}{h^\frac {m+1}{m}} \end {equation} we can write \begin {equation} |e_n| \leq \frac {1}{n} \left (\frac {1}{m}\frac {WD}{C E(h)}\sum _{i=1}^{n-1} |e_i| + \frac {1}{m C}\frac {\delta }{h E(h)}\right ). \end {equation} On the other hand, if $y(nh)\leq y_n$, then $\xi \geq y(nh)$ and \begin {equation} m C \; nh\leq (m+1) |e_n| \leq h W D\sum _{i=1}^{n-1} |e_i| + \delta . \label {eqn:enEst} \end {equation} where in the first inequality we have used Lemma \ref {lem:LemGron}. Hence \begin {equation} |e_n| \leq \frac {1}{n} \left (\frac {1}{m}\frac {WD}{C}\sum _{i=1}^{n-1} |e_i| + \frac {1}{m C}\frac {\delta }{h}\right ) \leq \frac {1}{n} \left (\frac {1}{m}\frac {WD}{C E(h)}\sum _{i=1}^{n-1} |e_i| + \frac {1}{m C}\frac {\delta }{h E(h)}\right ). \end {equation} since $E(h) < 1$ due to the assumption. It follows that regardless of the relative size of $y(nh)$ and $y_n$, the error term satisfies the above recurrence inequality. Now, we can invoke the Lemma \ref {lem:LemRecur} to conclude that since $E(h) = O(1)$ as $h\rightarrow 0^+$ we have \begin {equation} |e_n| \leq \text {const.} \times \frac {\max \{e_1,\delta h^{-1}\}}{n^{1-A}} = \text {const.} \times \frac {\max \{h^p,\delta h^{-1}\}}{h^{A-1}} \frac {1}{(nh)^{1-A}}, \end {equation} where $A:=\frac {1}{m}\frac {WD}{C E(h)}$. Now, when $h\rightarrow 0^+$ with $nh \rightarrow \text {const.}$ we obtain the assertion. The proof is complete.
END Proof
 
BEGIN Corollary \protect \let  
BEGIN Proof 
 If $\delta _n \leq 0$, then by Proposition \ref {prop:Mono} we have $y(nh) \leq y_n$. The proof follows the same steps as in Theorem \ref {thm:Conv} with the difference that we always have $\xi \geq y(nh)$ in (\ref {eqn:xi}). Hence the conclusion can be drawn from (\ref {eqn:enEst}) without any subsequent estimates.
END Proof
 
