\chapter{Current results}\label{chap:Results}

\section{Unweighted approximations}\label{sec:Unweighted}

In this section we briefly mention the state of the art of unweighted approximations for the $k$-set packing problem. There has been a long line of work in this area and Subsection \ref{subsec:HurkensSchrijver} starts with the first $\frac{k}{2} + \varepsilon$-approximation given already in 1989 by Hurkens and Schrijver \cite{HurkensSchrijver}. We give a proof and some intuition for the case of a 2-locally optimal solution.

Subsection \ref{subsec:UnweightedQuasi} continues with a $\frac{k+2}{3}$-approximation \cite{Halldorsson} and a $\frac{k+1}{3} + \varepsilon$-approximation \cite{Mastrolilli}. These search for improving sets of size $O(\log n)$, so the improved approximation guarantee is at the cost of the running time, which is $n^{O(\log n)}$ for both algorithms (also called quasi-polynomial running time).

%These are local search algorithms that search for improving sets of size $O(\log n)$. Their clever insight is that such a search space allows them to improve the approximation guarantee from about $\frac{k}{2}$ to about $\frac{k}{3}$. This is at the cost of the running time though: searching a $O(\log n)$ space costs quasipolynomial time, i.e. $O(n^{\log n})$ time.

Only this year these algorithms were adapted to run in polynomial time \cite{Cygan,Sviridenko}. These results are mentioned in Subsection \ref{subsec:UnweightedPoly}, together with an adaptation of the $\frac{k+2}{3}$-approximation to turn it into another $\frac{k+1}{3}$-approximation \cite{FurerYu}. These are currently the best results.

Next, Section \ref{sec:Weighted} treats the current state of the art of the weighted approximation algorithms, followed by the parameterized algorithms and the hardness results in Sections \ref{sec:Parameterized} and \ref{sec:Hardness}. The results on the linear programming and semidefinite programming relaxations are postponed to the next chapters, as these will be treated in more detail and we will provide a new bound for their integrality gap.

\subsection{The first approximation algorithm}\label{subsec:HurkensSchrijver}

%Here is a brief overview of the current results in the area of unweighted approximations for the $k$-set packing problem. %For a maximization problem, an approximation algorithm is said to approximate the problem within a factor $\rho > 1$ when the output $O$ of the algorithm satisfies $\frac{
The unweighted $k$-set packing problem can be solved in polynomial time for $k=2$ \cite{Minty}, so from now on assume $k \geq 3$.

\paragraph{2-locally optimal solution} Perhaps the easiest local search technique to try is to iteratively search for an improving set of size 2. This is either one subset in $\mathcal{C}$ that does not intersect the current solution $\mathcal{A}$ or a pair of subsets that intersect at most one subset of $\mathcal{A}$. Starting with the empty solution and iteratively adding improving sets of size 2 yields an algorithm that is $\frac{k+1}{2}$-approximate \cite{Weighted2,Halldorsson}.

\paragraph{First approximation algorithm} A natural extension of this search technique is to search for improving sets of larger cardinality. If instead of improving sets of size 2 improving sets of constant size $s$ are searched for increasing values of $s$, it is possible to obtain a polynomial time approximation ratio of $\frac{k}{2} + \varepsilon$. This was discovered by Hurkens and Schrijver \cite{HurkensSchrijver} and this was actually the first approximation algorithm for $k$-set packing. %For every $\varepsilon$ there exists an $s$ such that an $s$-locally optimal solution has approximation guarantee $\frac{k}{2} + \varepsilon$.
Even though the problem was well-studied in the years that followed, this remained the best polynomial time approximation for over 20 years.

To be precise, they proved the following.
%
\begin{theorem}\label{thm:HurkensSchrijver}
(\cite[Theorem 1]{HurkensSchrijver}) Let $E_1, \ldots, E_m$ be subsets of the set $V$ of size $n$, such that:
%
\begin{enumerate}
  \item Each element of $V$ is contained in at most $k$ of the sets $E_1, \ldots, E_m$;
  \item Any collection of at most $t$ sets among $E_1, \ldots, E_m$ has a system of distinct representatives\footnote{A system of distinct representatives of sets $\{E_i\}$ is a set $\{e_i\}$ such that $e_i \in E_i$ for all $i$ and $e_i \neq e_j$ for $i \neq j$.}.
\end{enumerate}
%
Then, we have the following:
%
\begin{equation*}
\begin{alignedat}{2}
\frac{m}{n} \leq \frac{ k (k-1)^r - k }{ 2 (k-1)^r - k } \quad & \textrm{ if } t = 2r-1; \ \\
\frac{m}{n} \leq \frac{ k (k-1)^r - 2 }{ 2 (k-1)^r - 2 } \quad & \textrm{ if } t = 2r.
\end{alignedat}
\end{equation*}
\end{theorem}
%
\paragraph{Proof for $t=2$} They provide a very keen yet complicated proof. We will give the proof and some intuition for the case $t=2$, which establishes that any 2-locally optimal solution is at most a factor of $\frac{k+1}{2}$ away from the optimal solution. Let $\mathcal{A}$ be any 2-locally optimal solution and $\mathcal{B}$ be any optimal solution. Denote by $\mathcal{B}_1$ and $\mathcal{B}_2$ the sets in $\mathcal{B}$ that intersect $\mathcal{A}$ in one set respectively at least two sets. As $\mathcal{A}$ is 2-locally optimal there are no sets in $\mathcal{B}$ that do not intersect any set of $\mathcal{A}$. Note that every set in $\mathcal{A}$ intersects with at most $k$ sets in $\mathcal{B}$. So
%
\begin{equation*}
|\mathcal{B}_1| + 2|\mathcal{B}_2| \leq k|\mathcal{A}|.
\end{equation*}
%
Since $\mathcal{A}$ is 2-locally optimal, $|\mathcal{B}_1| \leq |\mathcal{A}|$ and hence
%
\begin{equation*}
2|\mathcal{B}| = |\mathcal{B}_1| + 2|\mathcal{B}_2| + |\mathcal{B}_1| \leq k|\mathcal{A}| + |\mathcal{A}| = (k+1)|\mathcal{A}|,
\end{equation*}
%
and therefore $\frac{|\mathcal{B}|}{|\mathcal{A}|} \leq \frac{k+1}{2}$.

%The proof is not hard and we will give the proof and some intuition for the case $t=2$, which shows that any 2-local optimal solution is at most a factor of $\frac{k+1}{2}$ away from the optimal solution. Let us denote the singletons to be $\{E_1, \ldots, E_l\}$ for some $l \leq m$. Because $t=2$ we know that all singletons are distinct, implying $l \leq n$. Now note that
%%
%\begin{equation*}
%2m - l = l + 2 ( m - l ) \leq \sum_{i=1}^l |E_i| + \sum_{i=l+1}^m |E_i| = \sum_{i=1}^m |E_i| \leq kn.
%\end{equation*}
%%
%Therefore we have
%%
%\begin{equation*}
%2m = 2m - l + l \leq kn + n = (k+1)n,
%\end{equation*}
%%
%and we see that $\frac{m}{n} \leq \frac{k+1}{2}$.

\paragraph{Intuition} Intuitively the argument boils down to the following. Look at the intersection graph of the current 2-locally optimal solution $\mathcal{A}$ and some optimal packing $\mathcal{B}$. The sets in $\mathcal{B}$ that conflict with just one set in $\mathcal{A}$ are not particularly interesting as they do not form an improving set. Let us delete these sets from the intersection graph together with the sets in $\mathcal{A}$ they intersect. Because $\mathcal{A}$ is a 2-locally optimal solution, the sets in $\mathcal{B}$ conflicting with two sets in $\mathcal{A}$ still have degree at least 1, while the degrees of the other sets can be anything. That is fine, because in general when every degree is at most two we are done: the number of edges between $\mathcal{A}$ and $\mathcal{B}$ is then at most $2m$ and this number is upper bounded by $nk$ and hence $2m \leq nk$. So the core of the argument really consists of small conflicts posing no problem and concentrating on the remainder of the graph. The iterative process of deleting these sets cannot go on forever, because there are only so many vertices in $\mathcal{A}$ and possibly much more in $\mathcal{B}$.

\subsection{Quasi-polynomial time algorithms}\label{subsec:UnweightedQuasi}

%These are local search algorithms that search for improving sets of size $O(\log n)$. Their clever insight is that such a search space allows them to improve the approximation guarantee from about $\frac{k}{2}$ to about $\frac{k}{3}$. This is at the cost of the running time though: searching a $O(\log n)$ space costs quasipolynomial time, i.e. $O(n^{\log n})$ time.

\paragraph{A $\frac{k+2}{3}$-approximation} Halld\'{o}rsson \cite{Halldorsson} was the first to find $\frac{k+2}{3}$-approximate solutions. His clever insight is that using a search space of improving sets of size $O(\log n)$ allows to improve the approximation guarantee from about $\frac{k}{2}$ to about $\frac{k}{3}$. However, in a straightforward implementation of an iterative algorithm with such a search space, the running time would be quasi-polynomial, i.e. $n^{O(\log n)}$. The deduction of this running time is straightforward: one can find an improving set of size $O(\log n)$ in time $n^{O(\log n)}$, and the number of iterations is trivially upper bounded by $n$.

\paragraph{A $\frac{k+1}{3} + \varepsilon$-approximation} This year, Cygan, Grandoni and Mastrolilli \cite{Mastrolilli} improved upon this result and showed that the approximation guarantee of an $O(\log n)$-locally optimal solution can in fact be bounded by $\frac{k+1}{3} + \varepsilon$. The running time is still quasi-polynomial. Their analysis is more involved and they use a lemma from \cite{BermanMIS} as a black box. In Chapter \ref{chap:LP} the same lemma is used to prove an improved bound on the integrality gap of an LP formulation for the $k$-set packing problem.

\subsection{Polynomial time algorithms}\label{subsec:UnweightedPoly}

%Also continuing the work of Halld\'{o}rsson, Sviridenko and Ward \cite{Sviridenko} combined his analysis with the famous color coding technique \cite{ColorCoding} and an application of dynamic programming. This is a special case of the more general framework of searching for improving sets of size $O(\log n)$, where they use results from fixed parameter tractability to find well-structured improving sets of size $O(\log n)$, but now in polynomial time. This is the first polynomial time approximation since \cite{HurkensSchrijver}. The approximation guarantee is $\frac{k+2}{3}$.

\paragraph{A $\frac{k+2}{3}$-approximation} Sviridenko and Ward \cite{Sviridenko} recently established an elegant polynomial time approximation algorithm that beats the approximation guarantee of $\frac{k}{2}$ and this is the first polynomial time improvement over the $\frac{k}{2} + \varepsilon$ result from \cite{HurkensSchrijver}. Their approximation guarantee is $\frac{k+2}{3}$.

Their key insight is that using improving sets with a special structure in the analysis of Halld\'{o}rsson \cite{Halldorsson} allows to reduce the running time. They look at a graph with a vertex for every set in the current solution $\mathcal{A}$. Two vertices $S,T$ are connected if there is a set in $\mathcal{C} \setminus \mathcal{A}$ that intersects $\mathcal{A}$ only in $S$ and $T$. Possibly $S = T$, which gives rise to a loop; so every set that intersects with at most two sets in $\mathcal{A}$ corresponds to one edge. Then they define three canonical improvements, which are three types of connected graphs containing two distinct cycles, thus having more edges than vertices. They iteratively search for canonical improvements, and when one is found they remove the vertices from $\mathcal{A}$ and add the edges from $\mathcal{C} \setminus \mathcal{A}$ to the current solution.

Another key insight is then to combine this with the famous color-coding technique \cite{ColorCoding} and an application of dynamic programming, which allows them to efficiently search for these canonical improvements. The result is a new polynomial time approximation after more than 20 years.

\paragraph{A $\frac{k+1}{3} + \varepsilon$-approximation} Only two months later the polynomial time approximation guarantee was cleverly improved to $\frac{k+1}{3} + \varepsilon$ by Cygan \cite{Cygan}. He adapted the quasi-polynomial $\frac{k+1}{3}$-approximation from \cite{Mastrolilli}, also with the sharp insight that structured sets reduce the search space and the running time without losing the good approximation guarantee. His algorithm searches for elegant structures that he calls improving sets of bounded pathwidth. Again using the color-coding technique, the algorithm can search the space of $O(\log n)$ improving sets of bounded pathwidth in polynomial time.

%This is a continuation of the work in \cite{Mastrolilli}, which was a quasipolynomial time algorithm with the same approximation guarantee. He managed to turn this into a polynomial time algorithm by only considering what he calls improving sets of bounded pathwidth. Also using the color coding technique, he can search the space of $O(\log n)$ improving sets of bounded pathwidth in polynomial time.

\paragraph{Another $\frac{k+1}{3} + \varepsilon$-approximation} Independently, another $\frac{k+1}{3} + \varepsilon$ polynomial time approximation algorithm was found four months later by F\"{u}rer and Yu \cite{FurerYu}. This is an ingenious improvement of the polynomial time $\frac{k+2}{3}$-approximation of \cite{Sviridenko}. Let $\mathcal{A}$ denote the current solution again. Define a bipartite auxiliary graph $G_\mathcal{A}$ with a vertex for every set in $\mathcal{A}$ in one colour class and a vertex for every set in $\mathcal{C} \setminus \mathcal{A}$ in the other colour class. Connect two vertices from both colour classes if and only if the sets corresponding to them intersect. Indeed, this is the conflict graph of the instance with only a subset of the edges.

The algorithm starts by looking for improving sets of constant size as in \cite{HurkensSchrijver}. Then they guess $O(\log n)$ sets $\mathcal{I}$ in $\mathcal{C} \setminus \mathcal{A}$ that might form an improving set. Now consider any collection of sets $\mathcal{I}_3$ within $\mathcal{I}$ of degree at least 3 in the graph $G_\mathcal{A}$. For such a collection, the algorithm looks for a sequence of replacements that swaps $t$ sets in $\mathcal{A}$ with $t$ sets outside $\mathcal{A}$. The idea is to increase the number of sets in $\mathcal{C} \setminus \mathcal{A}$ that intersect $\mathcal{A}$ in at most two sets. If the degree of the vertices in $\mathcal{I}_3$ now drops to 2 or less, they check whether these vertices together with sets of degree at most 2 in $\mathcal{I}$, form a canonical improvement as in \cite{Sviridenko}. If so, this canonical improvement is added to obtain a solution of larger cardinality.

Again the color-coding technique is exploited to find all structures in polynomial time. In comparison, the local improvements in this paper are less general than in the other $\frac{k+1}{3} + \varepsilon$ polynomial time approximation by Cygan. This results in the fact that this new analysis is simpler than the one in \cite{Cygan}.

%The smart observation is that the improving sets the algorithm searches for can be less structured than in \cite{Sviridenko}, which allows for an improved approximation guarantee. However, the local improvements are more structured than in \cite{Cygan}. This results in the fact that the new analysis in \cite{FurerYu} is simpler than the one in \cite{Cygan}.

%This is based on the methods of \cite{Sviridenko}. Their algorithm is more general than the local improvements of \cite{Halldorsson} and \cite{Sviridenko} to improve the approximation guarantee. The local improvements in \cite{Cygan} however are more general than in this paper, resulting in the fact that this new analysis is simpler than the one in \cite{Cygan}.

\section{Weighted approximations}\label{sec:Weighted}

This section considers approximation algorithms for weighted $k$-set packing. In the weighted $k$-set packing problem, every set $S$ in $\mathcal{C}$ is associated with a weight $w(S)$ and the objective is to maximise the total weight of the packing rather than its cardinality. %We consider all weighted approximation algorithms %and then we mention some results on special cases.

Also the weighted $k$-set packing problem can be solved in polynomial time for $k=2$ \cite{Minty2}, so from now on assume $k \geq 3$.

%\subsection{Unweighted versus weighted $k$-set packing}\label{subsec:WeightedVSUnweighted}
%
%In the weighted $k$-set packing problem, every set $S$ in $\mathcal{C}$ is associated with a weight $w(S)$ and the objective is to maximize the total weight of the packing rather than its cardinality. For most problems the weighted version is not much more difficult than the unweighted version. For example, also weighted matchings can be found in graphs in polynomial time, and for a lot of problems adding weights to the LP-formulation does not change anything significantly. However, for $k$-set packing the difference between the weighted version and the unweighted version is nontrivial. As we saw in Section \ref{sec:Unweighted}, the analysis of the unweighted case hugely depends on the cardinality of every set. This is because the algorithms rely on local search and analyse a locally optimal solution. However, adding more sets than you remove from your current solution may not be advantageous in the weighted case, as the total weight might decrease while the cardinality of the solution increases. And in the weighted case, it could be the case that the total weight increases when you add less sets than you remove from your current solution. This proves hard to be incorporated in the analysis, and the weights of the sets cannot be handled in a straightforward way. This is why the results for the unweighted case do not easily extend to the weighted case. There have been less results on the weighted case and the algorithms do not immediately follow from the unweighted results, although they all use local search techniques.

\subsection{Three close to $k$-approximations}\label{subsec:Weighted1}

\paragraph{A $k$-approximation} The first approximation algorithm for weighted $k$-set packing is due to Hochbaum \cite{Hochbaum} who achieved an approximation guarantee of $k$. His algorithm first preprocesses the input and then more or less greedily adds sets to the current solution.

\paragraph{A $k - 1 + \frac{1}{k}$-approximation} A weighted $k-1+\frac{1}{k}$-approximation algorithm was found in \cite{Weighted2}. They show that the simple greedy algorithm approximates the weighted problem within a factor of $k$, where the greedy approach successively selects the subset with the highest weight from all subsets that do not intersect any selected subset so far. Using a similar local search analysis as in the unweighted case they are able to improve this bound to $k - 1 + \frac{1}{k}$.

\paragraph{A $k - 1 + \varepsilon$-approximation} This result was slightly improved by Arkin and Hassin \cite{Weighted1}. Their setting is more general in the following sense. They consider a bipartite graph $G = (U \cup V, E)$ where every vertex is associated with a weight. They use the shorthand notation $w(X) = \sum_{x \in X} w(x)$ for a subset of the vertices $X$, and write $E_v$ for the neighbourhood $N(v)$ of some vertex $v$. Then analogously to \cite{HurkensSchrijver} they assume that $|E_v| \leq k$ for all $v \in V$ and that any subset $R \subseteq U$ of at most $t$ nodes satisfies $w(R) \leq w\left( \bigcup_{u \in R} E_r \right)$.

Their main theorem is that $\frac{w(U)}{w(V)} \leq k - 1 + \frac{1}{t}$. The $k - 1 + \frac{1}{k}$-approximation from \cite{Weighted2} is the case where $k = t$, so this result is slightly more general. Effectively, they reached an approximation guarantee of $k - 1 + \varepsilon$.

%Their result is slightly more general and they effectively reached an approximation guarantee of $k - 1 + \varepsilon$.

\subsection{A $\frac{4k+2}{5}$ and a $\frac{2k+2}{3}$-approximation}\label{subsec:Chandra}

\paragraph{The algorithm} Chandra and Halld\'{o}rsson \cite{Chandra} were the first to beat this bound and obtain an approximation guarantee of $\frac{2k+2}{3}$. They combine the greedy algorithm to find an initial solution and local search techniques to improve upon this solution. Essentially they show that either the greedy solution is already good, or the local search improves it quite well. They reduce the problem to the independent set problem in a $k+1$-claw free graph $G$, and their local search finds an improving claw. This is a claw $C$ whose center vertex $v$ is in the current solution $A$ in $G$, and hence the talons $T_C$ of the claw (its other vertices forming an independent set) are not in $A$. They show that using local search to find any improving claw leads to an approximation guarantee of $\frac{4k+2}{5}$, while a local search for the best local improvement is $\frac{2k+2}{3}$-approximate. With the best improvement they mean the claw $C$ with the maximum ratio between the sum of the weights of $T_C$ and the sum of the weights of the neighbours of $T_C$ in $A$.

\paragraph{Time complexity} To show that the algorithm runs in polynomial time, they need to modify it a bit. First they find a solution $A$ using the greedy approach. Then they rescale the weight function such that $w(A) = kn$. Then they keep searching for the best local improvement using the weight function $\lfloor w \rfloor$. Now each iteration increases $\lfloor w \rfloor (A)$ by at least one. Since $\mathcal{A}$ is at most a factor of $k$ away from the optimal solution, the number of iterations is bounded by $k^2n$. Since in every turn they only inspect a polynomial number of candidates, the algorithm now runs in polynomial time.

\subsection{A close to $\frac{2k}{3}$-approximation}\label{subsec:2k/3}

Berman and Krysta \cite{BermanWeighted2} improved the approximation guarantee slightly. For every $k$ they find the optimal value for $\alpha$ such that any 2-locally optimal solution with respect to the ``misdirected'' weight function $w^\alpha$ achieves the best approximation guarantee. Surprisingly there are only three distinct values for $\alpha$ that cover all values of $k$. There is an optimal value of $\alpha$ for $k=3$ which yields an approximation guarantee of $\frac{2k}{3} \approx 0.66667k$, there is an optimal value for $k=4$ which approximates the problem within $\frac{\sqrt{13}-1}{4}k \approx 0.65139k$ and there is a value that is optimal for $k \geq 5$, resulting in a $2^{-\log_3 2}k \approx 0.64576k$-approximation.

\subsection{A $\frac{k+1}{2}$-approximation algorithm}\label{subsec:Berman}

Currently the best approximation algorithm for weighted $k$-set packing is from Berman \cite{Berman}, also using another objective function $w^2$ rather than $w$. Its approximation guarantee is $\frac{k+1}{2}$. Berman defines a function charge$(u,v)$ and searches for minimal claws that satisfy some condition on this charge function, called nice claws. He shows that if this algorithm terminates it achieves the desired approximation guarantee. To show that the algorithm terminates, he shows that every nice claw improves the square of the weight function. Similar to the running time analysis of \cite{Chandra}, he shows that the number of iterations where $w^2$ improves is polynomially bounded. And when there is no more claw that improves $w^2$, there is no nice claw anymore, and hence the algorithm terminates. Therefore it runs in polynomial time and achieves the approximation guarantee $\frac{k+1}{2}$. Chronologically, this result came before the result from the previous subsection \cite{BermanWeighted2}.

Chapter \ref{chap:Weighted} dives into the details of this paper and we show a simplified and more intuitive proof of the fact that a nice claw improves $w^2$.

\section{Parameterized complexity}\label{sec:Parameterized}

\paragraph{Background} Due to the limited results in the area of approximation algorithms for a long time, people started to study the parameterized problem where the cardinality of the solution is assumed to be $m$. In the $k$-set $m$-packing problem the goal is to find $m$ disjoint sets, each of size $k$. The running time of exact algorithms for this problem can be written in the form $f(m)poly(n)$, moving the exponential dependency to the parameter $m$ instead of $n$. There have been huge improvements on the running time of these exact algorithms. This is in spite of the fact that the $k$-set $m$-packing problem is $W[1]$-complete with respect to the parameter $m$ \cite{DowneyFellows} (see Subsection \ref{subsec:Hardness2} for details about $W[1]$-completeness). These results contain %both randomized and deterministic algorithms, and
some algorithms especially designed for the case $k=3$. Table~\ref{tab:3SP} in Appendix \ref{app:Parameterized} gives an overview of the results on the 3-set $m$-packing, and we refer to Table~\ref{tab:kSP} in Appendix \ref{app:Parameterized} for %an overview of the results on the parameterized complexity of
the more general $k$-set $m$-packing problem. Some results also extend to the weighted case, this is mentioned in the last column. We follow the convention from parameterized algorithms to let $O^*(f(m))$ denote $f(m)n^{O(1)}$.

\paragraph{Improvements} Some remarks in the last column of these tables require some explanation. Koutis' \cite{Koutis1} original result was an $O^*(2^{O(m)})$ time algorithm. In \cite{SmallColorCoding,GreedyLocalization} it was pointed out that the constants are huge in this approximation, showing the bound is at least $O^*(32000^{3m})$ when $k=3$. Like many results in this area, Koutis derandomised his algorithm using the color-coding technique from Alon, Yuster and Zwick \cite{ColorCoding}. In \cite{SmallColorCoding,GreedyLocalization} a new perfect hashing technique was introduced, with which Koutis' result could be improved to $O^*(25.6^{mk})$. A similar result is true for the deterministic algorithm by Fellows et al \cite{Fellows}. Its original bound was $exp(O(mk))$, which was actually $O^*((12.7D)^{3m})$ for some $D \geq 10.4$ (when $k=3$). This is improved to $O^*(13.78^{mk})$ using the new perfect hash function.

The first deterministic result of the form $O^*(g(m,k))$ for $k$-set $m$-packing is due to Jia, Zhang and Chen \cite{5.7kk}. Currently the best complexity result (randomised) result for $k$-set $m$-packing is $O^*(f(m,k))$ by Bj\"{o}rklund et al \cite{Bjorklund}. This function is not very readable or insightful, but behaves well for small $k$. For example, when $k=3$, we have $O^*(f(3,m)) \approx O^*(1.493^{3m})$, which is the result mentioned in Table~\ref{tab:3SP}. %It should be noted that
The function is %not $O^*((2-\varepsilon)^{mk})$ for any $\varepsilon$ (but it is
strictly smaller than $O^*(2^{mk})$.%)

\paragraph{Hardness} Some of the fastest parameterized algorithms rely on group algebra theory. %, reducing the problem to the detection of a multilinear polynomial of degree $mk$.
A variable is introduced for every element and a subset is the product of its variables. A packing then corresponds to a multilinear polynomial of degree $mk$. Koutis and Williams \cite{KoutisWilliams} showed that detecting such a polynomial cannot be done in their model in time faster than $O^*(2^{mk})$. These results still hold if the color-coding method is used (\cite{ColorCoding}) or the randomised divide-and-conquer approach (\cite{DivideAndConquer,SmallColorCoding}).

This is the only result on the limit of the time complexity of these parameterized algorithms. There are more hardness results known for the approximation algorithms of non-parameterized $k$-set packing. The next section captures an outline of these hardness results.

\section{Hardness results}\label{sec:Hardness}

Subsection \ref{subsec:Hardness0} considers some results on the general set packing problem. Subsections \ref{subsec:Hardness1}, \ref{subsec:Hardness2} and \ref{subsec:Hardness3} consider hardness results specifically for $k$-set packing. These are respectively a result on the hardness of approximation, on the non-existence of certain algorithms and on the limits of local search techniques for this problem.

\subsection{Hardness of set packing}\label{subsec:Hardness0}

Set packing is one of the standard packing problems, which are closely related to covering problems. Table \ref{tab:PackingCovering} lists some of these problems. Set packing is the most general of these packing problems, together with its LP-dual minimum set cover. We refer to \cite{Reductions} for a survey of these problems.
%Roughly speaking, in a packing problem one needs to find a maximum set such that every element is packed at most once, while in a covering problem one needs to find a minimum set such that every element is covered at least once. In fact, the LP-dual of a packing problem is a covering problem in general and vice versa. Table \ref{tab:PackingCovering} lists some of these LP-dualities. We refer to \cite{Reductions} for a survey of these problems.
%
\begin{table}
\centering
\begin{tabular}{ll}
  \toprule
  \multicolumn{2}{c}{Covering-packing dualities} \\
  \midrule
  Minimum set cover    & Maximum set packing \\
  Minimum vertex cover & Maximum matching \\
  Minimum edge cover   & Maximum independent set \\
  \bottomrule
\end{tabular}
\caption{Every horizontal pair of problems are each other's LP-duals.}
\label{tab:PackingCovering}
\end{table}

%Set packing is the dual of set cover, these two problems are the most general of the problems mentioned here. There are close connections between the problems, and connections with hypergraphs. The edge cover problem and the matching problem are the only problems that can be solved in polynomial time \cite{noPTAS2}.
The following results on the hardness of the problems also apply to the set packing problem. Arora et al showed that, unless $\mathcal{P} = \mathcal{NP}$, the vertex cover does not admit a polynomial time approximation scheme, even on bounded degree graphs \cite{noPTAS1}. Moreover, also assuming that $\mathcal{P} \neq \mathcal{NP}$, it has been shown that there is no constant-ratio polynomial time approximation scheme for independent set \cite{noPTAS2,NoPTAS3}. H{\aa}stad proved that set packing cannot be approximated within $n^{1 - \varepsilon}$, where $n$ is the number of sets, unless $\mathcal{NP} = \mathcal{ZPP}$ \cite{CliqueIsHard}. On the positive side, set packing can be approximated within a factor of $\sqrt{N}$ \cite{GeneralSP} (recall that $N$ is the number of elements in $\mathcal{U}$). H{\aa}stad's result also implies this is the best possible assuming $\mathcal{NP} \neq \mathcal{ZPP}$. %\cite{CliqueIsHard}.

\subsection{Hardness of approximation}\label{subsec:Hardness1}

As noted in Subsection \ref{subsec:UnweightedPoly}, currently the best approximation guarantee for unweighted $k$-set packing is $\frac{k+1}{3} + \varepsilon$. Hazan, Safra and Schwarz \cite{Hazan} showed the following hardness of approximation result. There is still a gap to bridge between the currently best approximation guarantee and this bound.

%Even though there has suddenly been an outburst of new polynomial time approximation algorithms for the unweighted case, there is still quite a gap to bridge between the current bound of $\frac{k+1}{3} + \varepsilon$ and the following hardness of approximation result of Hazan, Safra and Schwarz \cite{Hazan}.
%
\begin{theorem}\label{thm:HardnessHazan}
(\cite{Hazan}) It is NP-hard to approximate $k$-set packing in polynomial time within a factor of $O \left( \frac{k}{\log k} \right)$.
\end{theorem}
%
Here is a rough outline of their argument. Define gap-$P$-$[a,b]$ to be the following decision problem: decide on an instance of the decision problem $P$ whether there exists a fractional solution of size at least $b$ or whether every solution of the given instance is of fractional size smaller than $a$. Then if gap-$P$-$[a,b]$ is NP-hard, so is approximating $P$ within a factor smaller than $\frac{b}{a}$.

Define MAX-3-LIN-$q$ as the optimization problem where a set of linear equations over $GF(q)$ is given, each depending on 3 variables, and one needs to find an assignment maximising the number of satisfied equations. H{\aa}stad \cite{Hardness1} proved that gap-MAX-3-LIN-$q$-$[\frac{1}{q} + \varepsilon, 1 - \varepsilon]$ is NP-hard, which was a central result in the theory of hardness of approximation. %that formulated the PCP-theorem back in the nineties (see e.g. \cite{PCP1,noPTAS1,PCP2}).

In \cite{Hazan} they provide a polynomial time reduction from MAX-3-LIN-$q$ to $k$-uniform hypergraph matching (i.e. to $k$-set packing). They use what they call a $(q,\delta)$-Hyper-Graph-Edge-Disperser as a gadget (see \cite{Disperser} for background on dispersers and extractors). They construct such a gadget for every variable occurring in the equations. The set of all these gadgets is then the set of vertices of a hypergraph $H$. By a clever construction of the hyperedges they relate the satisfied equations to a packing in this hypergraph. This enables them to show that gap-$k$-SP-$[\frac{4}{q^3} + \varepsilon, \frac{1}{q^2} - 1]$ is NP-hard. As they have $k = \Theta(q \log q)$, they find a bound for the inapproximability factor for $k$-set packing of $\Omega(\frac{k}{\log k})$.

\subsection{Non-existence of certain algorithms}\label{subsec:Hardness2}

%This hardness result holds for any approach one could use to solve $k$-set packing.
There are also some other hardness results known, more specifically on the existence of certain algorithms or on the limits of what is achievable using local search techniques. Cygan \cite{Cygan} proved the following.
%
\begin{theorem}\label{thm:HardnessCygan}
(\cite[Theorem 1.1]{Cygan}) It is $W[1]$-hard to search the whole space of improving sets of size $r$ efficiently.

More formally, unless $FPT = W[1]$, there is no $f(r)poly(n)$ time algorithm that given a family $\mathcal{C} \subseteq 2^\mathcal{U}$ of sets of size 3 and a disjoint subfamily $\mathcal{A} \subseteq \mathcal{C}$ either finds a bigger disjoint family $\mathcal{B} \subseteq \mathcal{C}$ or verifies that there is no disjoint family $\mathcal{B} \subseteq \mathcal{C}$ such that $|\mathcal{A} \setminus \mathcal{B}| + |\mathcal{B} \setminus \mathcal{A}| \leq r$.
\end{theorem}
%
\paragraph{Fixed parameter tractability} This theorem requires some explanation (see e.g. \cite{FPT1,DowneyFellows}). $FPT$ is the set of fixed parameter tractable problems, which are problems with input size $n$ that can be solved in time $f(k)n^{O(1)}$ for some function $f$. $FPT$ thus classifies problems according to multiple parameters rather than a single parameter, where it is crucial that functions $f(n,k)$ like $n^k$ are not allowed. If the value of $k$ is fixed, the problem is said to be parameterized, which is the setting of Subsection \ref{sec:Parameterized} with parameter $m$. %There a lot of algorithms whose running time was polynomial in $n$ and exponential in $m$ were mentioned.

\paragraph{$W$-hierarchy of $FPT$} The $W$-hierarchy $\bigcup_{t \geq 0} W[t]$ has been introduced to formalise the level of intractability of problems. We have $FPT = W[0]$ and $W[i] \subseteq W[j]$ whenever $i \leq j$. We do not go into the details and refer the interested reader to \cite{FPT1,DowneyFellows}.

Theorem \ref{thm:HardnessCygan} assumes $FPT \neq W[1]$, which is a widely believed assumption. In particular, this assumption is equivalent to the famous Exponential Time Hypothesis (ETH) \cite{ETH}. If $FPT = W[1]$ then the ETH fails and vice versa \cite{Cygan,FPT1}. When a problem is $W[1]$-complete (i.e. not solvable in polynomial time unless $FPT = W[t]$, here for $t=1$), this is strong evidence that the problem is probably not fixed parameter tractable. For example, %the decision versions of
the clique problem and the independent set problem are $W[1]$-complete and %the decision versions of
the dominating set problem and the set cover problem are $W[2]$-complete. %(indeed, these last two problems are equivalent).

The assumption in Theorem \ref{thm:HardnessCygan} is thus plausible, and if this is true then there is no polynomial time algorithm which searches the whole space of size $r$ improving sets.

\subsection{Limits of local search}\label{subsec:Hardness3}

Next to this result on the existence of such an algorithm, there is also a lower bound on the approximation guarantee that algorithms based on local search techniques can achieve. %This shows the limits what this technique is capable of for $k$-set packing in terms of its approximation guarantee.
Sviridenko and Ward \cite{Sviridenko} proved the following.
%
\begin{theorem}\label{thm:HardnessSviridenko}
(\cite[Theorem 6.1]{Sviridenko}) The locality gap of a $t$-locally optimal solution is at least $\frac{k}{3}$, even when $t$ is allowed to grow on the order of $n$.

More formally, let $c = \frac{9}{2e^5k}$ and suppose that $t \leq cn$ for all sufficiently large $n$. Then there exist 2 pairwise disjoint collections of $k$-sets $\mathcal{A}$ and $\mathcal{B}$ with $|\mathcal{A}| = 3n$ and $|\mathcal{B}| = kn$ such that any collection of $a \leq t$ sets in $\mathcal{B}$ intersects with at least $a$ sets in $\mathcal{A}$.
\end{theorem}
%
This result shows that even local search algorithms that are allowed to examine some exponential number of possible improvements at each stage cannot achieve an approximation guarantee better than $\frac{k}{3}$. This suggests that local search algorithms, which currently achieve an approximation guarantee of $\frac{k+1}{3} + \varepsilon$, are not a good approach to beat the approximation guarantee much further. F\"{u}rer and Yu \cite{FurerYu} extended this result. %to the the following.
%
\begin{theorem}\label{thm:HardnessFurerYu}
(\cite[Theorem 7]{FurerYu}) There is an instance for $k$-set packing with locality gap $\frac{k+1}{3}$ such that there is no local improvement of size up to $O(n^{\frac{1}{5}})$.

More formally, for any $t \leq \left( \frac{3 e^3 n}{k} \right)^\frac{1}{5}$ there exist two disjoint collections of $k$-sets $\mathcal{A}$ and $\mathcal{B}$ with $|\mathcal{A}| = 3n$ and $|\mathcal{B}| = (k+1)n$ such that any collection of $t$ sets in $\mathcal{A}$ intersects with at least $t$ sets in $\mathcal{B}$.
\end{theorem}
%
So local search has reached its limits for all practical purposes. To achieve an approximation guarantee beating the order of $\frac{k}{3}$, other approaches to the problem are necessary to consider. Another approach one could take is the LP and SDP relaxations for $k$-set packing. The next chapters treats these two approaches and show improved bounds on their integrality gap. 