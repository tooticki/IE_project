\chapter{SDP formulation}\label{chap:SDP}

The first two sections treat background on semidefinite programming and the Lov\'{a}sz Theta function. These concepts are needed for the proof in Section \ref{sec:SDP} of the theorem that there exists a polynomially sized SDP with the improved integrality gap from Theorem \ref{thm:IntegralityGap2}. %We will treat the result that some SDP for $k$-set packing captures all constraints of the intersecting family LP \cite{LapChiLau}, leading to the theorem.

\section{Background on semidefinite programming}\label{sec:Background}

A semidefinite program is a more general form of a linear program. %and this section is devoted to this generalization.
General references for a thorough background about SDPs are \cite{SDP11,SDP2,SDP10,SDP5,SDP1}.

%A semidefinite program is a more general form of a linear program. In this section we will see this generalization. In order to compare the basic facts about LPs and SDPs we first go quickly over the well-known preliminaries for LPs. Then we introduce the notion of an SDP and we see to what extent the results on LPs extend to SDPs. General references for this section are \cite{SDP11,SDP2,SDP10,SDP5,SDP1}. [TODO]
%
%\subsection{Preliminaries about linear programming}\label{subsec:PreliminariesLP}
%
%A typical LP has the following form.
%%
%\begin{equation}\label{Primal LP}\tag{Primal LP}
%\begin{alignedat}{2}
%\text{max}  \quad & c^T x \ \\
%\text{s.t.} \quad & Ax \leq b
%\end{alignedat}
%\end{equation}
%%
%Here $x$ is the vector of decision variables we can assign values to in order to maximize $c^T x$. That is the inner product of two vectors of length $n$, and $Ax$ is the matrix product of an $m \times n$ matrix $A \in \mathbb{R}^{m \times n}$ with $x$. To be more precise, we can write the following.
%%
%%\begin{equation*}
%%\begin{alignedat}{2}
%%\text{max}  \quad & \left(
%%                      \begin{array}{cccc}
%%                        c_1 & c_2 & \cdots & c_n \\
%%                      \end{array}
%%                    \right) \left(
%%                              \begin{array}{c}
%%                                x_1 \\
%%                                x_2 \\
%%                                \vdots \\
%%                                x_n \\
%%                              \end{array}
%%                            \right)
%% \ \\
%%\text{s.t.} \quad & \left(
%%                      \begin{array}{cccc}
%%                        a_{11} & a_{12} & \cdots & a_{1n} \\
%%                        a_{21} & a_{22} & \cdots & a_{2n} \\
%%                        \vdots & \vdots & \ddots & \vdots \\
%%                        a_{m1} & a_{m2} & \cdots & a_{mn} \\
%%                      \end{array}
%%                    \right) \left(
%%                              \begin{array}{c}
%%                                x_1 \\
%%                                x_2 \\
%%                                \vdots \\
%%                                x_n \\
%%                              \end{array}
%%                            \right) \leq \left(
%%                                           \begin{array}{c}
%%                                             b_1 \\
%%                                             b_2 \\
%%                                             \vdots \\
%%                                             b_m \\
%%                                           \end{array}
%%                                         \right)
%%\end{alignedat}
%%\end{equation*}
%%
%\begin{equation*}
%\begin{alignedat}{2}
%\text{max}  \quad & \sum_{j=1}^n c_j x_j \ \\
%\text{s.t.} \quad & \sum_{j=1}^n a_{ij} x_j \leq b_i, & \quad j = 1, \ldots, m
%\end{alignedat}
%\end{equation*}
%%
%In this LP we are maximizing a linear objective function given $m$ linear constraints. Every solution to these constraints is called a feasible solution and a feasible solution is called optimal if it attains the maximum possible objective value. The set of feasible solutions form a convex polyhedron $P$ in $\mathbb{R}^n$ and each facet of $P$ corresponds to one of the inequalities. If the set of feasible solutions is empty the LP is called infeasible, and the LP is said to be unbounded if there exists no upper bound for its objective value.
%
%There is a large number of equivalent formulations of a linear program, all of which are easily reduced to one another. For example, it is possible to minimize rather than to maximize or to stipulate that the $x_i$ are nonnegative. We could also add equations and inequalities of the form $\geq$. Another standard setting for a linear programming is the following.
%%
%\begin{equation}\label{LP'}\tag{LP'}
%\begin{alignedat}{2}
%\text{max}  \quad & c^T x \ \\
%\text{s.t.} \quad & Ax = b \\
%            \quad & x \geq 0
%\end{alignedat}
%\end{equation}
%
%At the core of linear optimization we have the following well-known lemma. This is one of the many equivalent formulations.
%%
%\begin{lemma} \textbf{(Farkas Lemma)}
%Let $A$ be an $n \times m$ matrix and let $b \in \mathbb{R}^m$. Then exactly one of the following statements is true.
%%
%\begin{enumerate}
%  \item There exists an $x \in \mathbb{R}^n$ such that $Ax \leq b$;
%  \item There is an $y \in \mathbb{R}^m$ such that $y \geq 0$, $yA = 0$ and $yb<0$.
%\end{enumerate}
%\end{lemma}
%%
%With this lemma in mind, the following LP is called the dual LP of \eqref{Primal LP}.
%%
%\begin{equation}\tag{Dual LP}
%\begin{alignedat}{2}
%\text{min}  \quad & b^T y \ \\
%\text{s.t.} \quad & A^T y = c \\
%            \quad & y \geq 0
%\end{alignedat}
%\end{equation}
%%
%By weak duality we know that the objective value of the dual at any feasible solution is always greater than or equal to the objective value of the primal at any feasible solution. By strong duality, we know that if the primal has an optimal solution then the dual also has an optimal solution with the same objective value and vice versa. The primal program is unbounded if and only if the dual is infeasible and the dual is unbounded if and only if the primal is infeasible.
%
%\subsection{From linear to semidefinite programming}\label{subsec:LPtoSDP}

In a linear program, the objective is to maximise a linear function over a convex polyhedron. In a semidefinite program the numbers are substituted by vectors and the dot product of two vectors is used instead of the multiplication of two numbers. A semidefinite program can be written in the following form. %where we are minimizing over the vectors $x^1, \ldots, x^n \in \mathbb{R}^n$.
%
%\begin{equation*}
%\begin{alignedat}{2}
%\text{max}  \quad & \sum_{i,j} c_{ij} (x^i x^j) \ \\
%\text{s.t.} \quad & \sum_{i,j} a_{ijk} (x^i x^j) \leq b_k, & \quad k = 1, \ldots, m
%\end{alignedat}
%\end{equation*}
%%
%However, it is more convenient to think of semidefinite programs in another equivalent way. We consider semidefinite programs of the following form.
%
\begin{equation*}%\tag{Primal SDP}\label{Primal SDP}
\begin{alignedat}{2}
\text{max}  \quad & c^T x \ \\
\text{s.t.} \quad & x_1 A_1 + \ldots + x_n A_n - B \succeq 0
\end{alignedat}
\end{equation*}
%
Here $x \in \mathbb{R}^n$ is the vector of decision variables one needs to assign values to in order to maximise the inner product $c^T x$ with the given vector $c \in \mathbb{R}^n$. $A_1, \ldots, A_n, B \in \mathbf{S}^m$ are given symmetric $m \times m$ matrices, so one can think of $X := x_1 A_1 + \ldots + x_n A_n - B$ as a matrix whose entries are linear functions over the variables $x_i$. The constraint $X \succeq 0$ means $X$ needs to be positive semidefinite, which is equivalent to $y^T X y$ being nonnegative for all $y \in \mathbb{R}^m$ or to $X$ having only nonnegative eigenvalues. When $X \succeq 0$ for some $x \in \mathbb{R}^n$ we say $x$ is a feasible solution. Since both the objective function and the constraints are convex in $x$, a semidefinite program is a convex optimization problem. In contrast to a linear program, its feasible region is in general not a a polyhedron.

%An LP is just a special case of an SDP. To see this, consider \eqref{Primal SDP} and let $A_1, \ldots, A_n, B$ be diagonal matrices with $a_1, \ldots, a_n, \{b_i\}$ on its diagonal entries respectively. We now immediately see that the SDP translates back to \eqref{Primal LP}. A semidefinite programming is therefore a generalization of a linear program.
%
%\subsection{Basic results on semidefinite programming}\label{sec:ResultsSDP}
%
%There is also a duality theorem for semidefinite programs, for which we need the semidefinite version of the Farkas lemma. In order to define it, we first need some background on the semidefinite cone $\mathbf{S}^m_+$. We equip the space of symmetric matrices $\mathbf{S}^m$ with the Frobenius inner product:
%%
%\begin{equation*}
%A \bullet B = \sum_{i,j} A_{ij} B_{ij}.
%\end{equation*}
%%
%For some cone $C \subset \mathbf{S}^m $ we define its dual
%%
%\begin{equation*}
%C_* = \{ Y \in \mathbf{S}^m \mid Y \bullet X \geq 0 \quad \forall X \in C \}.
%\end{equation*}
%%
%Now the semidefinite cone $\mathbf{S}^m_+$ is a cone in $\mathbf{S}^m$ and consists of all positive semidefinite matrices. The dual of $\mathbf{S}^m_+$ is $\mathbf{S}^m_+$, i.e. it is self-dual. Given these definitions and properties we can characterize semidefinite programs in a geometrical way like we did with LPs: \eqref{Primal SDP} is the same as maximizing a linear objective function over the intersection of $\mathbf{S}^m_+$ with an affine plain.
%
%We now proceed with the semidefinite version of Farkas lemma.
%%
%\begin{lemma}%(\cite[Lemma 3.3]{SDP10})
%Let $A_1, \ldots, A_n, B \in \mathbf{S}^m$ be symmetric $m \times m$ matrices. Then exactly one of the following statements is true.
%%
%\begin{enumerate}
%  \item There exists an $x \in \mathbb{R}^n$ such that $x_1 A_1 + \ldots x_n A_n - B \succ 0$;
%  \item There exists a $Y \in \mathbf{S}^m$ such that $Y \neq 0$, $Y \succeq 0$, $A_i Y = 0$ for $i=1,\ldots,m$ and $B Y \succeq 0$.
%\end{enumerate}
%\end{lemma}
%%
%With this in mind we can now define the dual SDP of \eqref{Primal SDP}.
%%
%\begin{equation}\tag{Dual SDP}\label{Dual SDP}
%\begin{alignedat}{2}
%\text{min}  \quad & B \bullet Y \ \\
%\text{s.t.} \quad & A_i \bullet Y = c, & \quad & i = 1, \ldots, n \\
%            \quad & Y \succeq 0
%\end{alignedat}
%\end{equation}
%%
%The dual is again a semidefinite program and its optimal value is less than or equal to the optimal value of the primal SDP when both the primal SDP and the dual SDP have some feasible solution. We call a solution to some semidefinite program strictly feasible when the positive semidefinite constraints are met strictly, i.e. when for every constraint of the form $M \succeq 0$ in fact $M$ is positive definite (rather than semidefinite), meaning $y^T M y > 0$ for all properly sized real-valued vectors $y$. If \eqref{Primal SDP} has a strictly feasible solution, then the dual optimum is attained and the values of both optimal solutions coincide.
%
%Like in LPs we can write an SDP in many equivalent ways. Obviously we can minimize instead of maximize again or include the nonnegativity inequalities $x_i \geq 0$. By extending the matrices $A_1, \ldots, A_n, B$ with new diagonal entries, we could also include additional equations and inequalities on the $x_i$ variables. For example, note that changing the names of the matrices in \eqref{Dual SDP} gives the SDP-analogue of \eqref{LP'}.
%%
%%\begin{equation}\tag{SDP'}\label{SDP'}
%%\begin{alignedat}{2}
%%\text{min}  \quad & C \bullet X \ \\
%%\text{s.t.} \quad & A_i \bullet X = b_i, & \quad & i = 1, \ldots, n \\
%%            \quad & X \succeq 0
%%\end{alignedat}
%%\end{equation}
%
%Like linear programs semidefinite programs can be solved in polynomial time using the ellipsoid method. However, some technicalities arise that we do not discuss here. The interested reader may read \cite{Ellipsoid1,Ellipsoid2}. Unfortunately, also like for LPs, the ellipsoid method is very slow and impractical to use. In practice, SDPs can be solved efficiently using interior point methods \cite{Interior1,Interior2}.

\section{Background on the Lov\'{a}sz Theta function}\label{sec:Theta}

This section gives some background on the famous Lov\'{a}sz Theta function introduced in \cite{Shannon}. %Historically, the Lov\'{a}sz Theta function $\vartheta(G)$ for a graph $G$ has been introduced to upper bound the Shannon capacity $\Theta(G)$ \cite{Shannon}. Since the concept of the Shannon capacity is not required for the results on $k$-set packing we will not treat it here. For a long time it was an open question if the Shannon capacity of $C_5$ was equal to $\sqrt{5}$ or not, and in his paper Lov\'{a}sz \cite{Shannon} proved this in the affirmative and generalized his method to obtain upper bounds on the Shannon capacity of any graph: the Lov\'{a}sz Theta function. We have $\alpha(G) \leq \Theta(G) \leq \vartheta(G)$ for all graphs $G$.
We introduce the Lov\'{a}sz Theta function via orthogonal representations. In order to do that some background about the stable set polytope is first given in Subsection \ref{subsec:STAB}. Subsection \ref{subsec:ONR} will talk about orthogonal representations and then the Lov\'{a}sz Theta function is introduced in Subsection \ref{subsec:Theta}. General references for this section are \cite{SDP11,GLS,Lovasz1,SDP10}.

\subsection{The stable set polytope}\label{subsec:STAB}

%Before we continue with the results for a semidefinite program for $k$-set packing, we need some background on the famous Lov\'{a}sz Theta function. In order to see the connection of this function with the $k$-set packing problem, we first need some background about the stable set polytope. This is the topic in this section, and in the next section we treat the Lov\'{a}sz Theta function. General references for this section are \cite{SDP11,GLS,Lovasz1,SDP10}.

\paragraph{Definition} Stable set is another name for an independent set. Given a graph $G = (V,E)$, $\alpha(G)$ denotes the size of the maximum independent set in $G$. For every subset of the vertices $S \subseteq V$ its incidence vector is denoted by $\chi^S \in \mathbb{R}^V$, i.e. for all $i \in V$, $\chi^S_i = 1$ if $i \in S$ and $\chi^S_i = 0$ otherwise. Now define the stable set polytope STAB$(G)$ as follows.
%
\begin{equation*}
\textrm{STAB}(G) = \textrm{conv.hull}( \chi^S \in \mathbb{R}^V \mid S \textrm{ is an independent set in } G )
\end{equation*}
%
\paragraph{Properties} So STAB$(G)$ is the smallest convex set in $\mathbb{R}^V$ containing the incidence vectors of all independent sets. Since all extreme points of this polytope are $0,1$-vectors, there is a system of linear inequalities describing this convex set. Theoretically it is possible to find $\alpha(G)$ by optimising the linear objective function $\sum_i x_i$ over STAB$(G)$. However, the number of constraints is generally exponential in $|V|$ so this is not an efficient approach to find $\alpha(G)$, which should be expected as determining $\alpha(G)$ is NP-hard. What one can do, however, is to find extra inequalities for the stable set polytope and find upper bounds for $\alpha(G)$.

\paragraph{The clique constrained stable set polytope} With the intersecting family LP in mind it is natural to start with the following inequalities for $x \in \mathbb{R}^V$.
%
\begin{equation}\label{eq:STAB1}
x_i \geq 0 \quad i \in V,
\end{equation}
%
%\begin{equation}\label{eq:STAB2}
%x_i + x_j \leq 1 \quad \{i,j\} \in E.
%\end{equation}
%%
%Let us write FSTAB$(G)$ for the polytope generated by these constraints.
%%
%\begin{equation*}
%\textrm{FSTAB}(G) = \textrm{conv.hull}( x \in \mathbb{R}^V \mid \textrm{Constraints } \eqref{eq:STAB1} \textrm{ and } \eqref{eq:STAB2} \textrm{ hold} ).
%\end{equation*}
%%
%Now any independent set in $G$ corresponds to an integral vertex in FSTAB$(G)$ and vice versa. For bipartite graphs without isolated vertices FSTAB$(G)$ and STAB$(G)$ coincide. If we add the constraint $x_i \leq 1$ for all $i \in V$ they coincide for any bipartite graph. However, for general graphs FSTAB$(G)$ may have other non-integral vertices and FSTAB$(G)$ is larger than STAB$(G)$. Consider $G = K_3$ for example, a triangle. Then the maximum independent set in $G$ has size one, but FSTAB$(G)$ also contains the vertex $(\frac{1}{2},\frac{1}{2},\frac{1}{2})$. In a general $K_n$, FSTAB$(G)$ contains the vertex $x = (\frac{1}{n-1}, \ldots, \frac{1}{n-1})$ with $\sum_i x_i = \frac{n}{n-1} > 1$.
%
%This should remind the reader of the standard LP relaxation and the intersecting family LP described in Chapter \ref{chap:LP}. The intersecting family LP had an extra constraint for every intersecting family of hyperedges, which form a clique in the conflict graph. From every clique only one vertex can be contained in an independent set, so add the following constraint.
%
\begin{equation}\label{eq:STAB3}
\sum_{i \in V(Q)} x_i \leq 1 \quad \textrm{for all cliques } Q \textrm{ in } G.
\end{equation}
%
%The first polytope approximating $STAB(G)$ is the clique constrained stable set polytope, defined as follows.
Now define the clique constrained stable set polytope as follows.
%
\begin{equation*}
\textrm{QSTAB}(G) = \textrm{conv.hull}( x \in \mathbb{R}^V \mid \textrm{Constraints } \eqref{eq:STAB1} \textrm{ and } \eqref{eq:STAB3} \textrm{ hold} ).
\end{equation*}
%
Now any independent set in $G$ corresponds to an integral vertex in QSTAB$(G)$ and vice versa. The clique constrained stable set polytope is the first approximation of the stable set polytope that is considered here, but to formally define the Lov\'{a}sz Theta function another polytope is introduced in the next subsection.
%Note that Constraint \eqref{eq:STAB3} implies Constraint \eqref{eq:STAB2} if we consider any connected subgraph of two vertices to be a clique of size 2. This implies that QSTAB$(G) \subseteq$ FSTAB$(G)$. So we have
%%
%\begin{equation*}
%\textrm{STAB}(G) \subseteq \textrm{QSTAB}(G) \subseteq \textrm{FSTAB}(G).
%\end{equation*}
%%
%For perfect graphs, STAB$(G)$ and QSTAB$(G)$ coincide. A graph $G$ is called perfect if $\omega(G) = \chi(G)$ and if this also holds for any induced subgraph of $G$, where $\omega(G)$ is the clique number of $G$ (i.e. the size of the maximum clique) and $\chi(G)$ is the chromatic number of $G$ (i.e. the minimum number of colours needed to colour the vertices in $G$ such that any two adjacent vertices get distinct colours).
%
%The perceptive reader might have made a connection with matching theory and thought of another constraint to strengthen FSTAB$(G)$. Let a circuit be a chordless cycle and consider the circuit $G = C_{2k+1}$. The maximum independent set in this circuit is of size $k$, while the vertex $x = (\frac{1}{2}, \ldots, \frac{1}{2})$ with $\sum_i x_i = k + \frac{1}{2}$ is in FSTAB$(G)$. For every odd circuit $C$ we can define the following inequality.
%%
%\begin{equation}\label{eq:STAB4}
%\sum_{i \in V(C)} x_i \leq \frac{|V(C)| - 1}{2} \quad \textrm{for all odd circuits } C \textrm{ in } G,
%\end{equation}
%%
%and define the circuit constrained stable set polytope as follows.
%%
%\begin{equation*}
%\textrm{CSTAB}(G) = \textrm{conv.hull}( x \in \mathbb{R}^V \mid \textrm{Constraints } \eqref{eq:STAB1} \textrm{ and } \eqref{eq:STAB4} \textrm{ hold} ).
%\end{equation*}
%
%There are lots of results on all these polytopes, but for the Lov\'{a}sz Theta function we will restrict ourselves to STAB$(G)$ and QSTAB$(G)$.

\subsection{Orthogonal representations}\label{subsec:ONR}

\paragraph{Definition} This subsection introduces the Lov\'{a}sz Theta function via orthogonal representations. Let $G = (V,E)$ be a graph and let $\overline{E} = \{ \{i,j\} \in V \times V \mid \{i,j\} \not\in E \}$ be the complement of $E$. Formally, an orthogonal representation of $G$ is a mapping (labeling) $u: V \rightarrow \mathbb{R}^d$ for some $d$ such that $u_i^T u_j = 0$ for all $\{i,j\} \in \overline{E}$. In other words we need to assign a vector $u_v$ to every vertex $v$ such that the vectors of any two non-adjacent vertices are perpendicular to each other. Such a mapping always exists, in fact, there are two trivial mappings: map all vertices to 0, or map the vectors to a set of mutually orthogonal vectors in $\mathbb{R}^V$.

\paragraph{Orthogonal constrained stable set polytope}
An orthogonal representation $(u_i \mid i \in V)$ with $u_i \in \mathbb{R}^d$ is called orthonormal when every vector has unit length, i.e. when $\|u_i\| = 1$ for all $i \in V$. Let $c$ be some vector in $\mathbb{R}^d$ with $\|c\| = 1$ (for example, take $c = e_1$). Then for any stable set $S \subseteq V$ its vectors $\{u_i \mid i \in S\}$ are mutually orthonormal as the vertices are non-adjacent, and hence
%
\begin{equation*}
\sum_{i \in S} (c^T u_i)^2 \leq 1.
\end{equation*}
%
This is true because the left-hand side is the squared length projection of $c$ onto the subspace spanned by the $u_i$. The length of this projection is at most the length of $c$ which is 1. In fact, note that $\sum_{i \in V} (c^T u_i)^2 \chi^S = \sum_{i \in S} (c^T u_i)^2$, which yields that the following inequality holds for the incidence vector $\chi^S$ of any stable set $S \subseteq V$. It is called the orthogonality constraint.
%
\begin{equation}\label{eq:STAB5}
\sum_{i \in V} (c^T u_i)^2 x_i \leq 1.
\end{equation}
%
Similar like before, we can now define the orthogonal constrained stable set polytope as follows.
%
\begin{equation*}
\textrm{TSTAB}(G) = \textrm{conv.hull}( x \in \mathbb{R}^V \mid \textrm{Constraints } \eqref{eq:STAB1} \textrm{ and } \eqref{eq:STAB5} \textrm{ hold} ).
\end{equation*}

\subsection{The Lov\'{a}sz Theta function}\label{subsec:Theta}

%In general TSTAB$(G)$ is not a polytope. It is a convex set however, as it is the intersection of infinitely many halfplanes.
The most interesting property of TSTAB$(G)$ is the fact that one can optimise linear functions over it in polynomial time \cite[Theorem 9.3.30]{GLS}. %The relation of TSTAB$(G)$ to the clique constrained stable set polytope is considered in Section \ref{sec:SDP} as it is needed for the result on an SDP relaxation for $k$-set packing treated there.
%\subsection{Basic facts about the Lov\'{a}sz Theta function $\vartheta(G)$}\label{subsec:Theta1}
We can now succinctly define the Lov\'{a}sz Theta function.
%
\begin{equation*}
\vartheta(G) = \max \left\{ \sum_i x_i \mid x \in \textrm{TSTAB}(G) \right\}.
\end{equation*}
%
%In literature the leaning of an orthonormal representation is defined as $\sum_{i \in V} (c^T u_i)^2$, then $\vartheta(G)$ is the maximum leaning of any orthogonal representation.
The following is equivalent by writing out the definitions. Let ONR denote an orthonormal representation. Denote the following LP by \eqref{ThetaLP}.
%
\begin{equation}\tag{$\vartheta$-LP}\label{ThetaLP}
\begin{alignedat}{2}
\vartheta(G) = \text{max}  \quad & \sum_i x_i \ \\
               \text{s.t.} \quad & \sum_{i \in V} (c^T u_i)^2 x_i \leq 1 & \quad \forall c : \|c\| & = 1 \quad \forall \textrm{ONR}\{u_i\} \\
                                 & x_i \geq 0                            & \quad \forall i \in V
\end{alignedat}
\end{equation}
%
%Here are some basic facts about $\vartheta(G)$ relating it to other graph quantities and giving some bounds.
%%
%\begin{lemma}\cite{GLS,Lovasz1,SDP10}
%For a graph $G = (V,E)$ let $\vartheta(G)$ be defined as above. Define $\overline{G} = (V,\overline{E}) = (V,V^2 \setminus E)$. We have the following properties.
%%
%\begin{enumerate}
%  \item Let $\Theta(G)$ be the Shannon capacity of a graph. Then we have $\alpha(G) \leq \Theta(G) \leq \vartheta(G)$.
%  \item Suppose $G$ has an orthonormal representation in dimension $d$. Then $\vartheta(G) \leq d$.
%  \item We have $\vartheta(G) \vartheta(\overline{G}) \geq n$.
%  \item When $G'$ is a subgraph of $G$, we have $\vartheta(G') \leq \vartheta(G)$.
%  \item We have $\vartheta(K_n) = 1$ and $\vartheta(\overline{K_n}) = n$.
%  \item Define $\omega(G)$ as the size of the maximum clique in $G$ and $\chi(G)$ as the minimum number of colours needed to assign a colour to every vertex such that adjacent vertices are coloured distinctly. Then the Sandwich theorem holds: $\omega(G) \leq \vartheta(\overline{G}) \leq \chi(G)$.
%  \item Define $\kappa(G) = \max\{\sum_i x_i \mid x \in \textrm{QSTAB}(G)\}$ and let $\overline{\chi}(G)$ be the smallest number of cliques needed to cover all vertices in $G$. Then we have $\alpha(G) \leq \vartheta(G) \leq \kappa(G) \leq \overline{\chi}(G)$. As $\omega(G) = \alpha(\overline{G})$ and $\overline{\chi}(G) = \chi(\overline{G})$, this implies the Sandwich theorem above.
%  \item Define $S(G)$ as the collection of all independent sets in $G$ and $S(G,v) = \{ S \in S(G) \mid v \in S\}$. A fractional colouring of $G$ is an assignment of nonnegative values $f(S)$ to every element $S \in S(G)$ such that $\sum_{S \in S(g,v)} f(S) \geq 1$ for all $v \in V$. Define its weight to be the sum of all values and let the fractional chromatic number $\chi^*(G)$ be the minimum possible weight of any fractional colouring of $G$ \cite{ChiStar}. Then $\vartheta(G) \leq \chi^*(\overline{G})$.
%  \item Define the strong graph product of $G = (V,E)$ and $H = (V',E')$ as the graph $G \cdot H$ with vertex set $V \times V'$ and an edge between $(u_1,u_2)$ and $(v_1,v_2)$ if both $\{u_1,v_1\} \in E$ and $\{u_2,v_2\} \in E'$, or if $u_1 = v_1$ and $\{u_2,v_2\} \in E'$, or if $\{u_1,v_1\} \in E$ and $u_2 = v_2$. Then $\vartheta(G \cdot H) = \vartheta(G) \vartheta(H)$ and $\vartheta(\overline{G \cdot H}) = \vartheta(\overline{G}) \vartheta(\overline{H})$. This is the result used to upper bound $\Theta(G)$ by $\vartheta(G)$, see \cite{Shannon}.
%\end{enumerate}
%\end{lemma}

There are a lot of alternative and equivalent definitions for the Lov\'{a}sz Theta function. The semidefinite program for $k$-set packing uses the following equivalent definition, known in the literature as $\theta_3(G)$. An orthogonal representation $\{b_i\}$ is called normalised if $\sum_i \|b_i\|^2 = 1$, and define $\overline{G} = (V,\overline{E})$.
%
%\subsection{Alternative definitions of $\vartheta(G)$}\label{subsec:Theta2}
%
%Take the setting from the previous subsection and call an orthogonal representation $\{b_i\}$ normalized if $\sum_v \|b_v\|^2 = 1$. Then define the following quantity.
%
%\begin{equation*}
%\vartheta_1(G) = \min_{c,\{u_i\}} \max_{i \in V} \frac{1}{(c^T u_i)^2}.
%\end{equation*}
%%
%In order to introduce the other formulations we define the following. We call a matrix $A$ feasible for $G$ if it is a real symmetric matrix indexed by the vertices of $G$ with $A_{uu} = 1$ for all $u$ and $A_{uv} = 1$ if $uv \not\in E$. The other elements can be any real value (as long as $A$ is symmetric). For such a matrix $A$ let $\Lambda(A)$ be its maximum eigenvalue, or equivalently, $\Lambda(A) = \max\{x^T A x \mid \|x\| = 1\}$. Now define
%%
%\begin{equation*}
%\vartheta_2(G) = \min \{ \Lambda(A) \mid A \textrm{ is feasible for } G \}.
%\end{equation*}
%%
%Now call an orthogonal representation $\{b_i\}$ normalized if $\sum_v \|b_v\|^2 = 1$. Then
%%
\begin{equation*}
\vartheta_3(G) = \max \left\{ \sum_{u,v} b_u b_v \mid b \textrm{ is a normalised orthogonal representation of } \overline{G} \right\}.
\end{equation*}
%%
%Finally, let $c \in \mathbb{R}^d$ range over all vectors of unit length and let $\{b_i\}$ range over all orthonormal representation of $\overline{G}$ to define a quantity bearing some similarities to $\vartheta_1(G)$:
%%
%\begin{equation*}
%\vartheta_4(G) = \max_{d,\{b_i\}} \sum_{i \in V} (c^T b_i)^2.
%\end{equation*}
%%
%Then we have
%%
%\begin{lemma}(\cite[Theorem 9.3.12]{GLS},\cite[Theorem 12.1]{Lovasz1})\label{lem:Theta}
%For any graph $G$ we have
%%
%\begin{equation*}
%\vartheta(G) = \vartheta_1(G) = \vartheta_2(G) = \vartheta_3(G) = \vartheta_4(G).
%\end{equation*}
%\end{lemma}

%This is all the background we needed on semidefinite programming and the Lov\'{a}sz Theta function to state the theorem in the next section.

\section{An SDP for $k$-set packing}\label{sec:SDP}

%Given this background
This section contains the main theorem of this chapter. %Now apply this result to $k$-set packing as follows.
As before, view $k$-set packing as the independent set problem in a $k+1$-claw free graph and consider the following clique LP.
%
\begin{equation}\tag{Clique LP}\label{eq:CliqueLP}
\begin{alignedat}{2}
\text{max}  \quad & \sum_i x_i \ \\
\text{s.t.} \quad & x \in \textrm{QSTAB}(G)
\end{alignedat}
\end{equation}
%
This is optimising the size of an independent set over the clique constrained stable set polytope.
%
\begin{lemma}\label{lem:SDPnew1}
\eqref{eq:CliqueLP} is equivalent to the intersecting family LP of Chapter \ref{chap:LP}.
\end{lemma}
%
\begin{proof}
The nonnegativity constraints $x_i \geq 0$ for \eqref{eq:CliqueLP} obviously match the same constraints in the intersecting family LP. The clique constraints $\sum_{i \in Q} x_i \leq 1$ for cliques of size 1 imply the bound $x_i \leq 1$. Evidently they also imply the intersecting family constraints $x(K) \leq 1$ for $Q = K$. The constraints that $x(\delta(v)) \leq 1$ are also implied by the clique constraints: all hyperedges covering element $v$ form a clique in the conflict graph. The other way around is similar.
\end{proof}
%
Note that replacing QSTAB$(G)$ by TSTAB$(G)$ yields $\vartheta(G)$. These are related as follows.
%
\begin{lemma}(\cite[Lemma 4.3]{LapChiLau})\label{lem:SDPnew2}
Any feasible solution to \eqref{ThetaLP} is a feasible solution to \eqref{eq:CliqueLP}.
\end{lemma}
%
\begin{proof}
It suffices to show that the orthogonality constraints \eqref{eq:STAB5} imply the clique constraints \eqref{eq:STAB3}. Let $Q$ be a clique in $G$ and map all vertices of $Q$ to $c$ and all other vertices to mutually orthogonal vectors that are also orthogonal to $c$. Then the orthogonality constraint for $Q$ implies its clique constraint.
\end{proof}
%
In other words, for every graph $G$
%
\begin{equation*}%\label{eq:inclusions}
\textrm{STAB}(G) \subseteq \textrm{TSTAB}(G) \subseteq \textrm{QSTAB}(G).
\end{equation*}
%
Finally all linear and semidefinite programs can be linked.
%
\begin{lemma}\label{lem:SDPnew3}
$\vartheta_3(G)$ is a stronger relaxation than the intersecting family LP.
\end{lemma}
%
\begin{proof}
By Lemma \ref{lem:SDPnew2} \eqref{ThetaLP} is a stronger relaxation than the clique LP. Hence by Lemma \ref{lem:SDPnew1} \eqref{ThetaLP} is also stronger than the intersecting family LP. \eqref{ThetaLP} is equivalent to $\vartheta(G)$, which is equivalent to $\vartheta_3(G)$. Hence $\vartheta_3(G)$ is a stronger relaxation than the intersecting family LP.
\end{proof}
%
$\vartheta_3(G)$ can be written as follows.
%
\begin{equation}\tag{$\vartheta_3$-LP}
\label{Theta3LP}
\begin{alignedat}{2}
\vartheta_3(G) = \text{max}  \quad & \sum_{i,j \in V} u_i u_j \ \\
                 \text{s.t.} \quad & u_i u_j = 0,             & \quad \forall (i,j) \in E \\
                                   & \sum_{i=1}^n u_i^2 = 1   \\
                                   & u_i \in \mathbb{R}^d,    & \forall i \in V \quad \verb" "
\end{alignedat}
\end{equation}
%
This is a semidefinite program which is a stronger relaxation than the intersecting family LP. Then by Lemma \ref{lem:SDPnew3} and Theorem \ref{thm:IntegralityGap2} the following is true.
%
\begin{theorem}\label{thm:SDP2}
Let $\varepsilon > 0$. \eqref{Theta3LP} is an SDP relaxation for $k$-set packing with integrality gap at most $\frac{k}{3} + 1 + \varepsilon$. \textbf{[NOTE: see page iii]}
\end{theorem}
%
This is Theorem 1.5 from \cite{LapChiLau} with our improved bound on the integrality gap. Since this semidefinite program has a polynomial size, the following theorem is also true, similar to Theorem \ref{thm:LP2}.
%
\begin{theorem}\label{thm:SDP3}
Let $\varepsilon > 0$. There is a polynomially sized SDP for $k$-set packing with integrality gap at most $\frac{k}{3} + 1 + \varepsilon$. \textbf{[NOTE: see page iii]}
\end{theorem}

%\section{Extending the results to the weighted case}\label{sec:WeightedSDP} 