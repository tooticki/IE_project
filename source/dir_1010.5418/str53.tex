
\section{Stronger convergence}
\label{sec:str}

\setcounter{equation}{0}

In this section, we strengthen the convergence results of Section~\ref{sec:conv} under an additional condition, 
which we now explain.

Let $X'=(X'_n)_{n\geq0}$ a random walk independent from and equally distributed with $X$ and define 
\begin{equation}
\label{eq:int} 
\I_n=\I_n(X,X')=\{z\in\Z^d:\,X_i=X'_j=z\mbox{ for some }0\leq i,j\leq n\}=\RR_n(X)\cap\RR_n(X'),\, n\geq0,
\end{equation}
as the set of intersection points of the paths of $X$ and $X'$ up to (discrete) time $n$ (it can be seen also as indicated as the intersection of the
ranges of $X$ and $X'$ up to time $n$). Let now
\begin{equation}
\label{eq:nint} 
I_n=|\I_n|
\end{equation}
be the number of such intersection points. The additional condition we impose, in order that the results
of this section hold, is as follows.

\noindent{\bf Assumption} $\mathbf C$
\begin{equation}
\label{eq:adcond} 
\frac{I_n}{\esp(R_n)}\to0\mbox{ in probability as }n\to\infty.
\end{equation}

\begin{rmk}
 \label{rmk:adcond}
The expectation of the quotient in~(\ref{eq:adcond}) can be reexpressed as
\begin{equation}
\label{eq:adcond1} 
\frac{\sum_{x\in\Z^d}[\P(T_x\leq n)]^2}{\sum_{x\in\Z^d}\P(T_x\leq n)},
\end{equation}
where $T_x=\inf\{n\geq0:X_n=x\}$. We readily find that~(\ref{eq:adcond}) holds in either the general $d\geq2$ transient case, 
or the one dimensional  non integrable increment, transient case,
since in both these cases $\limsup_{\|x\|\to\infty}\P(T_x<\infty)=0$ (see e.g.~Proposition 25.3 in~\cite{kn:S}), and in general
$\lim_{n\to\infty}\esp(R_n)=\infty$.

It also holds for two dimensional mean zero, finite second moment random walks from results in~\cite{kn:L}.
We are uncertain about other recurrent planar walks, as well as 1-stable 1-dimensional recurrent walks. 
(See Remark~\ref{rmk:lln} above.)
\end{rmk}


Let $\B_u$ be the class of bounded uniformly continuous real functions on $(D,d)$. Here is the main result of this section.
Let $Y^{(\eps)}$ and $Z$ be as in Theorem~\ref{teo:conv1} above.

\begin{theo}
\label{teo:str}
Under Assumptions $A$, $B$ and $C$, for every $F\in\B_u$, we have
\begin{equation}
\label{eq:str}
\esp\left[\left.F(Y^{(\eps)})\right|\tau\right]\to\esp\left[F(Z)\right],
\end{equation}
in probability as $\eps\to0$.
\end{theo}



\begin{rmk}
\label{rmk:nec_cond}
As anticipated at the end of Section~\ref{sec:mod}, a condition like~(\ref{eq:adcond}) is needed for the validity of the above result. A case
where Assumptions $A$ and $B$ are satisfied, but not Assumption $C$, and~(\ref{eq:str}) does not hold, is when $X$ is one dimensional simple
asymmetric. This is particularly clear in the totally asymmetric case, when $C_n$ (see~(\ref{eq:clock})) is a partial sum of i.i.d.~random variables
in the basin of attraction of an $\a$-stable law, $\a\in(0,1)$, in which case it is well known to only converge, when properly rescaled, in
distribution. This prevents a result of the form of~(\ref{eq:str}) in that case.
\end{rmk}



\noindent{\bf Proof of Theorem~\ref{teo:str}}

Let $F\in\B_u$ be fixed. We may and will restrict to $F$ with bounded support, say $[0,T]$, where $T>0$ is arbitrary.

It follows from Theorem~\ref{teo:conv1} that 
\begin{equation}
\label{eq:s1}
\esp\left[F(Y^{(\eps)})\right]\to\esp\left[F(Z)\right].
\end{equation}
We will use this and~(\ref{eq:adcond}) to get~(\ref{eq:str}).

The strategy is to construct two sets of versions of $Y^{(\eps)}$. In each set of versions,
the different versions have independent dynamical random variables. The distinction is on the
environmental variables. On one set of versions the respective environments are also independent
among distinct versions, 
so that the versions are fully independent of one another: for this set the empirical 
mean of $F$ over the different versions yields the left hand side of~(\ref{eq:s1}) in the 
limit as the number of versions grows. 
On the other set of versions, we have a single environment for all the versions, so the empirical 
mean of $F$ over the different versions yields the left hand side of~(\ref{eq:str}).
% in the same limit as for the first set of versions. 
However, this single environment 
%of the latter set of versions 
can be constructed in a coupled way to the independent environments of the 
first set of versions, %, without loss of generality, 
so that the difference between
the empirical mean over the first set of versions and that over the second set of versions
vanishes as $\eps\to0$. This yields the result as soon as the limits as the number of versions
grows and as $\eps\to0$ are suitably taken.
We define the two sets of versions now.

%Let $\xk$, $k=1,2,\ldots$, be independent copies of $X$, and, for $k\geq1$,
Let
$X^{(1)},X^{(2)},\ldots$ and $\tau^{(0)},\tau^{(1)},\ldots$ be
iid copies of $X$ and $\tau$ respectively. 
For $k\geq1$, let $\rk$ 
%and $\txk$ 
be defined as in~(\ref{eq:range}),
%and~(\ref{eq:tx}) respectively, 
with $\xk$ replacing $X$.
Let now $\ZZ^{(1)}_n=\RR^{(1)}_n$ and for $k>1$  
\begin{equation}
\label{eq:zz}
\ZZ^{(k)}_n=\RR^{(k)}_n\setminus\left\{\cup_{i=1}^{k-1}\RR^{(i)}_n\right\}.
\end{equation}
We then define for each $N\geq1$
\begin{equation}
\label{eq:ttau} 
\tilde\tau^{(N)}=\{\tilde\tau_x^{(N)},\,x\in\Z^d\},
\end{equation}
where $\tilde\tau_x^{(N)}=\tau^{(k)}_x$, if $x\in\ZZ^{(k)}_N$ for some $k\geq1$, 
and $\tilde\tau_x^{(N)}=\tau^{(0)}_x$, otherwise. 

\begin{rmk}
 \label{rmk:tt}
$\tilde\tau^{(N)}$ and $\tau$ are equally distributed for every  $N\geq1$, whether or not $X^{(1)},X^{(2)},\ldots$ 
are given. 
%CHANGE 
In particular, $\tilde\tau^{(N)}$ is independent of $X^{(1)},X^{(2)},\ldots$.
\end{rmk}

Now let us consider two classes of clock processes $\ckn,\ck:\N\to[0,\infty)$, $k,N\geq1$:
\begin{equation}
\label{eq:cklock}
\ckn_n=\sum_{i=0}^n\tilde\tau^{(N)}_{\xk_i}\,T^{(k)}_i,\quad
\ck_n=\sum_{i=0}^n\tau^{(k)}_{\xk_i}\,T^{(k)}_i,\,\quad n\geq0,
\end{equation}
where $\{T^{(k)}_i,\,i\geq1\}=:T^{(k)},\,k\geq1$, are independent~families of iid mean 1 exponentials,
and their respective inverses $\ikn$ and $\ik$. Let then
\begin{equation}
\label{eq:yk}
\ykn_t=\tilde\tau^{(N)}_{\xk_{\ikn_t}},\quad \yk_t=\tau^{(k)}_{\xk_{\ik_t}},\quad t\geq0.
\end{equation}

\begin{rmk}
\label{rmk:ck} 
We remark that $\ckn=\ck=C$ and $\ykn=\yk=Y$ in distribution for all $k,N$; and that $\yk$, $k\geq1$, are iid; and 
$\ykn$, $k\geq1$, are iid given  $\tilde\tau^{(N)}$ for all $N$. 
%CHANGE 
The latter fact follows from the independence of
$\tilde\tau^{(N)}$ from $X^{(1)},X^{(2)},\ldots$ as remarked above (see Remark~\ref{rmk:tt}).
%{\sc This last fact ought to be better justified. In fact $\tilde\tau^{(N)}, \{\xk\}_k$ are independent although, say, 
%$\xk$ and $\tau^{(k)}$ are not.} 
\end{rmk}

The reason why the argument we outlined above, and then started to fill the details of, works is that, as already remarked
and used in the previous section, the contributions to the processes come from only a few deep traps (in the sense
specified at the proof of Theorem~\ref{teo:conv1}, see~(\ref{eq:taudelta}) and~(\ref{eq:tauedelta}) above), which 
are far off one another, and, given Asssumption $C$, are unlikely to lie in any intersection of ranges. 
For this reason the difference between $\yk$ and $\ykn$ 
(suitably rescaled) come from sites which {\em are not} deep traps, and thus are negligible. 
In order to make this argument, let us fix $\eps,\d>0$ and define for each $k,N\geq1$
\begin{eqnarray}
\label{eq:tked}
\tked\=\{\tked_x:=\tk_x\1\{\tk_x>\d (\eps q_\eps)^{-1}\},\,x\in\Z^d\},\\
\label{eq:ttned}
\ttned\=\{\ttned_x:=\tilde\tau^{(N)}_x\1\{\tilde\tau^{(N)}_x>\d (\eps q_\eps)^{-1}\},\,x\in\Z^d\},
\end{eqnarray}
and let
\begin{equation}
\label{eq:cked}
\ckned_n=\sum_{i=0}^n\tilde\tau^{(N,\eps,\d)}_{\xk_i}\,T^{(k)}_i,\quad\cked_n=\sum_{i=0}^n\tau^{(k,\eps,\d)}_{\xk_i}\,T^{(k)}_i,\,\quad n\geq0,
\end{equation}
with $\ikned$ and $\iked$ their respective inverses, and
\begin{equation}\nn
\label{eq:yked}
\ykne_t=\eps q_\eps\tilde\tau^{(N)}_{\xk_{\ikn_{\eps^{-1}t}}},\quad \ykned_t=\eps q_\eps\tilde\tau^{(N)}_{\xk_{\ikned_{\eps^{-1}t}}},\,\quad 
\yke_t=\eps q_\eps\tau^{(k)}_{\xk_{\ik_{\eps^{-1}t}}},\quad \yked_t=\eps q_\eps\tau^{(k)}_{\xk_{\iked_{\eps^{-1}t}}}.
\end{equation}

We then have that $\ykne=\yke=\ye$ in distribution; $\yke$, $k\geq1$, are iid; and 
$\ykne$, $k\geq1$, are iid given   $\tilde\tau^{(N)}$ for all $N$.



Consider now
\begin{equation}
\label{eq:av1}
\frac1{K}\sum_{k=1}^{K}F(\ykne)=\frac1{K}\sum_{k=1}^{K}F(\ykned)
+\frac1{K}\sum_{k=1}^{K}\rkned,
\end{equation}
where $K$ is an arbitrary positive integer, and $\rkned=F(\ykne)-F(\ykned)$.

With an argument similar to the one giving~(\ref{eq:c6}), one finds that 
\begin{equation}
\label{eq:av2}
\lim_{\d\to0}\limsup_{\eps\to0}\rkned=0
\end{equation}
in probability for all $k$.

%For $k,N\geq1$, $\eps,\d>0$, let 
%\begin{equation}
%\label{eq:jked}
%\jkned=\{0\leq i\leq N:\tk_{\txk_i}>\d a_\eps^{-1}\}.
%\end{equation}

Since $\ckn=\ck=C$ in distribution, we have that, given $\eta>0$, there exists $S=S_K>0$ such that 
\begin{equation}
\label{eq:av3}
\P\left(\ckn_{\nu_\eps S}\leq\eps^{-1}T\right)=\P\left(\ck_{\nu_\eps S}\leq\eps^{-1}T\right)\leq\frac\eta{2K}
\end{equation}
for all $N\geq1$ and $\eps$ sufficiently small (see the second point of Remark~\ref{rmk:conv2} above).

From now on we take $N=N_\eps=2\rho_{\nu_\eps S}$. 


For $k,\ell=1,\ldots,K$, let
\begin{equation}
\label{eq:av5}
A_{k,\ell}=\{\tk_{\xk_i}>\d (\eps q_\eps)^{-1}\mbox{ and }\xk_i\in\rl_{\nu_\eps S}\mbox{ for some }i\leq \nu_\eps S\},\quad
A_K=\cup_{k,\ell=1}^KA_{k,\ell}.
\end{equation}
Let also 
\begin{equation}
\label{eq:av6}
\I_{k,\ell}:=\I_{\nu_\eps S}(\xk,\xl)
\end{equation}
(see~(\ref{eq:int})).

Then, given $\lambda>0$
\begin{eqnarray}\nn
\P(A_{k,\ell})\=
\P\!\left(\sum_{x\in\I_{k,\ell}}\1\{\tk_x>\d (\eps q_\eps)^{-1}\}\geq1\right)\\
\label{eq:av7}
&\leq&\lambda N\P(\tau_0>\d (\eps q_\eps)^{-1})+\P(|\I_{k,\ell}|>\lambda N)+\P(R_{\nu_\eps S}^{(k)}>N).
\end{eqnarray}

By Assumption $B$, (\ref{eq:tail}, \ref{eq:ronr}, \ref{eq:vnu}),
the first term on the right of~(\ref{eq:av7}) is bounded above by
\begin{equation}
\label{eq:av8}
\lambda\, 3\d^{-\a} S^\a m\P(\tau_0>s_m)
\end{equation}
for all $\eps$ small enough, where $m=\rho_{\nu_\eps}$. By the definition of $s_m$ (see~(\ref{eq:sn}) above),
we may replace $m\P(\tau_0>s_m)$ by 1 in~(\ref{eq:av8}).

Putting this, Assumptions $A$ and~$C$ together, we conclude that $\P(A_{k,\ell})\to0$ as $\eps\to0$ for all $k,\ell$, and thus
\begin{equation}
\label{eq:av9}
\P(A_{K})\to0
\end{equation}
as $\eps\to0$ for every $K\geq1$.

We now go back to~(\ref{eq:av1}). By the above, the first term on its right can be written as
\begin{equation}
\label{eq:av10}
\frac1{K}\sum_{k=1}^{K}F(\yke)
+\frac1{K}\sum_{k=1}^{K}\rked
+\1\{A_K\cup B_K\}\frac1{K}\sum_{k=1}^{K}(F(\ykned)-F(\yked)),
\end{equation}
where
$\rked=F(\yked)-F(\yke)$ and
${\displaystyle B_K=\bigcup_{k=1}^K\left\{\{\ckn_{\nu_\eps S}\leq\eps^{-1}T\}\cup\{\ck_{\nu_\eps S}\leq\eps^{-1}T\}\right\}}$.
 This follows from the fact that outside $A_K\cup B_K$, we have that $\yked_t=\ykned_t$ for $t\in[0,T]$ and $1\leq k\leq K$, and 
all fixed $\eps,\delta,N$, as can be readily checked.

As in~(\ref{eq:av2}), we have that
\begin{equation}
\label{eq:av11}
\lim_{\d\to0}\limsup_{\eps\to0}\rked=0
\end{equation}
in probability for all $k$.

Now from (\ref{eq:av1},\ref{eq:av10}), and since  $F\in\B_u$, we get 
\begin{eqnarray}\nn
&\left|\esp\left[\left.F(\y1e)\right|\ttn\right]-\esp\left[F(Z)\right]\right|\leq&\\
&\left|\frac1{K}\sum_{k=1}^{K}\!\!\left(F(\ykne)-\esp\!\left[\left.F(\ykne)\right|\ttn\right]\right)\right|\!
+\!\left|\frac1{K}\sum_{k=1}^{K}\!\!\left(F(\yke)-\esp\!\left[F(\yke)\right]\right)\right|&
\label{eq:av12}
\end{eqnarray}
plus a term whose expectation is bounded above by 
\begin{equation}
\label{eq:av13}
\left|\esp(F(\y1e))-\esp(F(Z))\right|+\esp\left|\roned\right|+\esp\left|\r1ed\right|+2\|F\|_\infty\left(\P(A_K)+\P(B_K)\right),
\end{equation}
where we have used the fact that both $\esp\!\left[F(\yke)\right]$ and $\esp\!\left[\!\left.F(\ykne)\right|\ttn\right]$ are independent of $k$.

Recalling now Remark~\ref{rmk:ck}, and using Jensen, we find that the right hand side of~(\ref{eq:av12}) is bounded above by constant times
$K^{-1/2}$.
This and~(\ref{eq:s1}, \ref{eq:av2}, \ref{eq:av3}, \ref{eq:av9}, \ref{eq:av11}) yield
\begin{equation}
\label{eq:av14}
\limsup_{\eps\to0}\esp\left|\esp\left[\left.F(\y1e)\right|\ttn\right]-\esp\left[F(Z)\right]\right|\leq\mbox{const}\,(K^{-1/2}+\eta).
\end{equation}
Since $K$ and $\eta$ are arbitrary and the left hand side of~(\ref{eq:av14}) does not depend on either, we conclude that
\begin{equation}
\label{eq:av15}
\esp\left[\left.F(\y1e)\right|\ttn\right]\to\esp\left[F(Z)\right]
\end{equation}
in probability as $\eps\to0$, and the result follows from the fact that 
$\esp\left[\left.F(\y1e)\right|\ttn\right]$ and 
$\esp\left[\left.F(\ye)\right|\tau\right]$ have the same distribution for every $\eps>0,\, N\geq1$.
%$\left(\esp\left[\left.F(\y1e)\right|\ttn\right]\right)_{\eps>0}$ and 
%$\left(\esp\left[\left.F(\ye)\right|\tau\right]\right)_{\eps>0}$ have the same distribution.
$\square$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Stronger aging results}
\label{ssec:stra}

The above can be extended to strengthen the aging results of the previous section, under the same conditions of both this and that sections.
%(namely~(\ref{eq:adcond})).

\begin{theo}
\label{teo:ages}
If $X$ is transient and Assumption $C$ holds, then we have that
\begin{eqnarray}
\label{eq:ages_r}
&\lim_{t\to\infty}\P(Y_t=Y_{t+\theta t}|\tau)=R(\theta),&\\
\label{eq:ages_pi}
&\lim_{t\to\infty}\P(Y_t=Y_{t+r}\,\mbox{ for all }r\in[0,\theta t/q_{1/t}]|\tau)=\Pi(\theta),&
\end{eqnarray}
in probability as $t\to\infty$, where $R$ and $\Pi$ are as in Theorem~\ref{teo:age} (and indeed, both equal the right hand side of~(\ref{eq:age3})
above).
\end{theo}


\noindent{\bf Sketch of proof}


An argument like that of Lemma~\ref{lm:conv1} can be used to get that
\begin{equation}
\label{eq:conve1a}
(\yoed_t)\to(\bzd_t)
\end{equation}
as $\eps\to0$ in distribution on $(D,J_1)$ (in here, differently from Lemma~\ref{lm:conv1} case, $\tau$ is integrated; the argument might of course
use 
a version of $\tau$ such that~(\ref{eq:conve1a}) holds for $\tau$ in a set of full probability). We may then extend Theorem~\ref{teo:str} with a
similar 
proof to get
\begin{equation}
\label{eq:str1}
\esp\left[\left.G(Y^{(1,\eps,\d)})\right|\tau\right]\to\esp\left[G(\bzd)\right],
\end{equation}
in probability as $\eps\to0$, for every $\d>0$, with $G:D\to\{0,1\}$ such that either
\begin{equation}
\label{eq:g}
G(U)=\1\{U_1=U_{1+r}\,\mbox{ for all }r\in[0,\theta]\}\mbox{ or }G(U)=\1\{U_1=U_{1+\theta}\},\,U\in D. 
\end{equation}
We further need to extend Lemma~\ref{lm:comp}
to get that for all $t>0$
\begin{equation}
\label{eq:str2}
\lim_{\delta\to0}\limsup_{\eps\to0}\P(Y^{(1,\eps)}_t\ne Y^{(1,\eps,\d)}_t)=0
\end{equation}
(here the proof can be made again by using special versions of $\tau$ like in the argument for Lemma~\ref{lm:comp}).
%; it is at this point that the stronger Assumption $B'$ is used, like in the proof of that lemma).

From~(\ref{eq:str1}) and~(\ref{eq:str2}), and since $\esp\left[G(\bzd)\right]\to\esp\left[G(Z)\right]$ as $\d\to0$, we get
\begin{equation}\label{eq:str3}
\lim_{\eps\to0}\esp\left[\left.G(Y^{(1,\eps)})\right|\tau\right]
=\lim_{\delta\to0}\lim_{\eps\to0}\esp\left[\left.G(Y^{(1,\eps,\d)})\right|\tau\right]
=\lim_{\delta\to0}\esp\left[\left.G(\bzd)\right|\tau\right]
=\esp\left[G(Z)\right],
\end{equation}
in probability as $\eps\to0$, for $G$ as in~(\ref{eq:g}).
$\square$



\begin{rmk}
\label{rmk:strqage}
Under the same conditions of Theorem~\ref{teo:ages}, and by the same reasoning as above, we have that
\begin{equation}
\label{eq:strqage}
\textstyle \lim_{t\to\infty}\P(\sup_{r\in[0,t]}Y_r<\sup_{r\in[0,t+\theta t]}Y_{r}|\tau)=\Omega(\theta)
\end{equation}
in probability. (See Remark~\ref{rmk:age4} above.)
\end{rmk}



\begin{rmk}
\label{rmk:striage}
With the extra condition of this section, argueing as in Subsection~\ref{ssec:iage} above, with the help of Theorem~\ref{teo:str}, 
we may establish stronger results for integrated aging functions. We state the following result as an example,
in the spirit of Subsection~\ref{ssec:iage} above.
\begin{theo}
\label{teo:striage}
If $X$ satisfies Assumptions  $A$, $B$ and $C$, then 
\begin{equation}
\label{eq:striage}
\lim_{\lambda,\mu\to\infty\atop{\lambda/\mu\to\theta}}\esp[\bar R(\lambda{\mathcal T},\mu {\mathcal T})|\tau]=R(\theta)
\end{equation}
in probability, with $R$ as in Theorem~\ref{teo:age} above.
\end{theo}




\end{rmk}






