
\section{Convergence}
\label{sec:conv}

\setcounter{equation}{0}

This section is devoted to the proof of Theorem~\ref{teo:conv1}.
The main idea of the proof is to work with a %modified 
version of the
process in which the environment is dynamically constructed along the
trajectory of the walk and coupled to a stable process. Once
this %modified 
version is defined it is rather elementary to get the
convergence in a quenched form.

 
\begin{rmk}
 \label{rmk:inv}
We often in this and other sections consider the inverse $g$ of 
a (possibly random) monotonic unbounded function 
$f:{\mathbb D}\to[0,\infty)$, where ${\mathbb D}$ may be either 
$\N$ or $[0,\infty)$, defined (as usually)
\begin{equation}
  \label{eq:n}
  g(x)=\inf\{y\in{\mathbb D}:\,f(y)>x\}
\end{equation}
for $x\in[0,\infty)$. (Some times, we will write $f_n$ instead of $f(n)$.)
\end{rmk}

Let $X$ and $\tau$ be as in the previous sections. Assumptions $A$ and $B$ are in force.
In the build up to the proof of Theorem~\ref{teo:conv1}, we start by considering a particular construction of the law of $\X$ and $\E$
under $\P$, as follows.
Let $T_0,T_1,T_2,\ldots$ be a family of independent mean $1$ exponential random variables. Consider the following random function
$C:\N\to[0,\infty)$:
\begin{equation}
\label{eq:clock}
C_n=\sum_{i=0}^n\tau_{X_i} T_i,\quad n\geq0,
\end{equation}
and let $I$ denote its inverse. We may call $C=(C_n)$ the {\em clock process} associated to (this particular construction of) the trap model.
Now define for $t\geq0$
\begin{equation}
\label{eq:y}
Y_t=\tau_{X_{I_t}}.
\end{equation}
Note that $(X_{I_t})$  has the same law as $\X$ under $\P$, and $Y=(Y_t)$ has the same law as $\E$ under $\P$. Thus, making 
\begin{equation} 
\label{eq:yet}
Y^{(\eps)}_t=%\frac1{v_{\nu_\eps}}
\eps q_\eps Y_{\eps^{-1}t},\,t\geq0,
\end{equation}
we have that $Y^{(\eps)}=(Y^{(\eps)}_t)$ has the same law as $\E^{(\eps)}$ under $\P$. 




We will work with a particular version of $Y^{(\eps)}$ where a specific version of the (scaled) $\tau$ random variables coupled to $\Upsilon$ are
effectively placed over the range of $X$.
We define this specific version in~(\ref{eq:bye}) below. The proof of Theorem~\ref{teo:conv1} is then divided into first showing that that is
indeed a version, and next establishing convergence of the version.
The ingredients of the version are the above defined $X$ and $\Upsilon$, and a family of iid mean 1 exponential random variables
\begin{equation}
 \{\tj_i,\,j\geq0,\,i\geq1\}.
\end{equation}


Let us start by enumerating the {\em full range of $X$}
\begin{equation}\label{eq:enum}
 \RR:=\cup_{n\geq1}\RR_n=:\{\txt_0,\txt_1,\ldots\},
\end{equation} 
in {\em chronological} order (in the natural way, i.e., given $x,y\in\RR$, we have $x<y$ iff $X$ hits $x$ before it does $y$);
let $\psi:\N\to\N$ be the map
\begin{equation}
\label{eq:phi}
\psi(n)=m\quad\mbox{iff}\quad X_n=\txt_m.
\end{equation}

We now consider properly scaled $\tau_0$-distributed random variables, to be eventually placed over $\RR$, following the order therein.
For that let us (re)introduce
\begin{equation}
  \label{eq:vnu}
v_n=s_{\rho_n}\quad\mbox{and}\quad \phi_n=\frac{v_n}{r_n},
\end{equation}
where
\begin{equation}
  \label{eq:sn}
  s_n=\inf\{t\geq0:Q(t,\infty)\leq n^{-1}\}.%,
\end{equation}


Notice that all of this functions (of $n$), namely $v,\,\phi,\,s$ are nondecreasing and unbounded. %and regularly varying of index $\a^{-1}$.
They were discussed above (in the paragraph of~(\ref{eq:ae1})); we have now a more precise definition of $s_n$. 
Due to the regularly varying characters of $Q$ (see~(\ref{eq:tail})) and $r$ (Assumption B), $v,\,\phi,\,s$ 
are all regularly varying with a common index $\a^{-1}$).






It follows from elementary properties of monotonicity and regular variation of the above functions that
\begin{eqnarray}
  \label{eq:nuinf}
\nu_\eps\to\infty%\\%,
\end{eqnarray}
as $\eps\to0$, where $\nu_\eps$ was defined in~(\ref{eq:nue}) above.


Now let 
\begin{equation}
  \label{eq:te}
\te=1/\rho_{\nu_\eps},
\end{equation}
 and make, for $x\in\N$,
\begin{equation}
  \label{eq:taue}
  \tilde\tau_x^{(\eps)}=G^{-1}\!\left(\te^{-1/\a}(\ups_{\te x+\te}-\ups_{\te x})\right),
\end{equation}
where $G$ is defined by
\begin{equation}
  \label{eq:G}
  \P(\ups_1>G(y))=\P(\tau_0>y),\,y\geq0.
\end{equation}
By elementary properties of the subordinator $\ups$ and the definition of $G$, it readily follows that
$\{\tilde\tau_x^{(\eps)},\,x\in\N\}$ is an iid family with $\tilde\tau_0^{(\eps)}\stackrel{d}=\tau_0$, 
where ``$\stackrel{d}=$'' denotes equality in distribution.


We are now ready to define the version of the age process which will be used in the proof of Theorem~\ref{teo:conv1}. 
We start by defining the version of the clock process. For $n\geq0$, let
\begin{equation}
\label{eq:bclock}
\tilde C^{(\eps)}_n=\sum_{y\in\N} \tilde\tau_{y}^{(\eps)}
\sum_{i=1}^{L\!\left(\txt_{y},n\right)}\!\!\!\hat T^{(y)}_i,
\end{equation} 
and let $\tilde I^{(\eps)}$ denote the inverse of $\tilde C^{(\eps)}$. Then make
\begin{equation}
\label{eq:bye}
\tilde Y^{(\eps)}_t=%\frac1{v_{\nu_\eps}}
\tilde\tau^{(\eps)}_{\psi\left(\tilde I^{(\eps)}_t\right)},\,\,t\geq0.
%=g_\eps\!\left(\ups_{\te\left(\psi\left(\nu_\eps\bie_t)\right)+1\right)}-\ups_{\te\psi\left(\nu_\eps\bie_t)\right)}\right).
\end{equation}


\begin{rmk}
 \label{rmk:expl}
One may think of the latter version as one in which the environmental variables of the age process
$\tilde\tau^{(\eps)}=\{\tilde\tau_{x}^{(\eps)},\,x\in\,\N\}$
are placed over the range of $X$, $\{\txt_{x},\,x\in\N\}$, respectively.
The coupled construction of $\tilde\tau^{(\eps)}$ yields its strong convergence, when properly rescaled,
to its limiting counterpart $\ups$.
\end{rmk}



\noindent{\bf Proof of Theorem~\ref{teo:conv1}}

The result follows readily from Propositions~\ref{prop:version} and~\ref{lm:conv} stated and proven below.


$\square$

\begin{prop}
\label{prop:version}
For every $\eps>0$, $\tilde Y^{(\eps)}$ and $Y$ have the same distribution.
\end{prop}

Let $\hat Y^{(\eps)}_t=\eps q_\eps\tilde Y^{(\eps)}_{\eps^{-1}t},$ $t\geq0$.

\begin{prop}
\label{lm:conv}
For almost every $\ups$
\begin{equation}
\label{eq:conve}
\hat Y^{(\eps)}\to Z
\end{equation}
as $\eps\searrow0$ in distribution on $(D,d)$.
\end{prop}


\begin{rmk}
 \label{rmk:aeups}
The distribution referred to in the statement of Proposition~\ref{lm:conv} is the joint one of $X$ and $\{\tj_i\}$ (with $\ups$ fixed).
\end{rmk}



\noindent{\bf Proof of Proposition~\ref{prop:version}}

As noted right below~(\ref{eq:G}) above, we have that $\{\tau_{\hat X_y};\,y\in\N\}$ is independent of $X$ and
\begin{equation}
\label{eq:seq}
\{\tau_{\hat X_y};\,y\in\N\}\stackrel{d}=\{\tilde\tau^{(\eps)}_{y};\,y\in\N\}.
\end{equation}
It thus follows that
\begin{equation}
\label{eq:clock1}
C_n\stackrel{d}=\sum_{z\in\Z^d}\tau_{z}\sum_{i=1}^{L(z,n)} T^{(z)}_i
=\sum_{y\in\N}\tau_{\hat X_y}\sum_{i=1}^{L(\hat X_y,n)} T^{(\hat X_y)}_i
\stackrel{d}=\sum_{y\in\N}\tilde\tau^{(\eps)}_{y}\sum_{i=1}^{L(\hat X_y,n)} T^{(\hat X_y)}_i
\stackrel{d}=\sum_{y\in\N}\tilde\tau^{(\eps)}_{y}\sum_{i=1}^{L(\hat X_y,n)}\hat T^{(y)}_i,
\end{equation}
as vectors indexed by $n$, where $\{T^{(z)}_i;\,z\in\Z^d, i\geq1\}$ is an iid family of mean one 
exponential random variables.
$\square$

\bigskip



\noindent{\bf Proof of Proposition~\ref{lm:conv}}




It will be implicit (and sometimes explicit) in the claims made below that they hold for a.e.-$\ups$.
We will use the symbols ``$P$`` and ``$E$`` to denote the probability measure and expectation associated to the
distribution of $X$ and $\{\tj_i\}$ -- referred to sometimes below as dynamical random variables.
% -- (with $\ups$ fixed in a suitable full measure set).
%of the dynamical random variables.

We start by defining the set of {\em deep traps} or $\delta$-{\em traps}. 
For $x\geq0$, let $\mu(x)=\ups(x)-\ups(x-)$ and fix $\delta>0$ arbitrarily. Consider
\begin{equation}
\label{eq:taudelta}
\t_{\delta}=\{x\geq0:\, \mu(x)>\delta\}=\{x_1<x_2<\ldots\}.
\end{equation}
Let now $\yep_i=\lf\te^{-1}x_i\rf$, $i\geq1$, and define
\begin{equation}
\label{eq:tauedelta}
\teps_{\delta}=\{\yep_1,\yep_2,\ldots\}.
\end{equation}


Our strategy will be to consider modified versions of $\hat Y^{(\eps)}$ and $Z$,
respectively $\hat Y^{(\eps,\d)}$ and $Z^{(\d)}$, where only deep traps contribute
(see~(\ref{eq:c5}) below), and then show on the one hand that a version of~(\ref{eq:conve})
holds for the modified processes (see Lemma~\ref{lm:conv1}), and on the other hand that 
the errors due to the replacing $\hat Y^{(\eps)}$ with $\hat Y^{(\eps,\d)}$ 
are negligible (see Lemma~\ref{lm:conv2} below).
One other aspect to be considered in adopting this modified version, besides the fact that the 
main contributions for $\hat Y^{(\eps)}$ come from its largest values, or $\delta$-traps, 
is that these contributions are interspersed with the lesser contributions of shallower traps. 
This is illustrated in Figure~\ref{yte}, where the $\d$-traps 
are represented in solid horizontal lines, and the heights of the lesser traps do not appear (but, 
perhaps inconsistently, the respective sojourn times do appear). This interspersion of small 
and large traps does not occur to $\hat Y^{(\eps,\d)}$ -- see Figures~\ref{tyed} and~\ref{tyedp}
below.




\begin{figure}[htb]
\begin{center}
\input{yte.pspdftex}
\end{center}
\caption{Schematic realization of $\hat Y^{(\eps)}$. Largest values appear in full horizontal lines; other values are negligible as $\eps\to0$
and are not depicted.}
\label{yte}
\end{figure}





Let us first define a modified version of $\tilde Y^{(\eps)}$ in terms of a restricted clock process as follows. Let
\begin{equation}
\label{eq:rclock}
\tilde C^{(\eps,\d)}_n=\sum_{j\geq1} \tilde\tau_{\yep_j}^{(\eps)}\,\tilde T(\yep_j,n),
\end{equation}
where for $y\in\N$
\begin{equation}
\label{eq:tilt}
\tilde T(y,n)=\sum_{i=1}^{L\!\left(\txt_{y},n\right)}\!\!\!\hat T^{(y)}_i,
\end{equation}
and let $\tilde I^{(\eps,\d)}$ denote the inverse of $\tilde C^{(\eps,\d)}_n$ as a 
function of $n$. Then make 
\begin{equation}
\label{eq:byed}
\tilde Y^{(\eps,\d)}_t=%\frac1{v_{\nu_\eps}}
\tilde\tau^{(\eps)}_{\psi\left(\tilde I^{(\eps,\d)}_t\right)},\,\,t\geq0,
\end{equation}
and
\begin{equation}
\label{eq:hyed}
\hat Y^{(\eps,\d)}_t=%\frac1{v_{\nu_\eps}}
\eps q_\eps\tilde Y^{(\eps,\d)}_{\eps^{-1}t},\,\,t\geq0.
\end{equation}







\begin{figure}[htb]
\begin{center}
\input{tyed.pspdftex}
\end{center}
\caption{Schematic realization of $\hat Y^{(\eps,\delta)}$. Only values of $\delta$-traps appear, with sojourn times corresponding to
actual times spent in those values by $\hat Y^{(\eps)}$.}
\label{tyed}
\end{figure}


\begin{figure}[htb]
\begin{center}
\input{tyedp.pspdftex}
\end{center}
\caption{Schematic realization of $\hat Y^{(\eps,\delta')}$, $\delta'<\delta$. See Figure~\ref{tyed}.}
\label{tyedp}
\end{figure}

















%We now


Let us now define the modified versions of $Z$. Let
\begin{equation}
\label{eq:c4}
\bvd_s=\sum_{i=1,2,\ldots:\atop{x_i\leq s}}\mu(x_i)\,T_{x_i},\quad s\geq0,
\end{equation}
and consider the inverse $\bvd$ as a function of $s$, $\bwd$. Then let
\begin{equation}
\label{eq:c5}
\bzd_t=\mu(\bwd_t),\quad t\geq0.
\end{equation}






Theorem~\ref{teo:conv1} readily follows from Lemmas~\ref{lm:conv1},~\ref{rmk:conv_bvd} 
and~\ref{lm:conv2} below.


$\square$

%CHANGE

\begin{lm}
\label{lm:conv1}
Let $\delta>0$ be fixed.
Then as $\eps\searrow0$
\begin{equation}
\label{eq:conve1}
\byed\to\bzd
\end{equation}
in distribution on $(D,J_1)$, where $J_1$ is the usual Skorohod metric on $D$.
\end{lm}


\noindent{\bf Proof}





We start by fixing a positive integer $K$, and analysing the behavior of $\byed$ till it hits the $K+1$-st trap of
$\t_{\delta}$; in other words, till $X$ hits $\hat X_{\yep_{K+1}}$. 

For $j=1,2,\ldots$ let us define $\xe_j=\te\yep_j$ and 
\begin{equation}
\label{eq:mue}
\mue(\xe_j)=\eps q_\eps\tilde\tau^{(\eps)}_{\yep_j}.
\end{equation}
%, $\mu^{(\eps)}(x^{(\eps)}_{K+1})$. (Since $x_1,x_2,\ldots$ are almost surely all distinct, 
%we may assume $\eps$ small enough so that the same holds for $x_1,\ldots,x_{K+1}$.)
\noindent {\bf Claim 1} 
We claim that, outside an event of vanishing probability as $\eps\to0$, 
before $X$ hits $\hat X_{\yep_{K+1}}$, $\byed$ does nothing except 
\begin{itemize}
 \item[$i$.] visiting 
$\mue(\xe_1),\mue(\xe_2),\ldots\mue(\xe_K),$ 
successively, without backtracking;

 \item[$ii$.]  the sojourn times of those visits converge in distribution to an independent vector of $K$ 
exponential random variables with means $\mu(x_1),\ldots,\mu(x_K)$, respectively.
\end{itemize}



Proposition 3.1 of~\cite{kn:FIN} implies that for a.e.-$\ups$
\begin{equation}
\label{eq:mu}
\mue(\xe_j)\to\mu(x_j)\quad\mbox{as}\quad\eps\to0,
\end{equation}
for every $j\geq1$.
\begin{rmk}
 \label{rmk:fin}
Some matching of the notation of~\cite{kn:FIN} and the present one needs to be 
done in order to verify the above claim by resorting to that reference.
$V$ there corresponds to $\ups$ here. Respectively, $\eps$ corresponds to $\te$.
We have that $1/v_{\nu_\eps}(\sim\eps q_\eps)$ above corresponds to $c_{\te}$, 
with $c_\cdot$ introduced in (3.9) of~\cite{kn:FIN}, 
and our $\tilde\tau_x^{(\eps)}$ corresponds to $\tau_{x}^{(\te)}$ of that 
reference. (\ref{eq:mu}) follows then from the point process convergence
statement in Proposition 3.1 of~\cite{kn:FIN}.
\end{rmk}


Claim 1 above and~(\ref{eq:mu}) are the main ingredients of the proof of Lemma~\ref{lm:conv1}.

In order to justify Claim 1, let us introduce $\ze_i$, the hitting time of $\hat X_{\yep_i}$ 
by $X$, $i=1,2,\ldots$, and consider
\begin{equation}
\label{eq:l1}
L\!\left(\txt_{\yep_i},\ze_{i+1}\right),\,i=1,2,\ldots
\end{equation} 

Let us state a result concerning these hitting times, proven at the end of this section.
\begin{lm}
\label{lm:sclaim}
For all $s'>s>s''\geq0$ and $\d>0$, we have that outside an event of vanishing probability as $\eps\to0$
\begin{equation}
\label{eq:sc1}
L(\txt_{y},\,\nu_\eps s)=0 \mbox{ for all } y>\te^{-1}s',\,\mbox{ and }\,
L(\txt_{y},\,\nu_\eps s)>0 \mbox{ for all } y\in\N\cap[0,\te^{-1}s''].
\end{equation}
\end{lm}

\begin{rmk}
 \label{rmk:xitox}
The above statement is equivalent to
\begin{equation}
\label{eq:scalt}
\frac1{\nu_\eps} \ze_i\to x_i
\end{equation}
in probability as $\eps\to0$ for $i\geq1$.
\end{rmk}

It follows from Lemma~\ref{lm:sclaim} that, %almost surely for $\eps$ small enough 
outside an event of vanishing probability as $\eps\to0$, 
$\bced_{\nu_\eps s}$ vanishes if $s<x_1$, and is restricted to the $k$ 
first terms if $x_k<s<x_{k+1}$, $k=1,\ldots,K$.
Thus, given arbitrary $r_1<s_1<\ldots<r_{K+1}<s_{K+1}$ such that $0<r_i<x_i<s_i$, $i=1,\ldots,K+1$, 
outside an event of vanishing probability as $\eps\to0$ 
we have that 
%almost surely for all small enough $\eps$ and $i=1,\ldots,K+1$
\begin{equation}
\label{eq:l2}
\nu_\eps r_i\leq\ze_{i}\leq \nu_\eps s_i,
\end{equation}
and thus for $i=1,\ldots,K$
%\begin{eqnarray}\nn
%&L\!\left(\txt_{\te^{-1}\xe_i},\ze_{i}+\nu_\eps r_{i+1}-\nu_\eps s_i\right)
%\leq L\!\left(\txt_{\te^{-1}\xe_i},\ze_{i+1}\right)&\\\label{eq:l3}
%&\leq L\!\left(\txt_{\te^{-1}\xe_i},\ze_{i}+\nu_\eps s_{i+1}-\nu_\eps r_i\right).&
%\end{eqnarray}
\begin{equation}\label{eq:l3}%\nn
L\!\left(\txt_{\yep_i},\ze_{i}+\nu_\eps(r_{i+1}-s_i)\right)
\leq L\!\left(\txt_{\yep_i},\ze_{i+1}\right)%&\\\label{eq:l3}
\leq L\!\left(\txt_{\yep_i},\ze_{i}+\nu_\eps(s_{i+1}-r_i)\right)\!.
\end{equation}

%where $\tilde L(x,n)$ is the number of visits to $x$ up to time $n$ of a random walk with the same jump distribution of $X$ but starting at $x$.
Furthermore, by Corollary~\ref{rmk:labn} above, we get the following.


{\noindent \bf Key facts} Outside an event of vanishing probability as $\eps\to0$, we have equalities
in~(\ref{eq:l3}); namely
\begin{equation}\label{eq:kf}%\nn
L\!\left(\txt_{\yep_i},\ze_{i}+\nu_\eps(r_{i+1}-s_i)\right)
= L\!\left(\txt_{\yep_i},\ze_{i+1}\right)%&\\\label{eq:l3}
= L\!\left(\txt_{\yep_i},\ze_{i}+\nu_\eps(s_{i+1}-r_i)\right)\!,
\end{equation}
$i=1,\ldots,K$, and indeed also
\begin{equation}\label{eq:kf1}%\nn
L\!\left(\txt_{\yep_i},\ze_{i+1}\right)%&\\\label{eq:l3}
=L\!\left(\txt_{\yep_i},\ze_{K+1}\right)\!,\, i=1,\ldots,K.
\end{equation}

The first part of the Claim 1 at the beginning of this proof is thus established.
















To argue the second part, we start by observing that
the sojourn time of $\tilde Y^{(\eps,\d)}$ on $\yep_j$, $j=1,\ldots,K$, up until $X$ hits 
$\hat X_{\yep_{K+1}}$ is given by 
\begin{equation}
\label{bte}
\tilde\tau^{(\eps)}_{\yep_j}\sum_{i=1}^{L\!\left(\txt_{\yep_j},\,\ze_{K+1}\right)}\!\!\hat T^{(\yep_j)}_{i},
\end{equation}
so the respective time spent by $\hat Y^{(\eps,\d)}$ is $\eps$ times that, which can be rewritten as
\begin{equation}
\label{bte1}
\mue(\xe_j)\,\tilde T^{(\eps)}_j,
\end{equation}
where
\begin{equation}
\label{bte2}
\tilde T^{(\eps)}_j=\,r_{\nu_\eps}\!\!\!\!\!\sum_{i=1}^{L\!\left(\txt_{\yep_j},\,\ze_{K+1}\right)}\!\!\hat T^{(\yep_j)}_{i}.
\end{equation}
(Let us recall that $q_\eps^{-1}=r_{\nu_\eps}$.)













From the above we conclude that outside an event of vanishing probability as $\eps\to0$, 
up until $X$ hits $\hat X_{\yep_{K+1}}$, we have that
%$\byed_t$ equals $\bzed_t=\mue(\bwed_t)$, where $\bwed$ is the inverse of 
\begin{equation}
\label{eq:tzed}
\byed_t=\bzed_t:=\mue(\bwed_t),
\end{equation}
where $\bwed$ is the inverse of 
\begin{equation}
\label{eq:l7}
\bved_s=\sum_{j=1,2,\ldots:\atop{x_j\leq s}}\mue(\xe_j)\,\bte_j.
\end{equation}

We now argue that $\bte_j,\,j=1,2,\ldots,$ are asymptotically 
independent mean 1 exponential random variables as $\eps\to0$.
By the key fact~(\ref{eq:kf}) and Remark~\ref{rmk:labn} above, and the Markov property,
we have that for $a>0$, outside an event of vanishing probability as $\eps\to0$, 
each $\bte_j$ coincides with
\begin{equation}
r_{\nu_\eps}\!\!\!\!\sum_{i=1}^{L\!\left(\txt_{\yep_j},\,\ze_{j}+\nu_\eps(r_{j+1}-s_j)\right)}\!\!\hat T^{(\yep_j)}_{i}
=r_{\nu_\eps}\!\!\!\!\sum_{i=1}^{L\!\left(\txt_{\yep_j},\,\ze_{j}+a\nu_\eps\right)}\!\!\hat T^{(\yep_j)}_{i},
\end{equation}
and each of the latter quantities is equally distributed with
\begin{equation}
\label{eq:l5}
r_{\nu_\eps}\!\!\sum_{i=1}^{L\!\left(0,a\nu_\eps\right)}\!\!\!\hat T^{(1)}_i,
\end{equation}
$j=1,\ldots,K$.
%and this quantity is bounded above and below by $L\!\left(0,a \nu_\eps\right)$ (with a different $a>0$ for %{\sc CHANGE} 
%the lower and upper bound) as soon as $\eps$ is small enough.

Corollary~\ref{cor:approx1} %and~(\ref{eq:eq}) 
now implies that for any fixed $a>0$ the distribution of~(\ref{eq:l5})
converges to a mean $1$ exponential one as $\eps\to0$. This and~(\ref{eq:mu}) 
establish the exponentiality part of Claim 1.$ii$.

To establish the independence part, it is enough to argue that for $j\geq2$ we have that $\bte_j$
is asymptotically independent of the vector 
\begin{equation}
\label{eq:l7a}
(\bte_i,\,i=1,\ldots, j-1).
\end{equation}
By the key facts~(\ref{eq:kf}-\ref{eq:kf1}), we have that,
outside an event of vanishing probability as $\eps\to0$,
\begin{equation}
\bte_j=
r_{\nu_\eps}\!\!\!\!\sum_{i=1}^{L\!\left(\txt_{\yep_j},\,\ze_{j}+\nu_\eps(r_{j+1}-s_j)\right)}\!\!\hat T^{(\yep_j)}_{i}.
\end{equation}
Since $\ze_{j}$ is a stopping time, the strong Markov property yields the independence
of the latter random variable and the vector in~(\ref{eq:l7a}), and the claimed asymptotic
independence is established.
Claim 1 is thus established.
% by the identification $\bar T^{(\eps)}_i=\bte(\xe_i,\xie_{i+1})$.


In order to complete the argument for this proof, we first observe that we may replace $D$ 
with $D_T$, $T>0$ arbitrary. 

Note that from the above arguments, it follows that for every $s\geq0$ fixed, we have that
\begin{equation}\label{vtov}
 \bved_s\to V^{(\d)}_s
\end{equation}
in distribution as $\eps\to0$.
It is clear that 
\begin{equation}
\label{eq:vdinf}
\bvd_s\to\infty \mbox{ as } s\to\infty 
\end{equation}
almost surely, so given $\eta>0$, we may choose $S\notin\t_\d$ such that for all small enough $\eps$
\begin{equation}
\label{eq:st}
P(\bved_S>T)>1-\eta. 
\end{equation}
Let $K=|\t_{\delta}\cap[0,S]|$. Then outside an event of probability at
most $\eta$ the distance $D_T$ is bounded above by $D_{\bved_S}$, and combining this with Claim 1 at the beginning
of this proof, 
we get that outside an event of probability at most $2\eta$, uniformly in $\eps$ small, within $[0,T]$,
$\byed$ visits $\mue(\xe_1),\ldots,\mue(\xe_K)$ successively without backtracking (not necessarily all of them
on $[0,T]$),
with sojourn times converging to independent exponential random variables
with means $\mu(x_1),\ldots,\mu(x_K)$, respectively. 
The result follows.
%
$\square$


\begin{rmk}
 \label{rmk:conv2}\mbox{ }
\begin{enumerate}
\item We claimed above that the convergence in~(\ref{vtov}) holds for fixed times $s$.
In the next section we will need a stronger version of that convergence, namely one that holds for 
the trajectories, under the $J_1$ metric. It is pretty clear from the ingredients at end of the above proof that
\begin{equation}
\label{conv4}
\lim_{\eps\to0}(\bved_t)\stackrel{d}=(\bvd_t)
\end{equation}
on $(D,J_1)$. Let us sketch a brief argument. Indeed, $(\bved_t)$ is a jump process with jumps located at $\xe_1,\xe_2,\ldots$ with respective sizes
$\mue(\xe_1)\bte_1,\mue(\xe_2)\bte_2,\ldots$. As established above, the jump
locations converge to $x_1,x_2,\ldots$, and the jump sizes converge in distribution to independent exponentials of respective
means $\mu(x_1),\mu(x_2),\ldots$, which is a description of $(\bvd_t)$, and the convergences of jump locations and sizes
imply $J_1$-convergence.




\item Another point to be used below: it immediately follows from~(\ref{eq:l2}) that if $s\notin\t_\d$ then
\begin{equation}
\label{cv}
\eps\bced_{\nu_\eps s}=\bved_s
\end{equation} 
outside an event of vanishing probability as $\eps\to0$. It follows from~(\ref{vtov}) that,
for every fixed $s\geq0$, $\eps\bced_{\nu_\eps s}\to V^{(\d)}_s$
in distribution as $\eps\to0$. 





In particular, we may conclude that given $T,\eta,\delta>0$, there exist $\eps_0,S>0$
such that $\P(\eps\bce_{\nu_\eps S}\leq T)\leq\P(\eps\bced_{\nu_\eps S}\leq T)\leq\eta$ for all $\eps<\eps_0$.

\end{enumerate}


\end{rmk}


\begin{lm}
 \label{rmk:conv_bvd}
We have that
\begin{equation}
\label{eq:c5a}
(\bvd_t)\to(V_t)\, \mbox{ and }\, (\bzd_t)\to(Z_t)
\end{equation}
almost surely on $(D,J_1)$ as $\delta\to0$. 
\end{lm}



\noindent{\bf Proof}

Using the fact that 
$\sum_{x\in\t_{\delta}^c\cap[0,T]}\mu(x)\to0$ as $\delta\to0$ for arbitrary $T$,
the proof follows readily.

$\square$




We will claim several times below that for certain random variables depending on two parameters $\eps$ and $\d$,
called generically now $\Xi^{(\eps,\d)}$, it holds that $\lim_{\delta\to0}\limsup_{\eps\to0}\Xi^{(\eps,\d)}=0$
in probability. This means that for every $\eta>0$, we have that
$\lim_{\delta\to0}\limsup_{\eps\to0}\P(\Xi^{(\eps,\d)}>\eta)=0$.





\begin{lm}
\label{lm:conv2}
\begin{equation}
\label{eq:c6}
\lim_{\delta\to0}\limsup_{\eps\to0}d((\byed_t),(\hye_t))=0
\end{equation}
in probability. 
\end{lm}



\noindent{\bf Proof}

To establish~(\ref{eq:c6}), we introduce a further auxiliary process. The reason for this is the following
difficulty in the direct comparison between $\byed$ and $\hye$. The trajectories of those two processes are
somewhat poorly {\em aligned} when the latter one is visiting $\d$-traps --- which is the only occasion they 
can agree, since the former process lives on $\d$-traps. By poor alignment of these processes we mean that 
there are times when $\hye$ is visiting
%in its $i$-th visit of 
a given $\d$-trap, while $\byed$ is 
%either at its $i$-th visit of the same $\d$-trap, with $j>i$ (this is not really a problem, 
%since both processes agree on those times)
visiting a different $\d$-trap.
This is perhaps apparent on a comparison of Figures~\ref{yte} and~\ref{tyed}.
With the aim of minimizing this {\em bad} misalignment, we introduce a process,
$\vyed$, living on $\d$-traps which is {\em better} aligned with $\hye$ in the sense that the poor alignment 
described above does not take place, and which is easily compared to $\byed$ as well. More on this discussion 
after the definition of $\vyed$ next.


Let  $\bar x_0=0$, $\bar x_i=(x_i+x_{i+1})/2$, $i\geq1$, and define
\begin{equation}
\label{eq:vyed}
\vyed_t=\mue(\xe_i),\,\mbox{ if }\, \eps\bce_{\nu_\eps\bar x_{i-1}}<t\leq\eps\bce_{\nu_\eps\bar x_{i}},\,\,i\geq1.
\end{equation}

\begin{figure}[htb]
\begin{center}
\input{yted.pspdftex}
\end{center}
\caption{Schematic realization of $\bar Y^{(\eps,\delta)}$. Only values of $\delta$-traps appear, with sojourn times (slightly) longer than
actual times spent in those values by $\hat Y^{(\eps)}$. See Figure~\ref{tyed}.}
\label{byed}
\end{figure}


\begin{figure}[htb]
\begin{center}
\input{ytedp.pspdftex}
\end{center}
\caption{Schematic realization of $\bar Y^{(\eps,\delta')}$. See Figure~\ref{tyedp}.}
\label{byedp}
\end{figure}

Coming back to the alignment issue, we note that on any given finite time interval, outside an event of vanishing 
probability as $\eps\to0$,
%, even though $\vyed$ is not aligned with $\bye$, it has the following advantage with respect to $\byed$: 
everytime $\hye$ is visiting a $\d$-trap, $\vyed$ is visiting the same trap. Since $\vyed$ lives on
$\d$-traps, the $d$ distance between $\vyed$ and $\hye$ on a given time interval is, outside an event of vanishing 
probability as $\eps\to0$, bounded above by the size of the deepest $\d$-trap in that interval multiplied by the total 
time spent by $\hye$ outside $\d$-traps during that interval.

The comparison between $\vyed$ and $\byed$ is also quite simple, perhaps simpler. Both processes live on $\d$-traps, which 
they visit in the same order, with distinct sojourn times of order 1, which approach each other in the limit as $\eps\to0$. 
So we indeed have the vanishing of the $J_1$ distance of $\vyed$ and $\byed$ in the limit as $\eps\to0$.

We make these arguments more precisely now. We have that~(\ref{eq:c6}) follows from
\begin{eqnarray}
\label{eq:c7}
&\lim_{\delta\to0}\limsup_{\eps\to0}d((\hye_t),(\vyed_t))=0,&\\
\label{eq:c8}
&\lim_{\delta\to0}\limsup_{\eps\to0}J_1((\vyed_t),(\byed_t))=0,&
\end{eqnarray}
in probability.



To establish~(\ref{eq:c7}),  we claim that it is enough to consider $d_T$ instead of $d$, with
\begin{equation}
\label{eq:N}
T=T(\eps,S)=\eps\sum_{y\in\N} \tilde\tau_{y}^{(\eps)}\,\tilde T(y,\nu_\eps S),
\end{equation}
$S$ fixed arbitrarily.  To justify the claim, it suffices to argue that $T\to\infty$ in probability as $S\to\infty$ 
uniformly in $\eps$ at a
neighborhood of the origin. This can be done as follows. $T$ clearly dominates $\eps\bced_{\nu_\eps S}$ 
(see~(\ref{eq:rclock}) above); it then follows
from~(\ref{cv}) and~(\ref{eq:vdinf}) that
the latter quantity diverges in probability as $S\to\infty$ uniformly in $\eps$ around the origin. 
This closes the argument for the claim. 

Reasoning now again as in the proof of Lemma~\ref{lm:conv1}, we find that outside 
an event of vanishing probability as $\eps\to0$, $\hye$ and $\vyed$ coincide within $[0,T]$ 
whenever the first process is in a $\d$-trap. Since the set of $\d$-traps visited during $[0,T]$ 
is contained in $[0,S+1]$, we conclude that the $d_T$ distance between the two processes is bounded 
above by 
\begin{equation}
\label{eq:c9}
\max\{\mue(\xe_i),\,i\geq1,\,\xe_i\leq S+1\}\,\,\eps\!\!\!\!
\sum_{y\in\N\setminus\teps_\delta} \tilde\tau_{y}^{(\eps)}\,\bte(y,\nu_\eps S).
\end{equation}
(\ref{eq:c7}) then follows from~Lemma~\ref{lm:neg} below and the fact that 
the max is over a bounded set uniformly in $\eps$ and~(\ref{eq:mu}).


To establish~(\ref{eq:c8}), we again replace $D$ by $D_T$, this time $T$ deterministic, but otherwise 
arbitrarily fixed. For an arbitrary $\eta>0$, let $S$ be as in the second point of Remark~\ref{rmk:conv2}. 
Then, arguing as in the proof of Lemma~\ref{lm:conv1} (see the Claim 1 at the beginning of that 
proof, and also the paragraph of~(\ref{eq:tzed})), on the event that $\eps\bced_{\nu_\eps S}>T$ and outside 
an event of vanishing probability as $\eps\to0$, $(\byed_t)_{t\in[0,T]}$ successively visits the set of states 
$\{\mue(\xe_i),\,i\geq1:\,\xe_i\leq S+1\}$ (not necessarily all of them by time $T$, but in any case in that order),
with respective sojourn times 
$\{\mue(\xe_i)\,\bte_i,\,i\geq1:\,\xe_i\leq S+1\}$.
The same is of course true of $(\vyed_t)$, except that the sojourn times are given by 
$\{\tilde S^{(\eps)}_i:=\eps\bce_{\nu_\eps\bar x_{i}}-\eps\bce_{\nu_\eps\bar x_{i-1}},\,i\geq1:\,\xe_i\leq S+1\}$.                    
Furthermore, for $i\geq1$ such that $\xe_i\leq S+1$, we have that outside an event whose probability vanishes as $\eps\to0$,
$\tilde S^{(\eps)}_i\geq\hat S^{(\eps)}_i:=\mue(\xe_i)\, \bte_i$ for such $i$, 
and for the same $i$ the difference between $\tilde S^{(\eps)}_i$ and $\hat S^{(\eps)}_i$ is bounded above by




















\begin{equation}
\label{eq:c10}
%\sum_{y\in\N\setminus\teps_\delta} \tilde\tau_{y}^{(\eps)}\,\bte(y,\nu_\eps S).
\eps\!\!\!\!\sum_{x\in\te\N\cap[\bar x_{i-1},\,\bar x_{i}]\setminus\{\xe_i\}}
\tilde\tau_{\te^{-1}x}^{(\eps)} \,\bte(\te^{-1}x,\nu_\eps S)
\leq
\eps\!\sum_{y\in\N\setminus\teps_\delta}\tilde\tau_{y}^{(\eps)}\,\bte(y,\nu_\eps S),
\end{equation}
%with $\hat x_i=(3x_{i-1}+x_{i})/4$,  $\check x_i=(x_{i}+3x_{i+1})/4$. 
%We now observe that %the expected value of 
%the latter expression %in~(\ref{eq:c10})
and the latter expression vanishes in probability as $\eps\to0$
by Lemma~\ref{lm:neg}. The result follows since $\eta$ is arbitrary.
$\square$


\begin{rmk}
 \label{rmk:byed}
It follows from arguments in the last paragraph of the above proof together with other arguments that for all $t>0$ fixed
\begin{equation}
\label{eq:comp1a}
\lim_{\delta\to0}\limsup_{\eps\to0}P(\byed_t\ne\vyed_t)=0.
\end{equation}
Indeed for $t<T$, with $T$, $\eta$ and $S$ as in the proof of~(\ref{eq:c8}) above (see last paragraph of the above proof),
and setting $K=\max\{i\geq1:\,x_i<S+2\}$, let 
\begin{equation}
\tse_j=\sum_{i=1}^j\tilde S^{(\eps)}_i,\quad\hse_j=\sum_{i=1}^j\hat S^{(\eps)}_i,\,j=1,\ldots,K,
\end{equation}
where $\tilde S^{(\eps)}$ and $\hat S^{(\eps)}$ were defined right above~(\ref{eq:c10}). Then, since every random variable involved
is continuous and $\tilde S^{(\eps)}_j\geq\hat S^{(\eps)}_j$, $1\leq j\leq K$ (outside an event whose probability vanishes as $\eps\to0$),
we have that $\{\byed_t\ne\vyed_t\}$ %the event inside the probability sign in~(\ref{eq:comp1a}) 
is almost surely contained in 
$\cup_{j=1}^K\{\hse_{j}<t<\tse_{j}\}$. Now the fact argued at end of the above proof that the difference 
between $\hat S^{(\eps)}_{j}$ and $\tilde S^{(\eps)}_{j}$ vanishes as $\eps\to0$ in probability for $j=1,\ldots,K$,
and the fact that $\hat S^{(\eps)}_1,\ldots,\hat S^{(\eps)}_K$ are asymptotically independent, lead readily to the completion of the argument.
\end{rmk}





\begin{lm}
\label{lm:neg}
We have that
\begin{equation}
\label{eq:c2a}
\lim_{\delta\to0}\limsup_{\eps\to0}\,\,\eps\!\!\!\!\sum_{y\in\N\setminus\teps_\delta} 
\tilde\tau_{y}^{(\eps)}\,\bte(y,\nu_\eps s)=0
%\sum_{x\in\te\N\cap(\teps_\delta)^c}\mue(x)\,\bte(x,s)=0
\end{equation}
in probability for every $s\geq0$. 
\end{lm}


\noindent{\bf Proof}


We start by resorting to the Markov property and Corollary~\ref{cor:approx} % and~(\ref{eq:eq}), 
to find that for every $s>0$
\begin{equation}
\label{eq:expcurly}
r_{\nu_\eps}E(\bte(y,\nu_\eps s))\leq r_{\nu_\eps}E(L(0,\nu_\eps s))=
r_{\nu_\eps}U_{\nu_\eps s}\to1\quad\mbox{as}\quad\eps\to0.
\end{equation}

We next resort to Lemma~\ref{lm:sclaim} to restrict the sum in~(\ref{eq:c2a}) on $x\leq \te^{-1}s'$. 
By~(\ref{eq:expcurly}) the expectation of the restricted sum is bounded above by constant times 
\begin{equation}
\label{eq:c3}
\sum_{y\in\N\cap(\teps_\delta)^c\cap[0,\te^{-1}s']}\!\!\!\!\eps q_\eps\tilde\tau_{y}^{(\eps)} 
\end{equation}
(recall that $q_\eps=r_{\nu_\eps}^{-1}$). We now claim that the $\lim_{\delta\to0}\limsup_{\eps\to0}$ 
of~(\ref{eq:c3}) vanishes almost surely. In order to use results of~\cite{kn:FIN} for that, let us
extend to $\te\N$ the measure $\mue$ defined above for rescaled $\d$-trap sites only (see~(\ref{eq:mue})).
For $x\in\te\N$, let
\begin{equation}
 \label{eq:muee}
\mue(x)=\eps q_\eps\tilde\tau_{\te^{-1}x}^{(\eps)}.
\end{equation} 
Here we abuse notation by writing $\mue(\cdot)$, $\mu(\cdot)$ instead of $\mue(\{\cdot\})$, $\mu(\{\cdot\})$.

We may then rewrite the sum~(\ref{eq:c3}) as 
\begin{equation}
\label{eq:sf}
\mue([0,s'])-\mue([0,s']\setminus\te\t_\d^{(\eps)})
\end{equation}
Now by~(\ref{eq:mu}) we have that the second term of~(\ref{eq:sf}) converges to
$\mu([0,s']\cap\t_\d)$ almost surely as $\eps\to0$. It follows from Proposition 3.1 of~\cite{kn:FIN} (the vague 
convergence part) that $\mue([0,s'])\to\mu([0,s'])=\ups(s')$ almost surely as $\eps\to0$. See Remark~\ref{rmk:fin} 
above. Here we may use the fact that deterministic points like $s'$ are continuity points of $\ups$ almost surely.

We then have that~(\ref{eq:c3}) converges almost surely to 
\begin{equation}
\label{eq:sf1}
\mu([0,s']\setminus\t_\d) 
\end{equation}
as $\eps\to0$. It is again a standard fact that the expression in~(\ref{eq:sf1}) vanishes 
almost surely as $\d\to0$, and the claim and lemma follow. 
%
$\square$


\medskip


\noindent{\bf Proof of Lemma~\ref{lm:sclaim}} 


We start by pointing out that
\begin{equation}
\label{eq:condl}
L(\txt_y,\nu_\eps s)=0 \,\mbox{ if and only if } y>R_{\nu_\eps s}. 
\end{equation}
Given $\eta>0$ to be chosen below, consider the event
\begin{equation}
\label{eq:event}
\left\{\left|\frac{R_{\nu_\eps s}}{\rho_{\nu_\eps s}}-1\right|>\eta\right\}.
\end{equation}
Assumption A implies that the probability of this event vanishes as $\eps\to0$.

Now write $\rho_{\nu_\eps s}$
%the second factor in the product on the right hand side of the inequality within braces in~(\ref{eq:event}) 
as
\begin{equation}
\label{eq:rest1}
\frac{\rho_{\nu_\eps s}}{\nu_\eps s\,r_{\nu_\eps s}}\,%{\nu_\epsr_{\nu_\eps s}}{r_{\nu_\eps}}
%\frac{\nu(\eps_n^{-1}s)}{\nu(\eps_n^{-1})}\frac{r_{\nu(\eps_n^{-1}s)}}{r_{\nu(\eps_n^{-1})}}
%\frac{\nu_\eps s}{\nu_\eps}\,
s\,
\frac{r_{\nu_\eps s}}{r_{\nu_\eps}}\,
\frac{\nu_\eps r_{\nu_\eps}}{\rho_{\nu_\eps}}\,\te^{-1}.
\end{equation}
From~(\ref{eq:ronr}) and~(\ref{eq:nuinf}), we get that the $\lim_{\eps\to0}$ of the first and third quotients in~(\ref{eq:rest1}) both equal 1.
%(\ref{eq:convn}) says that the $\lim_{\eps\to0}$ of the second quotient equals $s^\a$.
Assumption B %and~(\ref{eq:nuinf}) 
implies that the $\lim_{\eps\to0}$ of the second quotient equals 1.

Now let us choose $\eta>0$ small enough so that $(1+2\eta)s\leq s'$ and $(1-2\eta)s\geq s''$. 
From the conclusion of the previous paragraph, we get that, for all $\eps$ small enough, the events
\begin{eqnarray}
\label{eq:events}
&\{R_{\nu_\eps s}\geq\te^{-1} x\,\mbox{ for some }\,x>s'\}=\{R_{\nu_\eps s}>\te^{-1}s'\}&\\
&\{R_{\nu_\eps s}\leq\te^{-1} x\,\mbox{ for some }\,x\leq s''\}=\{R_{\nu_\eps s}\leq\te^{-1}s''\}&
\end{eqnarray}
are contained in event~(\ref{eq:event}), which, as already pointed out, has vanishing probability as $\eps\to0$. 

Since outside the event 
\begin{equation}
\label{eq:union}
\{R_{\nu_\eps s}\geq\te^{-1} x\,\mbox{ for some }\,x>s'\}\cup\{R_{\nu_\eps s}\leq\te^{-1} x\,\mbox{ for some }\,x\leq s''\}
\end{equation}
we have that
(\ref{eq:condl}) yields~(\ref{eq:sc1}), the result follows. 
%
$\square$

