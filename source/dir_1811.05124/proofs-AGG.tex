We collect some facts about AGG models in this section.
In particular, we give approximate upper quantiles of the AGG tails (which extends naturally to lower quantiles), and show that iid AGG sequences have uniform relatively stable (URS) maxima (which extends naturally to URS minima).

% \begin{proposition}[$(1 - 1/p)$-th quantile] \label{prop:quantile}
% Let 
% \begin{equation} \label{eq:quantiles-Appendix}
% u_p = F^{\leftarrow}(1-1/p),    
% \end{equation}
% then,
% \begin{equation}
% \lim_{p\to\infty} \frac{u_p}{\left(\nu\log{p}\right)^{1/\nu}} = 1.
% \end{equation}
% \end{proposition}
% 
\begin{proof}[Proof of Proposition \ref{prop:quantile}]
By definition of AGG, for any $\epsilon>0$, there is a constant $C(\epsilon)$ such that for all $x\ge C$, we have
$$
-\frac{1}{\nu}x^\nu(1+\epsilon) \le \log{\overline{F}(x)} \le -\frac{1}{\nu}x^\nu(1-\epsilon).
$$
Therefore, for all $x < x_l := \left((1+\epsilon)^{-1}\nu\log{p}\right)^{1/\nu}$, we have
\begin{equation} \label{eq:AGG-quantiles-proof-1}
    -\log{p} = -\frac{1}{\nu}x_l^\nu(1+\epsilon) \le \log{\overline{F}(x_l)} \le \log{\overline{F}(x)},
\end{equation}
and for all $x > x_u := \left((1-\epsilon)^{-1}\nu\log{p}\right)^{1/\nu}$, we have
\begin{equation} \label{eq:AGG-quantiles-proof-2}
    \log{\overline{F}(x)} \le \log{\overline{F}(x_u)} \le -\frac{1}{\nu}x_u^\nu(1-\epsilon) = -\log{p}.
\end{equation}
By definition of generalized inverse,
\begin{equation*}
    u_p := F^\leftarrow(1-1/p) = \inf\{x:\overline{F}(x)\le 1/p\} = \inf\{x:\log{\overline{F}(x)} \le -\log{p}\}.
\end{equation*}
We know from relations \eqref{eq:AGG-quantiles-proof-1} and \eqref{eq:AGG-quantiles-proof-2} that 
$$
[x_u, +\infty) \subseteq \{x:\log{\overline{F}(x)} \le -\log{p}\} \subseteq [x_l, +\infty),
$$
and so $x_l\le u_p \le x_u$, and the expression for the quantiles follow.
\end{proof}

A final auxiliary result is used in Relation \eqref{eq:Sudakov-2} in the proof of Theorem \ref{thm:Gaussian-weak-dependence},
where it is applied to the Normal distributed random variables.
We give here a general statement which may be of independent interest.

\begin{lemma} \label{lemma:expectation-lower}
Let $(X_i)_{i=1}^p$ be $p$ i.i.d. random variables with distribution $F$ such that $\E[(X_i)_-]$ exists, i.e.,
$$
\E[\max\{-X_i, 0\}] < \infty.
$$
Let $M_p = \max_{i=1,\ldots,p}X_i$. Assume that $F$ has a density $f$, which is eventually decreasing. 
More precisely, we suppose there exists a 
$C_0$ such that $0<F(C_0)<1$, and $f(x_1) \ge f(x_2)$ whenever $C_0 > x_1 \ge x_2$. 
Under these assumptions, we have,
$$
\liminf_{p\to\infty}\frac{\E M_p}{u_{p+1}} \ge 1,
$$
where $u_{p+1} = F^{\leftarrow}(1 - 1/(p+1))$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:expectation-lower}]
% For ease of exposition we let $C_0 = 0$; the case where $C_0 > 0$ can be handled using exactly the same arguments.
Write 
$$
X_i = F^{\leftarrow}(U_i)
$$
where $U_i$ are i.i.d. uniform random variables on $(0,1)$.
Denote $M^{U}_p$ as the maximum of the $U_i$'s, we have $\E M_p = \E\left[F^{\leftarrow}(M^{U}_p)\right]$, and by conditioning, we obtain
\begin{align} \label{eq:lemma:expectation-lower-proof-1}
    \E M_p &= \E\left[F^{\leftarrow}(M^{U}_p)\;\big|\;M^{U}_p \ge F(C_0)\right] \P\left[M^{U}_p \ge F(C_0)\right] + \nonumber \\ 
           &\quad\quad +\E\left[F^{\leftarrow}(M^{U}_p)\;\big|\;M^{U}_p < F(C_0)\right] \P\left[M^{U}_p < F(C_0)\right]. 
\end{align} 
We first handle the first term in the summation. Since $f$ is decreasing beyond $C_0$, $F$ is concave on $(C_0, \infty)$, and $F^{\leftarrow}$ is convex on $(F(C_0), 1)$. By Jensen's inequality, we have
\begin{equation*}
    \E\left[F^{\leftarrow}(M^{U}_p)\;\big|\;M^{U}_p \ge F(C_0)\right] 
        \ge F^{\leftarrow}\left(\E[M^{U}_p\;|\;M^{U}_p\ge F(C_0)]\right).
\end{equation*}
With a direct calculation, one can show that 
\begin{equation*}
    F^{\leftarrow}\left(\E[M^{U}_p\;|\;M^{U}_p\ge F(C_0)]\right)
    = F^{\leftarrow}\left(\left(1-\frac{1}{p+1}\right)\left(\frac{1-F(C_0)^{p+1}}{1-F(C_0)^{p}}\right)\right),
\end{equation*}
and hence
\begin{align*}
    \E\left[F^{\leftarrow}(M^{U}_p)\;\big|\;M^{U}_p \ge F(C_0)\right] 
        &\ge F^{\leftarrow}\left(\left(1-\frac{1}{p+1}\right)\left(\frac{1-F(C_0)^{p+1}}{1-F(C_0)^{p}}\right)\right) \\
        &\ge F^{\leftarrow}\left(1-\frac{1}{p+1}\right) = u_{p+1}.
\end{align*}
Since $\P[M^{U}_p\le m\big|\;M^{U}_p < F(C_0)] = \left(m/F(C_0)\right)^p \le m/F(C_0)$ for $m\le F(C_0))$, we have
$$\left(M^{U}_p\;\big|\;M^{U}_p < F(C_0)\right)\stackrel{\text{d}}{\ge} \left(U_1\;\big|\;U_1 < F(C_0)\right),$$
where and the latter is the uniform distribution on $(0,F(C_0))$.
Therefore, for the second term of the sum in \eqref{eq:lemma:expectation-lower-proof-1}, by monotonicity of $F^{\leftarrow}$, we obtain
\begin{align*}
    \E\left[F^{\leftarrow}(M^{U}_p)\;\big|\;M^{U}_p < F(C_0)\right] 
        &\ge \E\left[F^{\leftarrow}(U_1)\;\big|\;U_1 < F(C_0)\right] \\
        &= \E\left[X_1\;\big|\;X_1 < C_0\right].
\end{align*}
Finally, since $\P\left[M^{U}_p < F(C_0)\right] = F(C_0)^p = 1 - \P\left[M^{U}_p \ge F(C_0)\right]$, by \eqref{eq:AGG-quantiles}, we have
\begin{align*}
    \frac{\E M_p}{u_{p+1}} 
        &\ge \left(1 - F(C_0)^p\right) + \frac{\E\left[X_1\;\big|\;X_1 < C_0\right]}{u_{p+1}} F(C_0)^p.
\end{align*}
The conclusion follows since the right-hand-side of the last inequality converges to 1.
\end{proof}
