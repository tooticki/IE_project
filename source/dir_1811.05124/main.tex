\documentclass[aos,preprint]{imsart}

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

% add packages
\usepackage{enumitem}
\usepackage{amsfonts,graphicx,bbm}
%\usepackage[square,numbers]{natbib}
\bibliographystyle{imsart-nameyear}
\usepackage{array,booktabs}
\usepackage[margin=1.25in]{geometry}

%
% My definitions:
%
\def\bbN{{\mathbb N}}
\def\bbZ{{\mathbb Z}}
\def\bbR{{\mathbb R}}
\def\bbC{{\mathbb C}}

\def\P{{\mathbb P}}
\def\E{{\mathbb E}}

\newcommand{\var}{{\rm {Var}}}
\newcommand{\cov}{{\rm {Cov}}}

\def\N{\mathbb N}
\def\R{\mathbb R}
\def\Z{\mathbb Z}
\usepackage{bm}
\renewcommand\vec[1]{{\bm #1}}

\def\alert#1{\fbox{#1}}
\def\what{\widehat}
\def\noi{\noindent}
\def\MSE{{\rm m.s.e.}}
\newcommand\comment[1]{{}}
\def\refeq#1{(\ref{e:#1})}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand\version[2]{{#1}} % The short version of the paper
%\newcommand\version[2]{{#2}} % The long version of the paper

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
% my defs
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}[section]
\endlocaldefs


% settings
%\pubyear{2005}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}
%\arxiv{arXiv:0000.0000}


\begin{document}

\begin{frontmatter}
\title{FUNDAMENTAL LIMITS OF EXACT SUPPORT RECOVERY IN HIGH DIMENSIONS\thanksref{T1}}
\runtitle{EXACT SUPPORT RECOVERY IN HIGH DIMENSIONS}
\thankstext{T1}{This work was partially supported by the NSF ATD grant DMS--1830293.}

\begin{aug}
\author{\fnms{Zheng} \snm{Gao}\ead[label=e1]{gaozheng@umich.edu}}
\and
\author{\fnms{Stilian} \snm{Stoev}\ead[label=e2]{sstoev@umich.edu}}

%\thankstext{t1}{ZG and SS were partially supported by the NSF ATD grant DMS--1830293.}
\runauthor{Z. Gao and S. Stoev}

\affiliation{University of Michigan, Ann Arbor}

\address{Department of Statistics\\
University of Michigan\\
1085 S. University Ave.\\
Ann Arbor, MI, 48105\\
\printead{e1}\\
\phantom{E-mail:\ }\printead*{e2}}

\end{aug}

\begin{abstract}
%
% Abstract #1
%
\comment{ We study the problem of exact support recovery of high dimensional sparse vectors under dependence. We characterize a {\em strong classification} boundary which describes the required signal sizes, as a function of the sparsity level, in order for the support recovery to be asymptotically exact.  Specifically, we show that when the signal is above a so-called strong classification boundary, several classes of well-known procedures can achieve asymptotically perfect support recovery.  This is so under arbitrary error dependence assumptions, provided that the marginal error distribution has rapidly varying tails.  

 We then show that under a very general, but not arbitrary class of dependence structures, the said boundary is tight. In particular, under this general class of error dependence, if the signal sizes are below the strong classification boundary, no thresholding procedure can achieve asymptotically exact support recovery. The concept of uniform relative stability (URS) -- a type of concentration of maxima phenomenon --  plays a key role. A complete characterization of the URS property is given for Gaussian triangular arrays in terms of their covariance structure.  Examples of non-URS errors are also given, where exact support recovery is possible for strictly smaller signal sizes.

 Finally we demonstrate, perhaps surprisingly, that thresholding procedures may not be optimal in support recovery problems especially in the regime of heavy-tailed super-exponential error distributions.}
 
 %
 % Abstract#2
 %
\comment{
We study the problem of the estimation of the support (set of non-zero components) of a sparse high-dimensional signal when observed with dependent noise. 
We characterize a phase-transition phenomenon for the asymptotically exact estimation of the signal support. 
% With the usual parameterization of the size of the support set and the signal magnitude, we characterize a phase-transition phenomenon. 
Specifically, we show that when the signal size is above a so-called strong classification boundary, several classes of well-known procedures can achieve asymptotically perfect support recovery. 
This is so under arbitrary error dependence assumptions, provided that the marginal error distribution has rapidly varying tails.  
Conversely, we show that no thresholding estimators can achieve perfect support recovery if the signal is below the boundary, under very mild, but not arbitrary dependence structures on the noise. 
Examples of errors are given, where the dependence conditions are violated and exact support recovery is possible for strictly smaller signal sizes.

The results are based on an important and little-understood concentration of maxima phenomenon, known as relative stability in the case of independent errors. 
As an important probabilistic result, we establish the complete characterization of the relative stability phenomenon for dependent Gaussian triangular arrays. 
Consequently, we obtain a rather complete understanding of support recovery problem for sparse signals observed with dependent Gaussian noise. The methods of proof involve Sudakov-Fernique and Slepian lemma techniques alongside a curious application of Ramsey theory.

Finally we demonstrate, perhaps surprisingly, that thresholding procedures may not be optimal in support recovery problems especially in the regime of heavy-tailed super-exponential error distributions.
In the case of log-concave error densities, the thresholding estimators are shown to be minimax optimal and hence in this setting, the strong classification boundary is universal.}

%
% Abstract#3
%
We study the support recovery problem for a high-dimensional signal observed with additive noise. 
With suitable parametrization of the signal sparsity and magnitude of its non-zero components, we characterize a phase-transition phenomenon akin to the signal detection problem studied by Ingster in 1998. 
Specifically, if the signal magnitude is above the so-called {\em strong classification 
boundary}, we show that several classes of well-known procedures achieve asymptotically perfect support recovery as the dimension goes to infinity. 
This is so, for a very broad class of error distributions with light, rapidly varying tails which may have arbitrary dependence.  
Conversely, if the signal is below the boundary, then for a very broad class of error dependence structures, no thresholding estimators (including ones with data-dependent thresholds) can achieve perfect support recovery.
The proofs of these results exploit a certain \emph{concentration of maxima} phenomenon known as relative stability. 
We provide a complete characterization of the relative stability phenomenon for Gaussian triangular arrays in terms their correlation structure. 
The proof uses classic Sudakov-Fernique and Slepian lemma arguments along with a curious application of Ramsey's coloring theorem. 

We note that our study of the strong classification boundary is in a finer, point-wise, rather than minimax, sense. 
We also establish the Bayes optimality and sub-optimality of thresholding procedures. 
Consequently, we obtain a minimax-type characterization of the strong classification boundary for errors with log-concave densities. 
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62G10}
\kwd{62G32}
\kwd[; secondary ]{62G20}
\kwd{62C20}
\end{keyword}

\begin{keyword}
\kwd{support recovery}
\kwd{high--dimensional inference}
\kwd{relative stability}
\kwd{rapid variation}
\kwd{concentration of maxima}
\kwd{Sudakov-Fernique inequality}
\kwd{Ramsey theory}
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}
\input{1.0}


\section{Sufficient conditions for exact support recovery}
\label{sec:sufficient}
\input{2.0}


\section{Uniform relative stability and dependent errors}
\label{sec:URS}
\input{3.0}

\section{Necessary conditions for exact support recovery}
\label{sec:necessary}
\input{4.0}


\section{Numerical illustrations}
\label{sec:numerical}
\input{5.0}

%\section{Proof of Theorem \ref{thm:necessary}}
%\label{sec:proof-necessary}
%\input{proofs-necessary_conditions}

\appendix

\section{Proofs}
\label{sec:proofs}
\input{proofs-URS}
\input{proofs-examples.tex}
\input{proofs-convexity.tex}


\section{Properties of asymptotic generalized Gaussian models and auxiliary facts}
\label{sec:AGG}
\input{proofs-AGG}

% \section{An erratum for Ji and Jin (2012)}
% \label{sec:errata}
% \input{errata-UPS}

\section{Strong classification boundaries in other light-tailed models}
\label{sec:other-boundaries}
\input{appendix-other-boundaries}

\section{Thresholding procedures under heavy-tailed errors}
\label{sec:heavy-tailed}
\input{appendix-heavy-tailed}


\section*{Acknowledgements}
The authors thank Jinqi Shen and Yuanzhi Li for inspiring discussions.
%See \ref{suppA} for supplementary material.

% \begin{supplement}
% \sname{Supplement A}\label{suppA}
% \stitle{Title of the Supplement A}
% \slink[url]{http://www.e-publications.org/ims/support/dowload/imsart-ims.zip}
% \sdescription{Dum esset rex in accubitu suo, nardus mea dedit odorem suavitatis. Quoniam confortavit seras portarum tuarum, benedixit filiis tuis in te. Qui posuit fines tuos}
% \end{supplement}


\bibliography{references}


\end{document}
