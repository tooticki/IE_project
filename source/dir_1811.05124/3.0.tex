
We study the asymptotic behavior of error maxima under dependence in this section.
We show in Proposition \ref{prop:rapid-varying-tails} below that the maxima of errors with rapidly varying tails can be bounded above using quantiles of their marginal distribution, regardless of their dependence structure. 
This result is a key step in the proof of Theorem \ref{thm:sufficient}.

On the other hand, a lower bound for the error maxima can be provided for a very general class of dependence structures, as we will see in Section \ref{subsec:URS}. 
Specifically, we characterized this class of dependence structure in the case of Gaussian errors with a transparent necessary and sufficient condition.
This will prepare us to state the converse of Theorem \ref{thm:sufficient}.

Some new results regarding the structure of correlation matrix of high-dimensional random variables may be of independent, and are collected in Section \ref{subsec:Ramsey}.

\subsection{Rapid variation and relative stability}
\label{subsec:RS}

The behavior of the maxima has been well-studied in the literature (see, e.g., \cite{leadbetter2012extremes,resnick2013extreme,embrechts2013modelling,de2007extreme} 
and the references therein). The concept of rapid variation plays an important role in the light-tailed case.

\begin{definition}[Rapid variation] \label{def:rapid-variation}
The right tail of a distribution, $\overline{F}(x) = 1 - F(x)$, is said to be rapidly varying if
\begin{equation}\label{e:def:rapid-variation}
\lim_{x\to\infty} \frac{\overline{F}(tx)}{\overline{F}(x)} 
    = \begin{cases}
    0, & t > 1\\
    1, & t = 1\\
    \infty, & 0 < t < 1
\end{cases}.
\end{equation}
\end{definition}

When $F(x)<1$ for all finite $x$, \citet{gnedenko1943distribution} showed that the distribution $F$ has rapidly varying tails if and only if the maxima of independent observations from $F$ are \emph{relatively stable} in the following sense.
\begin{definition}[Relative stability] \label{def:RS}
Let $\epsilon_p = \left(\epsilon_p(j)\right)_{j=1}^p$ be a sequence of random variables with identical marginal distributions $F$. Define the sequence $(u_p)_{p=1}^\infty$ to be the $(1-1/p)$-th generalized quantile of $F$, i.e., 
\begin{equation} \label{eq:quantiles}
    u_p = F^\leftarrow(1 - 1/p).
\end{equation}
The triangular array ${\cal E} = \{\epsilon_p, p\in\N\}$ is said to have relatively stable (RS) maxima if we have
\begin{equation} \label{eq:RS-condition}
    \frac{1}{u_{p}} M_p := \frac{1}{u_{p}} \max_{j=1,\ldots,p} \epsilon_p(j) \xrightarrow{\P} 1,
\end{equation}
as $p\to\infty$.
\end{definition}

In the case of independent and identically distributed $\epsilon_p(j)$'s, \citet{barndorff1963limit} and \citet{resnick1973almost} obtained necessary and sufficient conditions for the \emph{almost sure stability} of maxima, where the convergence in \eqref{eq:RS-condition} holds almost surely.

While relative stability (and almost sure stability) is well-understood in the independent case, the role of dependence has not been fully explored.
We start this exploration with a small refinement of Theorem 2 in \citet{gnedenko1943distribution} valid under arbitrary dependence.

\begin{proposition}[Rapidly variation and relative stability] \label{prop:rapid-varying-tails}
Assume that the array ${\cal E}$ consists of identically distributed random 
variables with distribution $F$, where $F(x)<1$ for all finite $x>0$. 
\begin{enumerate}
    \item If $F$ has rapidly varying right tail, then for all $\delta>0$,
        \begin{equation} \label{eq:rapid-varying-tails}
            \P\left[\frac{1}{u_p} M_p\le1+\delta\right] \to 1.
        \end{equation}
    \item If in addition, the array ${\cal E}$ has independent entries, then it is relatively stable if and only if $F$ has rapidly varying tail.
    \label{prop:rapid-varying-tails_part-ii}
\end{enumerate}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:rapid-varying-tails}] 
By the union bound and the fact that 
$p\overline F(u_p) \le 1$, we have
\begin{align}\label{e:prop:rapid-varying-tails_part-i-1}
\P [ M_p > (1+\delta)u_p] \le p \overline F((1+\delta)u_p)
 \le \frac{\overline F((1+\delta)u_p)}{\overline F(u_p)}.
\end{align}
In view of \eqref{e:def:rapid-variation} (rapid variation) and
since $u_p\to\infty$, as $p\to\infty$, the right-hand side of \eqref{e:prop:rapid-varying-tails_part-i-1} vanishes 
as $p\to\infty$, for all $\delta>0$.  This completes the proof of \eqref{eq:rapid-varying-tails}. Part 2 is a re-statement of the classic result due to Gnedenko in \cite{gnedenko1943distribution}.
\end{proof}
%The proof can be found in Appendix \ref{sec:proofs}.

We will see that Gaussian, Exponential, Laplace, Gamma (Example \ref{exmp:AGG}), log-normal (Example \ref{exmp:heavier-than-AGG}), and Gompertz (Example \ref{exmp:lighter-than-AGG}) distributions all have rapidly varying tails. On the other hand, heavy-tailed distributions like the Pareto and t-distribution do not.

% \begin{corollary} \label{cor:AGG-is-RS}
% If $F\in\text{AGG}(\nu)$, $\nu>0$, an independent array ${\cal E}$ is relatively stable. Further, $\text{AGG}(\nu)$ is the only class of model with $u_{p} \sim \left(\nu\log{p}\right)^{1/\nu}$.
% \end{corollary}
% 
% \begin{proof}[Proof of Corollary \ref{cor:AGG-is-URS}]
% By Proposition \ref{prop:rapid-varying-tails}, it is enough to show that in the AGG model,  $\overline{F}$ has rapidly varying tail. 
% By definition of the AGG tails \eqref{def:AGG}, we have
% $$
% \lim_{t\to\infty} \frac{\log{\left(\overline{F}(tx)\Big/\overline{F}(t)\right)}}{-\frac{1}{\nu}t^\nu(x^\n% u-1)} = 1,
% $$
% where the denominator tends to $+\infty$ or $-\infty$ depending on whether $0<x<1$ or $x>1$.
% Therefore, we must have $\overline{F}(tx) / \overline{F}(t)$ converging to $\infty$ or 0 in the correct range of $x$'s; the case where $x=1$ is trivial.
% The last claim follows from the expression for AGG quantiles; see Proposition \ref{prop:quantile}.
% \end{proof}

\begin{example}[Generalized AGG] \label{exmp:AGG}
A distribution is said to have \emph{Generalized AGG} right tail, if $\log{\overline{F}}$ is regularly varying,
\begin{equation} \label{eq:GAGG}
    \log{\overline{F}(x)} = - x^\nu L(x),
\end{equation}
where $L: (0,+\infty)\to(0,+\infty)$ is a slowly varying function. (A function is said to be slowly varying if $\lim_{x\to\infty}L(tx)/L(x) = 1$ for all $t>0$.) Note that the AGG model corresponds to the special case where $L(x)\to 1$, as $x\to\infty$.

Relation \eqref{eq:rapid-varying-tails} holds for all arrays $\cal E$ with \emph{generalized} AGG marginals; if the entries are independent, the maxima are relatively stable. 
This follows directly from Proposition \ref{prop:rapid-varying-tails}, once we show that $F$ has rapidly varying tail. 
Indeed, by \eqref{eq:GAGG}, we have
$$
\log{\left(\overline{F}(tx)\Big/ \overline{F}(x)\right)} = - L(x)x^\nu\left(t^\nu\frac{L(tx)}{L(x)} - 1\right),
$$
which converges to $-\infty$, 0, and $+\infty$, as $\to\infty$, when $t>1$, $t=1$, and $t<1$, respectively.
\end{example}


The AGG class encompasses a wide variety of rapidly varying tail models such as Gaussian and double exponential distributions. The larger class \eqref{eq:GAGG} is needed, however, for the Gamma distribution.
Models such as Log-normal have heavier tails than the AGG model, yet still have rapidly varying tails. therefore Proposition \ref{prop:rapid-varying-tails} is also applicable.

\begin{example}[Heavier than AGG] \label{exmp:heavier-than-AGG}
Let $\gamma>1$, $c>0$, and suppose that
\begin{equation} \label{eq:heavier-than-AGG}
    \log{\overline{F}(x)} = - \left(\log x\right)^\gamma \left(c+M(x)\right),
\end{equation}
where $\lim_{x\to\infty} M(x)\log{x}= 0$. Then, Relation \eqref{eq:rapid-varying-tails} holds under 
model \eqref{eq:heavier-than-AGG}. Further, if the entries in the array independent, the 
maxima are relatively stable.

The behavior of the quantiles $u_p$ in this model is as follows. As $p\to\infty,$
\begin{equation*}
    u_p \sim \exp{\left\{\left(c^{-1}\log{p}\right)^{1/\gamma}\right\}}
    \iff - \left(\log{u_p}\right)^{\gamma} \sim \log(p) = \log \overline{F}(u_p).
\end{equation*}
Since $u_p$ diverges, $M(u_p)$ is $o(\log(u_p)) = o(1)$, and the expression for the quantiles follows.
\end{example}

Lastly, Proposition \ref{prop:rapid-varying-tails} applies to error models with lighter tails than the AGG class.

\begin{example}[Lighter than AGG] \label{exmp:lighter-than-AGG}
With $\nu>0$, and $L(x)$ a slowly varying function, the class of distributions
\begin{equation} \label{eq:lighter-than-AGG}
    \log{\overline{F}(x)} = - \exp{\left\{x^\nu L(x)\right\}},
\end{equation}
is rapidly varying.
The quantiles can be derived explicitly in a subclass of \eqref{eq:lighter-than-AGG} where $L(x)\to 1$, or equivalently, when $\log{|\log{\overline{F}(x)}|}\sim x^\nu$,
\begin{equation*}
    u_p \sim \left(\log \log{p}\right)^{1/\nu}
    \iff - \exp{\left\{u_p^\nu\left(1+o(1)\right)\right\}} = \log(p) = \log \overline{F}(u_p).
\end{equation*}
\end{example}
%The proofs of the rapid variation of the distributions in Examples \ref{exmp:heavier-than-AGG} and \ref{exmp:lighter-than-AGG} are entirely analogous to that of Example \ref{exmp:AGG}, and omitted.
Strong classification boundaries for the classes of models in Examples \ref{exmp:heavier-than-AGG} and \ref{exmp:lighter-than-AGG} will be derived in Appendix \ref{sec:other-boundaries}.

\subsection{Dependence and uniform relative stability}
\label{subsec:URS}

An important ingredient needed for a converse of Theorem \ref{thm:sufficient} is an appropriate characterization of the error dependence structure under which the said boundary is tight.
%We see in Section \ref{subsec:RS} that relative stability holds when the error are light-tailed and independent.
The notion of \emph{uniform relative stability} turns out to be the key in characterizing such dependence structures when studying the behavior of maxima of dependent light-tailed sequences.

\begin{definition}[Uniform Relative Stability] \label{def:URS}
Under the notations established in Definition \ref{def:RS}, the triangular array ${\cal E}$ is said to have uniform relatively stable (URS) maxima if for \emph{every} sequence of subsets $S_p\subseteq\{1,\ldots,p\}$ such that $|S_p| \to \infty$, we have
\begin{equation} \label{eq:URS-condition}
    \frac{1}{u_{|S_p|}} M_{S_p} := \frac{1}{u_{|S_p|}} \max_{j\in S_p} \epsilon_p(j) \xrightarrow{\P} 1,
\end{equation}
as $p\to\infty$, where $u_q,\ q\in (0,1)$ is the generalized quantile in \eqref{eq:quantiles}.
The collection of arrays ${\cal E} = \{ \epsilon_p(j) \}$ with URS maxima is 
denoted $U(F)$.
\end{definition}

Uniform relative stability is, as its names suggests, a stronger requirement on dependence than relative stability. 
From the last section we see that, an array with iid components sharing a marginal distribution $F$ with rapid varying tails has relatively stable maxima; it is easy to see that URS also follows, by independence of the entries.

\begin{corollary} \label{cor:AGG-is-URS}
If $F\in\text{AGG}(\nu)$, $\nu>0$, an independent array ${\cal E}$ is URS; in this case, URS holds with $u_{|S_p|} \sim \left(\nu\log{|S_p|}\right)^{1/\nu}$.
\end{corollary}

On the other hand, RS and URS hold under much broader dependence structures than just 
independent errors. In turn, the stability concepts can be used to characterize dependence structures under which the maxima of error sequences {\em concentrate} around the quantiles \eqref{eq:quantiles} in the sense of \eqref{eq:RS-condition}.

The condition on the dependence structure of the array ${\cal E}$ imposed through the language of 
uniform relative stability seems, however, somewhat mysterious and implicit.  Fortunately, this is an 
extremely weak requirement.  In the rest of this section, we illuminate this condition in the case of 
Gaussian errors.  Specifically, we establish a simple necessary and sufficient condition for the 
uniform relative stability in terms of their covariance structure. 


\begin{definition}[Uniformly decreasing dependence (UDD)] \label{def:UDD}
Consider a triangular array of jointly Gaussian distributed errors 
${\cal E} = \left(\epsilon_p(j)\right)_{j=1}^p$ 
with unit variances,
$$
\epsilon_p \sim \text{N}(0, \Sigma_p), \quad p=1,2,\ldots.
$$
The array ${\cal E}$ is said to be uniform decreasingly dependent (UDD) if 
for every $\delta>0$ there exists a finite $N(\delta)<\infty$, such that for every $j\in\{1,\ldots,p\}$, and $p\in\N$, we have
\begin{equation} \label{eq:UDD-definition}
    \Big|\left\{k\in\{1,\ldots,p\}:\Sigma_p(j,k)>\delta\right\}\Big| \le N(\delta)\quad \text{for all  } \delta>0.
\end{equation}
\end{definition}
That is, for any coordinate $j$, the number of coordinates which are more than $\delta$-correlated with $\epsilon_p(j)$ does not exceed $N(\delta)$. 

Note that the bound in \eqref{eq:UDD-definition} holds uniformly in $j$ and $p$, and only depends on $\delta$.
Also observe that in on the left-hand side of \eqref{eq:UDD-definition}, we merely count in each row of $\Sigma_p$ the number of exceedances of covariances (not their absolute values!) over level $\delta$.

\begin{remark} \label{rmk:choice-of-N(delta)}
Without loss of generality, we may require that $N(\delta)$ be a non-increasing function of $\delta$, for we can take
$$
N(\delta) = \sup_{p,j} \Big|\{k:\Sigma_p(j,k)>\delta\}\Big|,
$$
which is non-increasing in $\delta$.
Definition \ref{def:UDD} therefore states that the array is UDD when $N(\delta)<\infty$ for all $\delta>0$.
\end{remark}


Observe that the UDD condition does not depend on the order of the coordinates in the error 
vector $\epsilon_p = (\epsilon_p(j))_{j=1}^p$.  Often times, however, the errors are thought of 
coming from a stochastic process indexed by time or space.  To illustrate the generality of the 
UDD condition, we formulate next a simple sufficient condition (UDD$^\prime$) that is easier to 
check in a time-series context.

\begin{definition}[UDD\,$^\prime$]\label{d:UDD-prime}
For $\epsilon_p \sim \mathcal N(0,\Sigma_p)$ with unit variances, an array ${\cal E} = \left(\epsilon_p(j)\right)_{j=1}^p$ is said to satisfy the UDD\,$^\prime$ condition if there 
exist:
\begin{enumerate}
    \item[(i)] permutations $l = l_p$ of $\{1,\ldots,p\}$, for all $p\in\N$, and
    \item[(ii)] a non-negative sequence $(r_n)_{n=1}^\infty$ converging to zero $r_n\to 0$, as $n\to\infty$,
\end{enumerate}
such that 
\begin{equation} \label{eq:weak-correlation}
    \sup_{p\in\N} |\Sigma_p\left(i',j'\right)| \le r_{|i-j|}.
\end{equation}
where $i' = l(i)$, $j' = l(j)$, for all $i,j\in\{1,\ldots,p\}$.
\end{definition}

\begin{remark}
Without loss of generality, we may also require that $r_n$ be non-increasing in $n$, for we can replace $r_n$ with $r'_n = \sup_{m\ge n} r_m$, which is non-increasing in $n$.
\end{remark}

\begin{proposition} \label{prop:UDD-equivalent}
UDD\,$^\prime$ implies UDD.
\end{proposition} 

\begin{proof}%[Proof of Proposition \ref{prop:UDD-equivalent}]
%The functions $N(\delta)$ and $r(n)=r_n$ are inverses of each other.
% {\bf $\text{UDD} \implies \text{UDD'}$:}
% If $N(0)<\infty$, then we can take 
% $$
% r_n = \underbrace{1, \ldots, 1}_{\lfloor N(0)/2\rfloor + 1}, 0, 0, \ldots.
% $$
% and recursively construct the permutations as follows.
% Start with any element

% We proof the contrapositive. 
% Suppose for all permutations and any sequence $r_n\to0$, there exists $i,j$ such that $\Sigma(i',j')>r_{|i'-j'|}$,
% then for any $\delta>0$ and any finite $M<\infty$, we can take $r_n$ to be a sequence of $M$ 1's followed by a $\delta$, i.e., 
% $$
% r_n = \underbrace{1, \ldots, 1}_{M+1}, \delta, \ldots.
% $$
% \fbox{Not true:}
% However, since there exists $i',j'$ such that $\Sigma(i',j')>r_{|i'-j'|}$, the set 
% $$
% S = l^{-1}\left(\left\{j',i'-N,\ldots,i'-1,i',i'+1,\ldots,i'+N\right\}\right)
% $$ 

% {\bf $\text{UDD'} \implies \text{UDD}$:} 
Since $r_n\to 0$, for any $\delta > 0$, there exists an integer 
$M = M(\delta)<\infty$ such that $r_n\le\delta$, for all $n\ge M$. 
Thus, by \eqref{eq:weak-correlation}, for every fixed 
$j' \in\{1,\ldots,p\}$, we can have $|\cov(\epsilon_p(k'),\epsilon_p(j'))| > \delta$,
only if $k'$ belongs to the set:
$$ 
 \left\{ k' \in \{1,\dots,p\} \, :\, j-M \le  k := l_p^{-1}(k') \le j+M \right\},
$$
where $j:= l_p^{-1}(j')$. That is, there are at most $2M+1<\infty$ indices  $k'\in\{1,\dots,p\}$, whose covariances with $\epsilon(j')$ may exceed $\delta$. 
Since this holds uniformly in $j'\in\{1,\ldots,p\}$, Condition UDD follows with 
$N(\delta) = 2M+1$.
\end{proof}



% \begin{definition}[Uniformly decreasing dependence (UDD)] \label{def:weak-dependence}
% Consider a triangular array of jointly Gaussian distributed errors $\left(\epsilon_p(j)\right)_{j=1}^p$ with unit variances, $\epsilon_p \sim \mathcal N(0,\Sigma_p)$. The array ${\cal E}$ is said to be uniform decreasingly dependent (UDD) with rate $(r_n)_{n=1}^\infty$ if 
% \begin{equation} \label{eq:weak-correlation}
%     \sup_p |\Sigma_p(i,j)| \le r_{|i-j|}
% \end{equation}
% such that $r_n\to 0$, as $n\to\infty$.
% \end{definition}
% 
% \begin{remark} \label{rmk:UDD-equivalent}
% In situations where there is no natural ordering of the components, it is sufficient that a permutation of the vector $\epsilon$ in its coordinates satisfy the requirements above. 
% In fact, the UDD condition can be equivalently stated as follows: for any $\delta>0$, and any coordinate $j\in\{1,\ldots,p\}$, there are at most $N(\delta)<\infty$ coordinates whose covariances with $\epsilon(j)$ exceed $\delta$; here $N(\delta)$ is a deterministic function independent of $p$.
% $N(\delta) \to 1$ as $\delta \to 0$.
% \end{remark}

We now state the last (and main) result of this section: a Gaussian sequence is URS if and only if it is UDD.
The URS condition essentially requires that the dependencies decay in a uniform fashion, the rate at which dependence decay does \emph{not} matter.

\begin{theorem} \label{thm:Gaussian-weak-dependence}
Let ${\cal E}$ be a Gaussian triangular array of with standard normal marginals.  
The array ${\cal E}$ has uniformly relatively stable (URS) maxima if and only if it is uniformly decreasing dependent (UDD).
\end{theorem}

The proof is given in Section \ref{sec:proof-UDD-URS-outline}. We finish this section with a brief discussion on the relationships between UDD and other dependence conditions in the context of extreme value theory.

Suppose that the array of errors  ${\cal E}$ comes from a stationary Gaussian time series $\epsilon(j),\ j\in \mathbb{N}$, with auto-covariance $r_p=\cov(\epsilon(j+p),\epsilon(j))$. 
One is interested in the asymptotic behavior of the maxima $M_p:=\max_{j=1,\dots,p} \epsilon(j)$.

In this setting, the Berman's condition, introduced in \cite{berman1964limit}, requires that
\begin{equation} \label{eq:Berman}
    r_p \log p \to 0,\ \ \mbox{ as }p\to\infty.
\end{equation}
This condition entails that 
\begin{equation}
    \label{eq:Gauss-max-in-distribution}
  a_p (M_p - b_p) \stackrel{d}{\longrightarrow } Z,\  \ \mbox{ as }p\to\infty,
\end{equation}
with the Gumbel limit distribution $\mathbb P [Z\le x] = \exp\{-e^{-x}\},\ x\in \mathbb R$, 
where 
$$
a_p = \sqrt{2\log p},\quad b_p  = \sqrt{2\log p} - \frac{1}{2}\left(\sqrt{2\log p}\right)^{-1}\left(\log \log (p) + \log(4\pi)\right),
$$ 
are {\em the same} centering and normalization sequences
as in the case of iid $\epsilon(j)$'s.  
The Berman's condition is one of the weakest dependence conditions  in the literature for which this result holds. See, for example, Theorem 4.4.8 in \citet{embrechts2013modelling}, where the Berman's condition is described as ``very weak''.

For dependence conditions weaker than \eqref{eq:Berman}, it is known that the sequences of normalizing and centering constants in \eqref{eq:Gauss-max-in-distribution} are {\em different} from the iid case, and the corresponding limit is no longer Gumbel; see, for example, Theorems 6.5.1 and  6.6.4 in \citet{leadbetter2012extremes}. 
In particular, \citet{mccormick1976weak} derived the normalizing constants when both $r_p\to 0$ monotonically and  $r_p \log p \to \infty$ monotonically, as $p\to\infty$. 
In this case, convergence in distribution still takes place, with the maxima concentrating along a sequence asymptotic to \eqref{eq:quantiles}.

On the other hand, if one is merely interested in the asymptotic relative stability of the Gaussian maxima rather than in their distributional limit, then the Berman's condition
can be relaxed significantly. 
Observe that by Proposition \ref{prop:UDD-equivalent},  the Berman condition \eqref{eq:Berman} implies UDD and hence the relative stability (Theorem \ref{thm:Gaussian-weak-dependence}), i.e., 
\begin{equation} \label{eq:Gaussian-URS}
  \frac{1}{b_p} M_p \stackrel{\mathbb P}{\to} 1,\quad\mbox{as}\quad p\to\infty.
\end{equation}
This {\em concentration of maxima} property can of course be readily deduced from \eqref{eq:Gauss-max-in-distribution}, since $a_p b_p \sim 2\log(p) \to \infty$ as $p\to\infty$.
Our Theorem \ref{thm:Gaussian-weak-dependence} shows that \eqref{eq:Gaussian-URS} holds if and only if the much weaker uniform dependence condition UDD holds. 
Note that this condition is coordinate free, i.e., neither monotonicity of the sequence $r_p$ nor stationarity of the underlying array is required. 
The method of proof is also very different from the results on distributional convergence in the references mentioned above. 
In our high dimensional support estimation context, the notion of relative stability is sufficient and more natural than the finer notions of distributional convergence.


\subsection{Proof of Theorem \ref{thm:Gaussian-weak-dependence}}
\label{sec:proof-UDD-URS-outline}

The proof of the `only if' part is detailed in Section \ref{subsubsec:URS=>UDD}. 
The proof uses a surprising, yet elegant application of Ramsey's Theorem from the study of combinatorics; this application, and its consequences in high-dimensional probability, are presented in Section \ref{subsec:Ramsey}.
The proof of the `if' part is sketched in Section \ref{subsubsec:UDD=>URS},
details are filled in later in Section \ref{subsec:bounding-upper-tails-of-maxima} and \ref{subsec:bounding-lower-tails-of-maxima}.

\subsubsection{{\bf URS implies UDD (`only if' part of Theorem \ref{thm:Gaussian-weak-dependence})}} \label{subsubsec:URS=>UDD}
In view of Remark \ref{rmk:choice-of-N(delta)}, UDD is equivalent to the requirement that
$N(\delta) := 1+\sup_{p} N_p(\delta) < \infty$ for all $\delta\in(0,1)$,
where 
\begin{equation} \label{eq:N_p(c)}
    N_p(\delta) := \max_{j\in\{1,\ldots,p\}} \Big|\{i:i\neq j,\;\Sigma_p(j,i) > \delta\}\Big|.
\end{equation}
Therefore, if ${\cal E}$ is not UDD, then there must exist a constant $c\in (0,1)$ for which $N(c)$ is infinite, i.e., there is a subsequence $\widetilde p\to\infty$ such that $N_{\widetilde p}(c) \to \infty$.
Without loss of generality,  we may assume that $\widetilde{p}=p$.

Let $j_p(c)$ be the maximizers of \eqref{eq:N_p(c)}, and let
\begin{equation} \label{eq:sub-sequence_of_sets}
S_p(c):= \{ i\in\{1,\dots,p\}\, :\, \Sigma_p(j_p(c), i) > c \}.% \quad\quad \text{for all }k\in S_p(c).
\end{equation}
Observe that $|S_p(c)| = N_p(c)+1 \to \infty$, as $p\to\infty$ 
(note $j_p(c) \in S_{p}(c)$).

Applying Lemma \ref{lemma:positive-correlation} (see Section \ref{subsec:Ramsey} below) to the set of random variables indexed by $S_p(c)$, we conclude, for $N_p(c) \ge 2^{2\lceil2/c^2\rceil+4}$, there must be a further subset 
\begin{equation} \label{eq:further_sub-sequence_of_sets}
  K_p(c) \subseteq S_p(c),
\end{equation}
of cardinality 
\begin{equation} \label{eq:further_sub-sequence_of_sets_size}
k_p(c) := \left|K_p(c)\right| \ge \log_2{\sqrt{N_p(c)}},
\end{equation}
such that all pairwise correlations of the random variables indexed by $K_p(c)$ are greater than $c^2/2$.
Since the sequence $N_p(c)\to\infty$, by \eqref{eq:further_sub-sequence_of_sets_size}, we have $k_p(c)\to\infty$ as $p\to\infty$.

Therefore, we have identified a sequence of subsets $K_p(c)\subseteq\{1,\ldots,p\}$ with the following two properties:
\begin{enumerate}
  \item $k_p(c) := \left|K_p(c)\right| \to \infty$, as $p\to\infty$, and
  \item For all $i,j\in K_p(c)$, we have
  \begin{equation} \label{eq:further_sub-sequence_of_sets_cor}
    \Sigma_p(i,j) > c^2/2.
  \end{equation}
\end{enumerate}
Without loss of generality, we may assume $K_p(c) = \{1,\ldots,k_p(c)\} \subseteq \{1,\ldots,p\}$, upon re-labeling of the coordinates. 

Now consider a Gaussian sequence $\epsilon^* = \{\epsilon^*(j),\;j = 1,2,\ldots\}$, independent of ${\cal E}$, defined as follows:
$$
\epsilon^*(j):= Z \left(c/\sqrt{2}\right) + Z(j) \sqrt{1-{c^2}/{2}}, \quad j = 1, 2, \ldots,
$$ 
where $Z$ and $Z(j), j = 1, 2, \ldots$ are independent standard normal random variables. 
Hence,
\begin{equation} \label{eq:Slepian-conclusion-condition-1}
    {\rm Var}(\epsilon^*(j)) = 1 = {\rm Var}(\epsilon_p(j)),
\end{equation}
and
\begin{equation} \label{eq:Slepian-conclusion-condition-2}
    \cov(\epsilon^*(i),\epsilon^*(j)) = \frac{c^2}{2} \le \cov(\epsilon_p(i),\epsilon_p(j)),
\end{equation}
for all $p$, and all $i\neq j$, $i,j\in K_p(c)$.
Thus we have, as $p\to\infty$, 
\begin{equation} \label{eq:!UDD=>subsequence-fail}
    \frac{1}{u_{k_p(c)}} \max_{j\in K_p(c)} \epsilon^*(j) = \frac{c/\sqrt{2}}{u_{k_p(c)}}Z + \frac{\sqrt{1-c^2/2}}{u_{k_p(c)}} \max_{j\in K_p(c)} Z(j) \stackrel{\mathbb P}{\to} \sqrt{1-\frac{c^2}{2}},
\end{equation}
where the convergence in probability follows from Proposition \ref{prop:rapid-varying-tails} part \ref{prop:rapid-varying-tails_part-ii}.
%The fact that the last limit is strictly less than $1$, together with Relation \eqref{eq:Slepian-conclusion}, shows that \eqref{eq:URS-condition} is impossible, for $S_p:=K_p(c)$.

Relations \eqref{eq:Slepian-conclusion-condition-1} and \eqref{eq:Slepian-conclusion-condition-2}, by Slepian's Lemma \cite{slepian1962one}, also imply,
\begin{equation}\label{eq:Slepian-conclusion}
  \frac{1}{u_{k_p(c)}} \max_{j\in K_p(c)} \epsilon^*(j) \stackrel{d}{\ge} \frac{1}{u_{k_p(c)}} \max_{j\in K_p(c)} \epsilon_p(j).
\end{equation}
Therefore, by \eqref{eq:Slepian-conclusion} and \eqref{eq:!UDD=>subsequence-fail}, for all $\sqrt{1-c^2/2} \le \delta < 1$, we have,
$$
\P\left[\frac{1}{u_{k_p(c)}} \max_{j\in K_p(c)} \epsilon_p(j) < \delta \right] \to 1 \quad\mbox{as  }p\to\infty.
$$
This contradicts the definition of URS (with the particular choice of $S_p:=K_p(c)$), and the proof of the `only if' part is complete.

\subsubsection{{\bf UDD implies URS (`if' part of Theorem \ref{thm:Gaussian-weak-dependence})}} \label{subsubsec:UDD=>URS}

Recall that our objective is to show \eqref{eq:URS-condition}. 
We will do so in two stages; namely, we will prove that for all $\delta>0$, we have 
\begin{equation} \label{eq:URS-condition-upper-side}
    \P\left[\frac{M_{S_p}}{u_{|S_p|}} > 1+\delta\right] \to 0,
\end{equation}
and
\begin{equation} \label{eq:URS-condition-lower-side}
    \P\left[\frac{M_{S_p}}{u_{|S_p|}} < 1-\delta\right] \to 0,
\end{equation}
for any sequence of subsets $S_p$ such that $|S_p|\to\infty$.
Although the first step \eqref{eq:URS-condition-upper-side} was already shown in Proposition \ref{prop:rapid-varying-tails}, regardless of the dependence structure, we provide in this section a more refined result. 
Specifically, the following result states that for the AGG model, the constant $\delta$ in Proposition \ref{prop:rapid-varying-tails} can be replaced by a vanishing sequence $c_p\to 0$.

\begin{lemma}[Upper tails of AGG maxima] \label{lemma:AGG-maxima-upper-tails}
Let ${\cal E}$ be an array with marginal distribution $F\in\text{AGG}(\nu)$, $\nu>0$. If we pick
\begin{equation} \label{eq:choice-of-c_p}
    c_p = \frac{u_{p\log{p}}}{u_p} - 1,    
\end{equation} 
where $u_p = F^{\leftarrow}(1-1/p)$, then we have $c_p>0$, $c_p\to 0$, and
\begin{equation} \label{eq:AGG-max-upper-bound}
    \P\left[\frac{M_p}{u_p}-(1+c_p) > 0\right] \to 0.
 \end{equation}
\end{lemma}
The proof can be found in Appendix \ref{subsec:bounding-upper-tails-of-maxima}.

Since Lemma \ref{lemma:AGG-maxima-upper-tails} holds regardless of the dependence structure, the same conclusions hold if one replaces $M_p$ by $M_{S_p} = \max_{j\in S_p}\epsilon(j)$ and $p$ by $q = q(p)=|S_p|$, where $S_p$ is any sequence of sets such that $q \equiv |S_p| \to \infty$.
This entails \eqref{eq:URS-condition-upper-side}.

On the other hand, the proof of \eqref{eq:URS-condition-lower-side} uses a more elaborate argument based on the Sudakov-Fernique bound.
We proceed by first bounding the probability by an expectation. 
For all $\delta>0$, we have
\begin{align}
    \P\left[\frac{M_{S_p}}{u_q}<1-\delta\right] 
        &= \P\left[-\left(\frac{M_{S_p}}{u_q} - (1+c_q)\right) > \delta + c_q\right] \nonumber \\
        %&\le \P\left[\max{\left\{-\left(\frac{M_{S_p}}{u_q} - (1+c_q)\right),0\right\}} > \delta + c_q\right] \nonumber \\
        &\le \P\left[\left(\frac{M_{S_p}}{u_q} - (1+c_q)\right)_->\delta+c_q\right] \nonumber \\
        &\le \frac{1}{\delta + c_q}\E\left[\left(\frac{M_{S_p}}{u_q} - (1+c_q)\right)_-\right], \label{eq:Gaussian-maxima-lower-expectation-bound}
\end{align}
where $(x)_-:=\max\{-x,0\}$ and the last line follows from the Markov inequality.
The next result shows that the upper bound in \eqref{eq:Gaussian-maxima-lower-expectation-bound} vanishes.
\begin{lemma} \label{lemma:Gaussian-maxima-lower-expectation}
  Let ${\cal E}$ be a Gaussian UDD  array  and 
  $S_p\subseteq\{1,\ldots,p\}$ be an arbitrary sequence of sets 
  such that $q = q(p) = |S_p|\to\infty$.  Then, for $M_{S_p}:= \max_{j\in S_p} \epsilon_p(j)$ and $c_q$ as in \eqref{eq:choice-of-c_p}, we have
  \begin{equation} \label{eq:Gaussian-maxima-lower-expectation}
    \E\left[\left(\frac{M_{S_p}}{u_q} - (1+c_q)\right)_-\right] \to 0,\ \ \quad \mbox{ as }p\to \infty.
  \end{equation}
\end{lemma}
The proof of the lemma is given in Appendix \ref{subsec:bounding-lower-tails-of-maxima}.


Going back to the proof of Theorem \ref{thm:Gaussian-weak-dependence}, we observe that Relations \eqref{eq:Gaussian-maxima-lower-expectation-bound} and \eqref{eq:Gaussian-maxima-lower-expectation} imply \eqref{eq:URS-condition-lower-side}, which completes the proof of the `if' part. \qed 

\begin{remark} Only the Sudakov-Fernique minorization argument used in the proof of Lemma \ref{lemma:Gaussian-maxima-lower-expectation}, relies on the Gaussian assumption. We expect the techniques and results here to be useful in extending Theorem \ref{thm:Gaussian-weak-dependence} to more general class of distributions, say, the AGG model.
\end{remark}

\subsection{Ramsey's Coloring Theorem and structure of correlation matrices} \label{subsec:Ramsey}

We provide a general result on the structure of arbitrary correlation matrices in this section, which may be of independent interest. 
Its proof uses the Ramsey Theorem from graph theory, which we briefly review next.

Given any integer $k\ge 1$, there is always an integer $R(k,k)$ called the {\em Ramsey number}:
\begin{equation}\label{eq:Ramsey-number}
k\le R(k,k)\le { 2k-2 \choose k-1 }
\end{equation}
such that the following property holds:
every undirected graph with at least $R(k,k)$ vertices will contain {\em either} a clique of size $k$, or an {\em independent set} of $k$ nodes. 
Recall that a clique is a complete sub-graph where all pairs of nodes are connected, and an independent set is a set of nodes where no two nodes are connected.

This result is a consequence of the celebrated work of \citet{ramsey2009problem}, which 
gave birth to Ramsey Theory (see e.g., \citet{conlon2015recent}).  
The Ramsey Theorem and the upper bound \eqref{eq:Ramsey-number} (established first in \cite{erdos1935combinatorial}) are at the heart of the proof of the following result.
%An excellent introduction to Ramsey theory is given in \url{http://math.mit.edu/~fox/MAT307-lecture05.pdf}.

\begin{proposition} \label{prop:lower-bound-correlation-Ramsey}
  Fix $\gamma\in(0,1)$ and let $P = \left(\rho(i,j)\right)_{n\times n}$ be an arbitrary correlation
  matrix. If 
  \begin{equation}\label{eq:Ramsey-the-k-def}
   k:= \lfloor \log_2({n})/2 \rfloor  \ge \lceil 1/\gamma \rceil + 1,
  \end{equation}
  then there is a set of $k$ indices $K = \{l_1, \ldots, l_k\}\subseteq \{1,\ldots,n\}$ 
  such that 
  \begin{equation} \label{eq:lower-bound-correlation-Ramsey}
      \rho(i,j) \ge -\gamma, \mbox{ for all } i,j\in K.
  \end{equation}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:lower-bound-correlation-Ramsey}]
By using \eqref{eq:Ramsey-number} and a refinement of the Stirling's formula, 
we will show at the end of the proof that for $k \le \log_2({n})/2$, we have 
\begin{equation}\label{eq:Ramsey-bounds}
 R(k,k) \le n,
\end{equation}
where $R(k,k)$ is the Ramsey number.  

Now, construct a graph with vertices $\{1,\dots,n\}$ such that there is an edge between nodes $i$ and $j$ if and only if $\rho(i,j) > -\gamma$. 
In view of \eqref{eq:Ramsey-bounds} and Ramsey's theorem (see e.g., Theorem 1 in \cite{fox2009lecture} or \cite{conlon2015recent} for a recent survey on Ramsey theory), there is a subset of $k$ nodes $K =\{l_1,\dots,l_k\}$, which is either a {\em complete graph} or an {\em independent set}.

If $K$ is a complete graph, then by our construction of the graph, Relation \eqref{eq:lower-bound-correlation-Ramsey} holds. 

Now, suppose that $K$ is a set of independent nodes.  This means, again by the construction of our graph, that
$$
\rho(i,j) < -\gamma,\quad\mbox{for all }i\not= j\in K.
$$
Let $Z_i,\ i \in K$ be zero-mean random variables such that 
$\rho(i,j) = E [Z_iZ_j]$. Observe that
\begin{equation} \label{eq:Ramsey-proof-contradiction}
    \var\left( \sum_{i\in K} Z_i\right) 
    = \sum_{i\in K} \var(Z_i) + \sum_{\substack{i\not=j\\i,j \in K}} \cov(Z_i, Z_j) 
    <  k - k(k-1)\gamma,
\end{equation}
since $\var(Z_i)=1$ and $\rho(i,j)<-\gamma$ for $j\neq j$.
By our assumption, $k\ge \left(\lceil 1/\gamma \rceil + 1\right)$, or equivalently, $(k-1) \ge 1/\gamma$, the variance in \eqref{eq:Ramsey-proof-contradiction} is negative. 
This is a contradiction showing that there are no independent sets $K$ with cardinality $k$.

To complete the proof, it remains to show that Relation \eqref{eq:Ramsey-bounds} holds.
In view of the upper bound on the Ramsey numbers \eqref{eq:Ramsey-number}, it 
is enough to show that $k \le \log_2(\sqrt{n})$ implies
$$
 { 2 k - 2 \choose k-1 } \le n.
$$
This follows from a refinement of the Stirling formula, due to \citet{robbins1955remark}:
$$
 \sqrt{2\pi} m^{m+1/2} e^{m} e^{\frac{1}{(12 m +1)}} \le  m! \le \sqrt{2\pi} m^{m+1/2} e^{m} 
 e^{\frac{1}{12 m}}.
$$
Indeed, letting $\widetilde k:= k-1$, and applying the above upper and lower bounds 
to the  terms $(2\widetilde k)!$ and $\widetilde k!$, respectively, we obtain:
\begin{align*}
{ 2 k - 2 \choose k-1 } \equiv \frac{(2\widetilde k)!}{ (\widetilde k!)^2 }
\le \frac{2^{2\widetilde k}}{\sqrt{\pi \widetilde k}}\exp\left \{ \frac{1}{24 \widetilde k} -
\frac{2}{ 12 \widetilde k +1}\right\} < 2^{2 k}
\end{align*}
where the last two inequalities follow by simply dropping positive factors less than $1$.
Since $2k \le \log_2(n)$, the above bound implies Relation \eqref{eq:Ramsey-bounds} 
and the proof is complete.
\end{proof}

Using Proposition \ref{prop:lower-bound-correlation-Ramsey}, we establish an auxiliary result used in the proof of Theorem \ref{thm:Gaussian-weak-dependence}.


\begin{lemma} \label{lemma:positive-correlation}
  Let $c\in(0,1)$, and $P = \left(\rho(i,j)\right)_{(n+1)\times(n+1)}$ be a correlation matrix such that \begin{equation} \label{eq:positive-correlation-lemma-condition}
      \rho(1,j) > c \quad \mbox{for all } j = 1,\ldots,n+1.
  \end{equation}
  If $n \ge 2^{2\lceil2/c^2\rceil+4}$, then there is a set of indices $K = \{l_1, \ldots, l_k\}\subseteq \{2,\ldots,n+1\}$ of cardinality $k = |K| = \lfloor\log_2{\sqrt{n}}\rfloor$, such that 
  \begin{equation} \label{eq:positive-correltation-lemma-conclusion}
      \rho(i,j) > \frac{c^2}{2} \quad\mbox{for all } i,j\in K.
  \end{equation}
  That is, all entries of the $k\times k$ sub-correlation matrix $P_K:=\left(\rho(i,j)\right)_{i,j\in K}$ are larger than $c^2/2$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:positive-correlation}]
    Let $Z_1, \ldots, Z_{n+1}$ be random variables with covariance matrix $P$.
    Denote $\rho_j = \rho(1,j)$ and define 
    \begin{equation}
      R(j) = 
      \begin{cases}
        \frac{1}{\sqrt{1-\rho_j^2}}\left(Z(j) - \rho_j Z(1)\right), &\mbox{if } \rho_j<1,\\
        R^* &\mbox{if } \rho_j=1,
      \end{cases}
    \end{equation}
    where $R^*$ is an arbitrary zero-mean, unit-variance random variable.
    It is easy to see that $\var(R(j)) = 1$, and
    \begin{align*}
    \cov\left(Z(i), Z(j)\right) &= \cov\left(\rho_i Z(1) + \sqrt{1-\rho_i^2} R(i), \; \rho_j Z(1) + \sqrt{1-\rho_j^2} R(j)\right) \\
        &= \rho_i\rho_j + \sqrt{1-\rho_i^2}\sqrt{1-\rho_j^2} \;\cov\left(R(i), R(j)\right) \\
        &\ge c^2 + \min\left\{\cov\left(R(i), R(j)\right), 0\right\}.
    \end{align*}
    
    Therefore, Relation \eqref{eq:positive-correltation-lemma-conclusion} would hold if we can find a set of indices $K = \{l_1,\ldots,l_k\}$ such that $\cov\left(R(i),R(j)\right)>-c^2/2$ for all $i,j\in K$, where $k=|K|=\lfloor\log_2\sqrt{n}\rfloor$.
    This, however, follows from Proposition \ref{prop:lower-bound-correlation-Ramsey} applied to $\left(R(j)\right)_{j=2}^{n+1}$ with $\gamma = c^2/2$, provided that 
    $$
    k = \lfloor\log_2\sqrt{n}\rfloor \ge \lceil 2/c^2 \rceil.
    $$
    The last inequality indeed follows form the assumption that $n \ge 2^{2\lceil2/c^2\rceil+4}$.
\end{proof}


