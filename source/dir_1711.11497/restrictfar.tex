\newcommand{\rdist}{\mathsf{rdist}}
\newcommand{\lmax}{\lambda_{max}}
\section{Separation in Restriction Distance}
For a parameter $\epsilon>0$ to be chosen later and $s\in\{0,1\}^{\M_d}$ let
$$p_s(x_1,\ldots,x_n):=e_d(x_1,\ldots,x_n)-\epsilon\sum_{M\in\M_d} s_M q_M(x_1,\ldots,x_n).$$
For any polynomial $p$ hyperbolic with respect to $\one$, define the {\em restriction embedding}
$$ \Lambda(p):=(\lmax(p(t\one+\one_S)))_{S\in\S_d}.$$

\begin{lemma} \label{lem:restrictfar} If $0<\epsilon<R/N$ and $s,s'\in\{0,1\}^{\M_d}$ are distinct then both $p_s$ and $p_s'$ are hyperbolic
with respect to $\one$ and
$$\|\Lambda(p_s)-\Lambda(p_{s'})\|_\infty\ge \Delta:=\epsilon \left(\binom{n}{d} d e\right)^{-1}.$$
\end{lemma}

\begin{proof} Since $\epsilon<R/N$, we have $\|e_d-p_s\|_1,\|e_d-p_{s'}\|_1 \le R$ so by Theorem \ref{thm:perturb} both of them must be hyperbolic with respect to $\one$.

Since $s\neq s'$, suppose $s_M=0$ and $s'_M=1$ for some matching $M\in\M_d$, and let $S\in \S_d$ be a set which fully crosses $M$.  By Lemma \ref{lem:manymatchings}, $M$ is the {\em only} matching in $\M_d$ which crosses $S$, so we have $q_M(\one_S)=1$ and $q_{M'}(\one_S)=0$ for all other $M'\neq M\in\M_d$. Thus, along the restriction $t\one +\one_S$, one has
\begin{equation}\label{eqn:jacobi} q_s(t\one+\one_S) = e_d(t\one + \one_S)=:J(t)\end{equation}
and
$$q_{s'}(t\one+\one_S) = e_d(t\one + \one_S)-\epsilon q_M(\one_S) = e_d(t\one+\one_S)-\epsilon=J(t)-\epsilon,$$
where we have again used that the $q_M$ are translation invariant. Note that $J(t)$ has positive leading coefficient, so subtracting a constant from it increases its largest root. Thus, our task is reduced to showing that
$$\lambda_{max}(J(t)-\epsilon)\ge \lambda_{max}(J(t))+\Delta.$$

To analyze the behavior of this perturbation, first we note that 
$$J(t)= e_d(t1, \ldots, t1, t1 + 1, \ldots, t1 + 1) = \frac{1}{(n-d)!} D^{n-d} e_N(t \one + \one_S) = \frac{1}{(n-d)!} D^{n-d}t^{n-d} (t+1)^d.$$
Since $t^{n-d}(t+1)^d$ has roots in $[-1,0]$ and the roots of the derivative of a polynomial interlace its roots, 
we immediately conclude that the zeros of $J$ satisfy $-1\le z_1\le \ldots \le z_d\le 0$. 
Again by interlacing, we see that $J'(z)\ge 0$ and $J''(z)\ge 0$ for $z\ge z_d$, whence $J$ is monotone and convex above $z_d$. Thus, $\lambda:=\lambda_{max}(J(t)-\epsilon)$ is the least $\lambda>z_d$ such that $J(\lambda)\ge \epsilon$. Let $\theta>0$ be a parameter to be set later. Then either $\lambda\ge z_d+\theta$ or we have by convexity that
$$J(z_d+\theta)\le J(z_d)+\theta J'(z_d+\theta).$$
The first term is zero and we can upperbound the second term as:
$$|J'(z_d+\theta)| \le \binom{n}{d} \sum_{i\le d}|\prod_{j\neq i}(z_d+\theta-z_i)|\le \binom{n}{d} d\cdot (1+\theta)^{n-1},$$
since $|z_d-z_i|\le 1$ for every $i\le d$. Thus, we have
$$ J(z_d+\theta)\le \binom{n}{d}d\theta(1+\theta)^{n-1}\le \binom{n}{d} d\theta e$$
whenever $\theta<1/n$, which is less than $\epsilon$ for $\theta=\epsilon (\binom{n}{d} d e)^{-1}$. This means that in either case we must have
$$\lambda\ge z_d+	\epsilon \left(\binom{n}{d} d e\right)^{-1},$$
as desired.
\end{proof}
