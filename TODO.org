* TODO introduction(19/09)
** DONE install grobid
** DONE read code & documentation
** DONE decide what to do next
* DONE beginning(19/09-26/09)
** DONE xgrep (xmllint or other tool for regexping xmls)
*** DONE try xgrep
****  xgrep -s 'div:p/Lemma 1./' ./internship_report.tei.xml 
*** DONE do something nontrivial
** DONE find beginning of proofs
* DONE simple extraction(26/09-2/10)
** DONE More reasonable regex for extraction
** DONE Learn to trivially extract full texts of results (problem when no proof!)
* DONE get source(26/09-2/10)
** DONE get some source code from ArXiv 
** DONE curl, write script unpacking everything 
** TODO ?(MORE source code from Amazon cloud?)
* DONE (not used) compare with the source automatically(26/09-.)
** DONE get some ideas
** DONE count each type of results (14/10)
** DONE comparison of source and my extraction (14/10-7/11)
*** DONE find each result of source in my extraction: info about missed and extra results (14/10)
*** DONE remove bugs in comparison (24/10)
**** DONE dots, newlines
**** DONE ^A, ^L
**** DONE formulas from source-part: bug of pdftotext hmmh
     1412.7885 - pdftotext
     1502.03280 - pdftotext remark: cannot do much better, strange font with spaces :(
     1503.02231 - just didn't work the compilation part
     1510.05543 - pdftotext
     1511.08784 -  pdftotext (k)... etc
     1411.0867 - first lemma moved, pdftotext
**** DONE analyse only results
**** DONE others
     1509.08966 ? Lemma num
*** DONE write new comparison script to process tex source fragments (7/11-)
*** DONE compare borders...
* DONE learn how to use scikit-learn, pandas (22/11)
** DONE read (25/11)
** DONE simple example (25/11)
** DONE try on lines
* TODO generate training data
** DONE use LaTex itself: extract.sty (26/09-7/11-)
*** DONE find all newtheorems to determine which keywords are used (2/10)
*** DONE for each instance of one of these keywords, extract its body, number (page...) > res-proofs.txt (26/09)
*** DONE extract proofs at the same time > res-proofs.txt (26/09)
*** DONE make the extraction compilable > res-proofs.tex (26/09)
*** DONE Make all this UNIVERSAL: for all results etx (2/10)
*** DONE Actually, it's better to use tex->pdf->pdftotext, implement! (25/11)
**** DONE change extract.sty, source_extract.sh (29/11)
**** DONE add pdf->xml to source_extract.sh (29/11)
**** DONE resolve bugs: some papers compile forever (see *problems): timeout (29/11)
**** DONE write a testing function: to count ill-formed xml-outputs (29/11)
**** DONE write a python-script generating text from xml
*** TODO  etc
**** TODO postprocessing of LaTex-generated extraction (not needed)
***** DONE compile the extraction (2/10)
***** DONE get the text of compiled extraction > res-proofs-text.txt (14/10)
***** DONE pdftotext and miner: to compare  (26/10)
   146 times pdftotxt was better and 187 times miner was better
***** DONE pdftotxt and miner: to combine. SORT !(26/10)
***** TODO assign each proof to the closest result
**** TODO Latex: when there is no newtheorem: hand-made definitions (Not needed)
     Not much: forget about it for now
**** TODO Don't use tex->pdf->pdftotext. Just do directely tex->text (7/11-18/11)-
**** DONE resolve the bug. 46 good out of 128 :-(  (9/11-18/11)
**** DONE remove fatal errors: all of them are "*.sty not found" --- not my fault: 24 of them
**** TODO resolve remaining bugs. 73 good out of 102 why?
** DONE script to generate xml-lines: pdfs_to_xml_lines.sh (29/11)
   generate two files with xml-lines out of two files: paper.pdf and paper_res.pdf 
   put them in source_extraction/results_pdf_xml/ paper_xml.xml and paper_resxml.xml
** DONE source to xml-lines: extract_source_to_xml.sh (29/11)
** DONE write the code generating data to learn: source_to_training_data.py (29/11-5/12)
*** DONE for one file (1/12)
*** DONE vectors to stdout (5/12)
*** DONE generate csv (5/12)
*** TODO test better
*** DONE visualise it
** TODO add more features: first letter, first word, is_bold (12/12-20/12-)
*** DONE implement
*** DONE test
*** TODO add "is first word proof" feature
** DONE add more types: text, heading, body, first_line_after_body (13/12)
** TODO analyse existent fonts and make good regexps for them (16/12-)
*** DONE extract
*** DONE check the most common (20/12)
*** TODO check all
*** TODO extract fonts ignoring before+ and ofntsize (20/12)
** DONE keep font for each letter: change word data structure (16/12)
** TODO ignore strange formula-lines in all this (what is formula? ...) (25/12-)
** TODO test for correctness: remove bugs !!! (24/12-) 
*** DONE how many cases fail with .95-correctess limit?
87 of papers did not succeed in matching
98 of papers succeeded in matching
*** TODO what happens with each of these cases(24/12-)
**** TODO debug invalid results-input: just do not consider them (24/12)
1612.00765

xml-problem:
dasha@too-ticki:~/ENS/IE_project/source_extraction/results_pdf_xml$ ls | grep "_res" | wc -l
185
dasha@too-ticki:~/ENS/IE_project/source_extraction/results_pdf_xml$ find . -empty -type f | grep "_res" | wc -l
70

45 of papers did not succeed in matching
19 of papers succeeded in matching
121 of papers got invalid results-input
**** TODO debug the rest (25/12)
     +add restart_counter_for each line!!!
     -remove the global counter 
     -count not founf result lines

*** TODO add function correcting bugs: body in the middle of text etc

** DONE generate many documents instead of one (17/12)
* TODO extraction, borders
** DONE use pdftohtml, italic fonts for theorems: fail? (14/10)
** DONE html idea: in 1/4 of cases could work: try to find the end of italic
   html fonts
html: <i> tag is italic (inside results)

Problem: sometimes, pdftohtml doesn't see the italic font. WHY? 

Only
81/229 contain <i>,
50/229 contain <i>Proof.

fonts:
1502.06131.html <i> proposition 
1504.06288.html <i> theorem

nofonts:
1410.5106s.html <no i> theorem
tools don't work:
 - sejda 
 - pdftohtml 
 - libreoffice
 - calibre

** DONE use xml, pdfminer pdf2txt -t xml(9/11-12/11)
*** DONE use xgrep to extract only lines
*** DONE extract letters+font
*** DONE extract list of lines where each line is a list of pairs (word, its font)
** DONE obtain a vector for each line: number of italic words etc (12/11)
** DONE find theorems borders in function of this vector (18/11,21/11-)
*** DONE in a stupid way
*** TODO adjust constants
** TODO use learning (26/11-)
*** DONE lines to dataframe(26/11)
*** DONE try "istheorem" with Naive Bayes Classification (4/12-11/12)
**** DONE precision: 0.84725722039154872 -> 0.89
**** DONE Add math-font to the vector (6/12)
**** DONE plot more readable? (8/12)
**** DONE how adding/removing parameters change the precision?
**** TODO get more precise information: detailed accuracy etc
*** TODO crf - random fields (15/12-...)
**** TODO understand (15/12-)
***** TODO An Introduction to Conditional Random Fields (By Charles Sutton and Andrew McCallum) (17/12-)
      (p. 11)
***** TODO Understand the difference between: (crfsuite)
      – 'lbfgs' - Gradient descent using the L-BFGS method
      – 'l2sgd' - Stochastic Gradient Descent with L2 regularization term
      – 'ap' - Averaged Perceptron
      – 'pa' - Passive Aggressive (PA)
      – 'arow' - Adaptive Regularization Of Weight Vector (AROW)
**** DONE crfsuite - treat many documents instead of one (17/12)
**** DONE resolve bugs with types (19/12)
**** DONE implement
**** TODO Make work better
**** TODO test
*** TODO Compare different algorithms
* TODO get more data(19/11-)
** DONE download 300 more papers
** TODO take care of folders containing source (not only tex-files)  
** TODO 2000
   


* 19/12
+ pairplot: make all points visible!!!
+ other fonts: italic, math in the name analyse: test how many...;
  analyse fonts in all -results of all papers, give percentage of each
                       -fulltexts  
+ font for each letter
+ other features: first letter, first word,...
  + do
+ other types: beginning of theorem, end of theorem
- debug generator!!!!
- ignore formula-lines (<4 words etc) ???

- use scikit-learn crfsuite 
  + start
  + debug....

* NEXT 2
+ make generator accept only  fully good files

- Bugs results and fulltexts are no the same: lines exchanges!!!
  + rewrite all code: generatetrainingdate
  + problem: 1005.3030 : exchanged pages.....
  + ignore empty and strange strings
  - debug all bad papers using vectors_text
  - test

(
xml-problem:
70 empty files out of 185 extracted xmls

dasha@too-ticki:~/ENS/IE_project/source_extraction/results_pdf_xml$ ls | grep "_res" | wc -l
185
dasha@too-ticki:~/ENS/IE_project/source_extraction/results_pdf_xml$ find . -empty -type f | grep "_res" | wc -l
70
)


fontforge for examples of symbols 
  + fonts: forget about the part before +
  + cmex: math symbols
  + analyse fonts !

-more DATA

- bayess method:  more info
check

- change grid_search -> model_selection

- keep the same c1,c2

- ne feature: first word is Proof

- more training data: x10

- try more with crfsuite
check up on forums etc

- naive method written by myself works better?

* problems
- source extraction
-- sourcetopdf
114 good, 71 empty
--- 0812.4628 takes tons of time! Fatal error
--- 1809.05121 doesn't stop: why?

* source:
  ./get_source.sh
  
* source_extr and comparison
  ./extract_source_to_xml.sh

* extraction:
  ./pdfs_to_xmls_lines.sh   - for each file.pdf, create file_xml.xml
  consisting of a list of xml-lines (xml-line per line) of the file

  python ./lines_to_text.py - does the extraction working on
  file_xml.xml
  
  
* etc  
26/11 17h


source to text: hyperref !!!!:


remove usepackage hyperref, hypersetup ?


 - html for cases when it works: proportion of <i> in text
  differentiate lines with formulas from others 

- itextrups
 TF: fonts R48, objects
font descriptor 49 italic angle
70 = 64 + 4 + 2
64 = italic
TD


CMTI  - latex italic
pdftohtml -xml fontfullname

pdfminer pdf2txt -t xml: font names !! Use it?
letter by letter python ../../miner/tools/pdf2txt.py -t xml ../../tmp/bestiary/1511.08784_extr.pdf >../../ blabla.txt


xml: 
italic:             xgrep -x "//text[@font='TNLIMT+CMTI10']" text_xgrep/1407.8035_xml.xml 
italic plus spaces: xgrep -t -x "//text[@font='TNLIMT+CMTI10' or not (@*)]" ./1407.8035_xml.xml  > out.txt

all text:           xgrep -t -x "//text" ./1407.8035_xml.xml  > out_all_text.txt




Try to line by line heuristics: density of it, equations

vector with features.

 


scikiy-learn python
weka java
mlpack C++
